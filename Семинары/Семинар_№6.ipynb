{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUQ87950Wb0i"
      },
      "source": [
        "# Семинар 6. Пошаговое руководство по популярным моделям и реализациям"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dAUPguGp9qsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTsuoQ7KJz3h"
      },
      "source": [
        "**Преподаватель:** Никита Киселев"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db71l_BhJz3h"
      },
      "source": [
        "## Введение"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjTlD-3GWkNu"
      },
      "source": [
        "На этом семинаре мы\n",
        "- Рассмотрим особенности практической реализации современных диффузионных моделей.\n",
        "- Поговорим о DDIM как ускоренном методе сэмплирования для DDPM.\n",
        "- Обсудим, что еще придумали в последнее время для повышения качества диффузионных моделей."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uhoBXlyXsnN"
      },
      "source": [
        "**План занятия:**\n",
        "\n",
        "1. DDIM как метод ускорения сэмплирования в DDPM.\n",
        "2. Различные планировщики шума: примеры от базовых до продвинутых.\n",
        "3. Использование нескольких text encoders для повышения качества восприятия текстовой информации (на примере SDXL и SD3).\n",
        "4. Трансформерные архитектуры в диффузионных моделях на примере DiT, а также имплементация одного слоя MM-DiT из SD3.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eQSe_UeJz3i"
      },
      "source": [
        "## 1. DDIM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUiT5YM-Jz3i"
      },
      "source": [
        "Сегодняшнее наше занятие мы начнем с очень важной темы в диффузионных моделях — Denoising Diffusion Implicit Models (DDIM).\n",
        "\n",
        "Для начала давайте кратко пробежимся по тому, как устроены Denoising Diffusion **Probabilistic** Models (DDPM), которые вам уже прекрасно известны."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbqrpkk7Jz3j"
      },
      "source": [
        "### 1.1. Воспоминание о DDPM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al-fgVaxJz3j"
      },
      "source": [
        "1. Последовательные прямой и обратный процессы являются **марковскими** процессами:\n",
        "$$\n",
        "q(\\mathbf{x}_t | \\mathbf{x}_{t-1}) = q(\\mathbf{x}_t | \\mathbf{x}_{t-1}, \\mathbf{x}_0),\n",
        "$$\n",
        "то есть распределение каждого следующего изображения зависит только от предыдущего, и не от каких более ранних.\n",
        "\n",
        "2. **Переходное распределение** в прямом процессе задается следующим образом:\n",
        "   $$\n",
        "        q(\\mathbf{x}_t | \\mathbf{x}_{t-1}) = \\mathcal{N}\\left( \\sqrt{1 - \\beta_t} \\mathbf{x}_{t-1}, \\beta_t \\mathbf{I} \\right),\n",
        "   $$\n",
        "   где $\\{ \\beta_t \\in (0, 1) \\}_{t=1}^{T}$ и $\\beta_1 \\leq \\beta_2 \\leq \\ldots \\leq \\beta_T$ — расписание дисперсий. То есть постепенно, шаг за шагом, мы зашумляем исходное изображение вплоть до случайного шума. Также, вводя для простоты обозначений $\\alpha_t = 1 - \\beta_t$, его можно переписать в виде\n",
        "   $$\n",
        "        q(\\mathbf{x}_t | \\mathbf{x}_{t-1}) = \\mathcal{N}\\left( \\sqrt{\\alpha_t} \\mathbf{x}_{t-1}, (1 - \\alpha_t) \\mathbf{I} \\right).\n",
        "   $$\n",
        "   Здесь получается, что каждое новое изображение будет в среднем по амплитуде отличаться от предыдущего в $\\sqrt{\\alpha_t}$ раз и при этом будет еще более шумным, с дисперсией $(1 - \\alpha_t)$.\n",
        "\n",
        "3. Полезным также является выражение зашумленного на $t$-м шаге изображения через исходное:\n",
        "   $$\n",
        "        q(\\mathbf{x}_t | \\mathbf{x}_0) = \\mathcal{N}\\left( \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1 - \\bar{\\alpha}_t) \\mathbf{I} \\right),\n",
        "   $$\n",
        "   где $\\bar{\\alpha}_t = \\prod_{i=1}^{t} \\alpha_i$, то есть $\\bar{\\alpha}_t$ является произведением всех предыдущих коэффициентов и постепенно стремится к нулю при увеличении номера шага $t$.\n",
        "\n",
        "4. Наконец, напомним выражение для обратного процесса (с учетом исходного изображения $\\mathbf{x}_0$), которое также является гауссовским, со средним значением $\\tilde{\\boldsymbol{\\mu}}(\\mathbf{x}_t, \\mathbf{x}_0)$, зависящим от зашумленного изображения $\\mathbf{x}_t$ и исходного изображения $\\mathbf{x}_0$, и дисперсией $\\tilde{\\sigma}_t^2$ (их можно честно вывести, используя формулу Байеса):\n",
        "   $$\n",
        "        q(\\mathbf{x}_{t-1} | \\mathbf{x}_t, \\mathbf{x}_0) = \\mathcal{N}\\left( \\tilde{\\boldsymbol{\\mu}}(\\mathbf{x}_t, \\mathbf{x}_0), \\tilde{\\sigma}_t^2 \\mathbf{I} \\right),\n",
        "   $$\n",
        "   где\n",
        "   $$\n",
        "        \\tilde{\\boldsymbol{\\mu}}(\\mathbf{x}_t, \\mathbf{x}_0) = \\frac{\\sqrt{{\\alpha}_t} (1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t}{1 - \\bar{\\alpha}_t} \\mathbf{x}_0,\n",
        "   $$\n",
        "   $$\n",
        "        \\tilde{\\sigma}_t^2 = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t,\n",
        "   $$\n",
        "   или же можно переписать среднее в другом виде, не через исходное изображение $\\mathbf{x}_0$, а через шум $\\boldsymbol{\\epsilon}_t$:\n",
        "   $$\n",
        "        \\tilde{\\boldsymbol{\\mu}}(\\mathbf{x}_t, \\boldsymbol{\\epsilon}_t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_t \\right),\n",
        "   $$\n",
        "   где\n",
        "   $$\n",
        "        \\boldsymbol{\\epsilon}_t = \\frac{1}{\\sqrt{1 - \\bar{\\alpha}_t}} \\left( \\mathbf{x}_t - \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 \\right).\n",
        "   $$\n",
        "\n",
        "5. В итоге мы обучаем одну из трех моделей (как раз-таки нейронных сетей) для вариационного распределения $p_\\theta(\\mathbf{x}_{t-1} | \\mathbf{x}_t)$:\n",
        "\n",
        "1) Предсказание среднего значения $\\tilde{\\boldsymbol{\\mu}}(\\mathbf{x}_t, \\mathbf{x}_0)$ с помощью нейронной сети $\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)$:\n",
        "$$\n",
        "    p_\\theta(\\mathbf{x}_{t-1} | \\mathbf{x}_t) = \\mathcal{N}\\left( \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\tilde{\\sigma}_t^2 \\mathbf{I} \\right).          \n",
        "$$\n",
        "\n",
        "2) Предсказание исходного изображения $\\mathbf{x}_0$ с помощью нейронной сети $\\hat{\\mathbf{x}}_\\theta(\\mathbf{x}_t, t)$:\n",
        "$$\n",
        "    p_\\theta(\\mathbf{x}_{t-1} | \\mathbf{x}_t) = \\mathcal{N}\\left( \\frac{\\sqrt{{\\alpha}_t} (1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t}{1 - \\bar{\\alpha}_t} \\hat{\\mathbf{x}}_\\theta(\\mathbf{x}_t, t), \\tilde{\\sigma}_t^2 \\mathbf{I} \\right).         \n",
        "$$\n",
        "\n",
        "3) Предсказание шума $\\boldsymbol{\\epsilon}_t$ с помощью нейронной сети $\\hat{\\boldsymbol{\\epsilon}}_\\theta(\\mathbf{x}_t, t)$:\n",
        "$$\n",
        "    p_\\theta(\\mathbf{x}_{t-1} | \\mathbf{x}_t) = \\mathcal{N}\\left( \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\boldsymbol{\\epsilon}}_\\theta(\\mathbf{x}_t, t) \\right), \\tilde{\\sigma}_t^2 \\mathbf{I} \\right).         \n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOT2c-cOJz3k"
      },
      "source": [
        "**Вопрос.** Имея зашумленную картинку $\\mathbf{x}_t$ и предсказанный шум $\\hat{\\boldsymbol{\\epsilon}}_t(\\mathbf{x}_t, t)$, как мы можем получить предсказание $\\mathbf{x}_0$? (Обозначим его $\\mathbf{x}_{0|t}$.)\n",
        "\n",
        "<details>\n",
        "  <summary><b>Ответ</b></summary>\n",
        "\n",
        "  <font color='green'>$\\mathbf{x}_{0|t} = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}} \\left( \\mathbf{x}_t - \\sqrt{1 - \\bar{\\alpha}_t} \\hat{\\boldsymbol{\\epsilon}}_\\theta(\\mathbf{x}_t, t) \\right).$\n",
        "\n",
        "  <b>Решение.</b> Получаем ответ напрямую из формулы $\\boldsymbol{\\epsilon}_t = \\frac{1}{\\sqrt{1 - \\bar{\\alpha}_t}} \\left( \\mathbf{x}_t - \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 \\right)$.\n",
        "  </font>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak2g4y1IJz3k"
      },
      "source": [
        "А теперь давайте подумаем о том, какие есть плюсы и минусы у DDPM:\n",
        "\n",
        "$+$ Качественные сгенерированные изображения\n",
        "\n",
        "$+$ Разнообразие объектов\n",
        "\n",
        "$-$ Очень медленная скорость генерации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWx3NdtUJz3k"
      },
      "source": [
        "**Вопрос.** Как **ускорить** обратный процесс, при этом сохранив высокое качество генераций?\n",
        "\n",
        "**Ответ.** А для этого и придумали DDIM!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdlxeKkPJz3k"
      },
      "source": [
        "### 1.2. Введение в DDIM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn4O0EOcJz3k"
      },
      "source": [
        "**Вопрос.** Какая была отличительная особенность последовательных прямого и обратного процессов в DDPM?\n",
        "\n",
        "<details>\n",
        "  <summary><b>Ответ</b></summary>\n",
        "  \n",
        "  <font color='green'>\n",
        "  \n",
        "  Они были марковскими, то есть на примере прямого процесса: $q(\\mathbf{x}_t | \\mathbf{x}_{t-1}) = q(\\mathbf{x}_t | \\mathbf{x}_{t-1}, \\mathbf{x}_0)$.\n",
        "  </font>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuaYbj3dJz3k"
      },
      "source": [
        "Авторы [Denoising Diffusion Implicit Models (DDIM)](https://arxiv.org/abs/2010.02502) предложили использовать **не марковский** прямой процесс! А какой же тогда? Давайте разбираться.\n",
        "\n",
        "Прямой марковский процесс в DDPM выглядит вот так:\n",
        "\n",
        "$$\n",
        "    q(\\mathbf{x}_{1:T} | \\mathbf{x}_0) = \\prod_{t=1}^{T} q(\\mathbf{x}_{t} | \\mathbf{x}_{t-1}).\n",
        "$$\n",
        "\n",
        "<figure align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=1p-jML32gx9C-8bKuMvLKN5juBHU9gZzT\" alt=\"ddpm\" width=\"500\"/>\n",
        "    <figcaption> Прямой процесс DDPM. Источник: <a href=\"https://arxiv.org/abs/2006.11239\">Ho et al. 2020</a> </figcaption>\n",
        "</figure>\n",
        "\n",
        "А вот авторы DDIM предложили построить процесс другим образом, введя\n",
        "\n",
        "$$\n",
        "    q_\\sigma(\\mathbf{x}_{1:T} | \\mathbf{x}_0) = q_\\sigma(\\mathbf{x}_T | \\mathbf{x}_0) \\prod_{t=2}^{T} q_\\sigma(\\mathbf{x}_{t-1} | \\mathbf{x}_t, \\mathbf{x}_0).\n",
        "$$\n",
        "\n",
        "> **Комментарий.** В отличие от предыдущей формулы, здесь появляется дополнительный индекс $\\sigma$. Как мы увидим позже, он означает наличие дополнительного набора параметров — расписания дисперсий $\\sigma_1, \\ldots, \\sigma_T$.\n",
        "\n",
        "<figure align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=1T3SV7kc8b-Tbr_EVLpdaKNUYAPmw-EFY\" alt=\"ddim\" width=\"500\"/>\n",
        "    <figcaption> Прямой процесс DDIM. Источник: <a href=\"https://arxiv.org/abs/2006.11239\">Ho et al. 2020</a> </figcaption>\n",
        "</figure>\n",
        "\n",
        "Таким образом,\n",
        "- Последний в цепочке объект $\\mathbf{x}_T$ получается напрямую из $\\mathbf{x}_0$\n",
        "- Каждый промежуточный зашумленный объект $\\mathbf{x}_{t-1}$ получается из исходного $\\mathbf{x}_0$ и следующего за ним $\\mathbf{x}_t$!\n",
        "\n",
        "Сразу же появляется вопрос, а как тогда определить $q_\\sigma(\\mathbf{x}_{t-1} | \\mathbf{x}_t, \\mathbf{x}_0)$?\n",
        "\n",
        "Давайте отталкиваться от того, что мы уже видели, а именно рассмотрим Гауссовское распределение. Причем важно, что мы сделаем среднее как линейную функцию от $\\mathbf{x}_0$ и $\\mathbf{x}_t$. Запишется это требование следующим образом:\n",
        "$$\n",
        "    q_\\sigma(\\mathbf{x}_{t-1} | \\mathbf{x}_t, \\mathbf{x}_0) = \\mathcal{N}\\left( w_0 \\mathbf{x}_0 + w_t \\mathbf{x}_t + b, \\sigma_t^2 \\mathbf{I} \\right)\n",
        "$$\n",
        "\n",
        "**Вопрос.** Как определить $w_0$, $w_t$ и $b$?\n",
        "\n",
        "<details>\n",
        "  <summary><b>Ответ</b></summary>\n",
        "  \n",
        "  <font color='green'>\n",
        "\n",
        "  Идея в том, чтобы сделать $q_\\sigma(\\mathbf{x}_t | \\mathbf{x}_0)$ таким же, как было в DDPM, т.е.\n",
        "$$\n",
        "    q_\\sigma(\\mathbf{x}_t | \\mathbf{x}_0) = \\mathcal{N}\\left( \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1 - \\bar{\\alpha}_t) \\mathbf{I} \\right)\n",
        "$$\n",
        "  </font>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXfDhTRDJz3l"
      },
      "source": [
        "### 1.3. Вывод обратного процесса в DDIM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB-4spMkJz3l"
      },
      "source": [
        "Вывод будем производить последовательно, а именно пусть мы уже доказали, что\n",
        "$$\n",
        "    q_\\sigma(\\mathbf{x}_t | \\mathbf{x}_0) = \\mathcal{N}\\left( \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1 - \\bar{\\alpha}_t) \\mathbf{I} \\right),\n",
        "$$\n",
        "как тогда подобрать $w_0$, $w_t$ и $b$, чтобы обеспечить\n",
        "$$\n",
        "    q_\\sigma(\\mathbf{x}_{t-1} | \\mathbf{x}_0) = \\mathcal{N}\\left( \\sqrt{\\bar{\\alpha}_{t-1}} \\mathbf{x}_0, (1 - \\bar{\\alpha}_{t-1}) \\mathbf{I} \\right)?\n",
        "$$\n",
        "\n",
        "**Идея 1.** Обусловим распределение на $\\mathbf{x}_t$ и проинтегрируем по нему, формально ничего не потеряв и не добавив:\n",
        "$$\n",
        "    q_\\sigma(\\mathbf{x}_{t-1} | \\mathbf{x}_0) = \\int q_\\sigma(\\mathbf{x}_{t-1}, \\mathbf{x}_t | \\mathbf{x}_0) d\\mathbf{x}_t = \\int q_\\sigma(\\mathbf{x}_{t-1} | \\mathbf{x}_t, \\mathbf{x}_0) q_\\sigma(\\mathbf{x}_{t} | \\mathbf{x}_0) d\\mathbf{x}_t.\n",
        "$$\n",
        "\n",
        "**Идея 2.** Будем использовать интересное свойство нормального распределения. А именно, если $p(\\mathbf{x}) = \\mathcal{N}(\\boldsymbol{\\mu}, \\sigma_{\\mathbf{x}}^2\\mathbf{I})$ и $p(\\mathbf{y} | \\mathbf{x}) = \\mathcal{N}(a\\mathbf{x} + b, \\sigma_{\\mathbf{y}}^2\\mathbf{I})$, то\n",
        "$$\n",
        "    p(\\mathbf{y}) = \\int p(\\mathbf{y}, \\mathbf{x}) d\\mathbf{x} = \\int p(\\mathbf{y} | \\mathbf{x}) p(\\mathbf{x}) d\\mathbf{x} = \\mathcal{N}\\left(a \\boldsymbol{\\mu} + b, \\left( \\sigma_{\\mathbf{y}}^2 + a^2 \\sigma_{\\mathbf{x}}^2 \\right) \\mathbf{I}\\right).\n",
        "$$\n",
        "\n",
        "Воспользуемся **Идеей 2** для того, что мы получили в **Идее 1**. Тогда сразу же получим\n",
        "$$\n",
        "    q_\\sigma(\\mathbf{x}_{t-1} | \\mathbf{x}_0) = \\mathcal{N}\\left( w_0 \\mathbf{x}_0 + w_t (\\sqrt{\\bar{\\alpha}}_t \\mathbf{x}_0) + b, \\left( \\sigma_t^2 + w_t^2 (1 - \\bar{\\alpha}_t) \\right) \\mathbf{I}\\right).\n",
        "$$\n",
        "\n",
        "**Вопрос.** Как дальше нужно действовать, чтобы подобрать коэффициенты $w_0$, $w_t$ и $b$?\n",
        "\n",
        "<details>\n",
        "  <summary><b>Ответ</b></summary>\n",
        "  \n",
        "  <font color='green'>Просто приравниваем! При этом начинаем с дисперсии.</font>\n",
        "\n",
        "  <font color='green'>\n",
        "\n",
        "1. Сначала приравниваем дисперсию, т.е.\n",
        "$$\n",
        "    \\sigma_t^2 + w_t^2 (1 - \\bar{\\alpha}_t) = 1 - \\bar{\\alpha}_{t-1},\n",
        "$$\n",
        "откуда выражаем $w_t$ через все остальное:\n",
        "$$\n",
        "    w_t = \\sqrt{\\frac{1 - \\bar{\\alpha}_{t-1} - \\sigma_t^2}{1 - \\bar{\\alpha}_t}}.\n",
        "$$ </font>\n",
        "\n",
        "  <font color='green'>\n",
        "\n",
        "2. А теперь приравниваем средние и подставляем уже найденный $w_t$:\n",
        "$$\n",
        "    w_0 \\mathbf{x}_0 + w_t (\\sqrt{\\bar{\\alpha}}_t \\mathbf{x}_0) + b =  \\sqrt{\\bar{\\alpha}_{t-1}} \\mathbf{x}_0,\n",
        "$$\n",
        "причем для простоты можем приравнять $b = 0$, откуда сразу легко следует\n",
        "$$\n",
        "    w_0 = \\sqrt{\\bar{\\alpha}_{t-1}} - \\sqrt{\\bar{\\alpha}_t} \\sqrt{\\frac{1 - \\bar{\\alpha}_{t-1} - \\sigma_t^2}{1 - \\bar{\\alpha}_t}}.\n",
        "$$</font>\n",
        "  \n",
        "</details>\n",
        "\n",
        "Таким образом, мы получили формулу обратного процесса в DDIM:\n",
        "$$\n",
        "    q_\\sigma(\\mathbf{x}_{t-1} | \\mathbf{x}_t, \\mathbf{x}_0) = \\mathcal{N}\\left( \\sqrt{\\bar{\\alpha}_{t-1}} \\mathbf{x}_0 - \\sqrt{\\bar{\\alpha}_t} \\sqrt{\\frac{1 - \\bar{\\alpha}_{t-1} - \\sigma_t^2}{1 - \\bar{\\alpha}_t}} \\mathbf{x}_0 + \\sqrt{\\frac{1 - \\bar{\\alpha}_{t-1} - \\sigma_t^2}{1 - \\bar{\\alpha}_t}} \\mathbf{x}_t, \\sigma_t^2 \\mathbf{I} \\right),\n",
        "$$\n",
        "или, наконец, финальное выражение:\n",
        "$$\n",
        "    q_\\sigma(\\mathbf{x}_{t-1} | \\mathbf{x}_t, \\mathbf{x}_0) = \\mathcal{N}\\left( \\sqrt{\\bar{\\alpha}_{t-1}} \\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_{t-1} - \\sigma_t^2} \\cdot \\frac{\\mathbf{x}_t - \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0}{\\sqrt{1 - \\bar{\\alpha}_t}}, \\sigma_t^2 \\mathbf{I} \\right),\n",
        "$$\n",
        "причем, подчеркнем, **при любых** $\\sigma_t^2$ гарантированно $q_\\sigma(\\mathbf{x}_t | \\mathbf{x}_0)$ остается таким же, как в DDPM!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn0vkMyPJz3l"
      },
      "source": [
        "Теперь давайте еще раз взглянем на DDPM и DDIM со стороны:\n",
        "\n",
        "**DDPM**\n",
        "\n",
        "- $q(\\mathbf{x}_t | \\mathbf{x}_{t-1})$ определено;\n",
        "- $\\color{purple}{q(\\mathbf{x}_t | \\mathbf{x}_0)}$ и $\\color{olive}{q(\\mathbf{x}_{t-1} | \\mathbf{x}_t, \\mathbf{x}_0)}$ выводятся из $q(\\mathbf{x}_t | \\mathbf{x}_{t-1})$.\n",
        "\n",
        "**DDIM**\n",
        "\n",
        "- $\\color{olive}{q(\\mathbf{x}_{t-1} | \\mathbf{x}_t, \\mathbf{x}_0)}$ определено;\n",
        "- $\\color{purple}{q(\\mathbf{x}_t | \\mathbf{x}_0)}$ и $q(\\mathbf{x}_t | \\mathbf{x}_{t-1})$ выводятся из $\\color{olive}{q(\\mathbf{x}_{t-1} | \\mathbf{x}_t, \\mathbf{x}_0)}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcTnh-jwJz3l"
      },
      "source": [
        "**Вопрос.** Что будет, если мы сделаем $\\sigma_t^2 = 0$?\n",
        "\n",
        "<details>\n",
        "  <summary><b>Ответ</b></summary>\n",
        "\n",
        "  <font color='green'>Прямой и обратный процессы станут детерминистичными!</font>\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "А теперь давайте подумаем, что это означает и почему это может быть хорошо.\n",
        "\n",
        "1. Для **одинаковых** $\\mathbf{x}_T \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ мы всегда получаем **один и тот же** $\\mathbf{x}_0$.\n",
        "\n",
        "2. Благодаря такой консистентности DDIM, мы можем осуществлять **семантическую интерполяцию** между изображениями, манипулируя именно **латентными переменными**, то есть тем шумом, с которого мы стартуем.\n",
        "\n",
        "3. Как мы обсудим позже, детерминистичность DDIM также позволяет использовать **меньшее число шагов сэмплирования без потери качества**! А это критически важно для диффузионных моделей, которые в оригинале (DDPM) имеют очень медленное сэмплирование.\n",
        "\n",
        "4. Наконец, в продолжение предыдущего пункта, если мы начинаем с одного и того же шума $\\mathbf{x}_T \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ и генерируем сэмплы с **различным числом шагов**, например 50 и 1000, то эти сэмплы практически **не будут отличаться по семантике**."
      ],
      "metadata": {
        "id": "JnuE55604xtw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZQFBBTyJz3l"
      },
      "source": [
        "### 1.4. Обучение DDIM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вопрос.** Как Вам кажется, нужно ли для каждого выбранного расписания дисперсий $\\{\\sigma_1, \\ldots, \\sigma_T\\}$ обучать отдельную модель DDIM?\n",
        "\n",
        "<details>\n",
        "  <summary><b>Ответ</b></summary>\n",
        "\n",
        "<font color='green'> Оказывается, что нет! И эта особенность позволяет использовать DDIM намного проще. Мало того, вся прелесть DDIM заключается в том, что на самом деле его **обучение ничем не отличается** от обучения DDPM! Именно такую теорему доказывают авторы статьи. </font>\n",
        "\n",
        "<font color='green'> Если говорить конкретно, то можно честно выписать и положить перед собой следующие две формулы:\n",
        "- Функция потерь $L_{\\gamma}(\\hat{\\boldsymbol{\\epsilon}}_\\theta)$ в модели DDPM:\n",
        "$$\n",
        "    L_{\\gamma}(\\hat{\\boldsymbol{\\epsilon}}_\\theta) = \\sum_{t=1}^{T} \\gamma_t \\mathbb{E}_{\\mathbf{x}_0 \\sim q(\\mathbf{x}_0), \\boldsymbol{\\epsilon}_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})} \\left[ \\left\\| \\hat{\\boldsymbol{\\epsilon}}_{\\boldsymbol{\\theta}}(\\sqrt{\\alpha_t} \\mathbf{x}_0 + \\sqrt{1 - \\alpha_t} \\boldsymbol{\\epsilon}_t, t) - \\boldsymbol{\\epsilon}_t \\right \\|_2^2 \\right].\n",
        "$$\n",
        "- Функция потерь $J_{\\sigma}(\\hat{\\boldsymbol{\\epsilon}}_\\theta)$ в модели DDIM:\n",
        "$$\n",
        "    L_{\\gamma}(\\hat{\\boldsymbol{\\epsilon}}_\\theta) \\equiv \\sum_{t=1}^{T} \\frac{1}{2 d \\sigma_t^2 \\alpha_t} \\mathbb{E}_{\\mathbf{x}_0 \\sim q(\\mathbf{x}_0), \\boldsymbol{\\epsilon}_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})} \\left[ \\left\\| \\hat{\\boldsymbol{\\epsilon}}_{\\boldsymbol{\\theta}}(\\sqrt{\\alpha_t} \\mathbf{x}_0 + \\sqrt{1 - \\alpha_t} \\boldsymbol{\\epsilon}_t, t) - \\boldsymbol{\\epsilon}_t \\right \\|_2^2 \\right],\n",
        "$$</font>\n",
        "<font color='green'>где мы обозначили за $d$ размерность $\\mathbf{x}$, а знак $\\equiv$ означает отбрасывание слагаемых, не зависящих от $\\hat{\\boldsymbol{\\epsilon}}_\\theta$.</font>\n",
        "\n",
        "<font color='green'>\n",
        "\n",
        "И можно видеть прекрасную вещь: действительно, полагая\n",
        "$$\n",
        "    \\gamma_t = \\frac{1}{2 d \\sigma_t^2 \\alpha_t},\n",
        "$$\n",
        "мы можем увидеть, что\n",
        "$$\n",
        "    J_{\\sigma} = L_{\\gamma} + \\mathrm{const}.\n",
        "$$</font>\n",
        "\n",
        "<font color='green'>И тут мы понимаем, что нам **не нужно переучивать предсказатель шума**. Следовательно, предсказатель шума $\\hat{\\boldsymbol{\\epsilon}}_\\theta(\\mathbf{x}_t, t)$, обученный для DDPM, может быть напрямую использован в обратном процессе DDIM! </font>\n",
        "</details>"
      ],
      "metadata": {
        "id": "ATjI-A2DBJj4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uvK5X4EJz3l"
      },
      "source": [
        "### 1.5. Обратные процессы в DDPM и DDIM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ULScLViJz3l"
      },
      "source": [
        "А теперь давайте еще раз пробежимся по тому, как выглядят обратные процессы в обоих методах.\n",
        "\n",
        "**DDPM**\n",
        "\n",
        "Формула обратного процесса:\n",
        "\n",
        "$$\n",
        "    q(\\mathbf{x}_{t-1} | \\mathbf{x}_t, \\mathbf{x}_0) = \\mathcal{N}\\left( \\frac{\\sqrt{{\\alpha}_t} (1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t}{1 - \\bar{\\alpha}_t} \\mathbf{x}_0, \\left( \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t \\right) \\mathbf{I} \\right)          \n",
        "$$\n",
        "\n",
        "Для каждого шага $t = T, \\ldots, 1$, повторяем:\n",
        "\n",
        "1. Подсчитать $\\mathbf{x}_{0|t} = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}} \\left( \\mathbf{x}_t - \\sqrt{1 - \\bar{\\alpha}_t} \\hat{\\boldsymbol{\\epsilon}}_\\theta(\\mathbf{x}_t, t) \\right)$\n",
        "2. Подсчитать $\\color{purple}{\\tilde{\\boldsymbol{\\mu}}(\\mathbf{x}_t, \\mathbf{x}_0) = \\frac{\\sqrt{{\\alpha}_t} (1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t}{1 - \\bar{\\alpha}_t} \\mathbf{x}_{0|t}}$\n",
        "3. Просэмплировать $\\mathbf{z}_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$\n",
        "4. Подсчитать $\\mathbf{x}_{t-1} = \\tilde{\\boldsymbol{\\mu}}(\\mathbf{x}_t, \\mathbf{x}_0) + \\color{olive}{\\sqrt{\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t}} \\mathbf{z}_t$\n",
        "\n",
        "**DDIM**\n",
        "\n",
        "<figure align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=1dAr0q1UUXoL1E7TVKrvvE9URA0XYR9D_\" alt=\"ddim\" width=\"700\"/>\n",
        "    <figcaption> Обратный процесс DDIM. Источник: <a href=\"https://arxiv.org/abs/2006.11239\">Ho et al. 2020</a> </figcaption>\n",
        "</figure>\n",
        "\n",
        "Формула обратного процесса:\n",
        "\n",
        "$$\n",
        "    q_\\sigma(\\mathbf{x}_{t-1} | \\mathbf{x}_t, \\mathbf{x}_0) = \\mathcal{N}\\left( \\sqrt{\\bar{\\alpha}_{t-1}} \\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_{t-1} - \\sigma_t^2} \\cdot \\frac{\\mathbf{x}_t - \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0}{\\sqrt{1 - \\bar{\\alpha}_t}}, \\sigma_t^2 \\mathbf{I} \\right)\n",
        "$$\n",
        "\n",
        "Для каждого шага $t = T, \\ldots, 1$, повторяем:\n",
        "\n",
        "1. Подсчитать $\\mathbf{x}_{0|t} = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}} \\left( \\mathbf{x}_t - \\sqrt{1 - \\bar{\\alpha}_t} \\hat{\\boldsymbol{\\epsilon}}_\\theta(\\mathbf{x}_t, t) \\right)$\n",
        "2. Подсчитать $\\color{purple}{\\tilde{\\boldsymbol{\\mu}}(\\mathbf{x}_t, \\mathbf{x}_0) = \\sqrt{\\bar{\\alpha}_{t-1}} \\mathbf{x}_{0|t} + \\sqrt{1 - \\bar{\\alpha}_{t-1} - \\sigma_t^2} \\cdot \\frac{\\mathbf{x}_t - \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_{0|t}}{\\sqrt{1 - \\bar{\\alpha}_t}}}$\n",
        "3. Просэмплировать $\\mathbf{z}_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$\n",
        "4. Подсчитать $\\mathbf{x}_{t-1} = \\tilde{\\boldsymbol{\\mu}}(\\mathbf{x}_t, \\mathbf{x}_0) + \\color{olive}{\\sigma_t} \\mathbf{z}_t$\n",
        "\n",
        "И на самом деле оказывается, что **DDIM — это обобщение DDPM**!\n",
        "\n",
        "А именно, они совпадают, если\n",
        "$$\n",
        "    \\sigma_t^2 = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t.\n",
        "$$\n",
        "\n",
        "В таком случае DDIM представляет собой марковский процесс!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewMhlU4iJz3l"
      },
      "source": [
        "### 1.6. Контроль стохастичности"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9ijuErIJz3l"
      },
      "source": [
        "Учитывая похожесть DDIM и DDPM, обычно вводят следующую величину, а именно перепараметризовывают\n",
        "$$\n",
        "    \\sigma_t = \\color{olive}{\\eta} \\sqrt{\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t},\n",
        "$$\n",
        "тогда\n",
        "- при $\\eta = 0$ получаем детерминистичный процесс, то есть для одного и того же $\\mathbf{x}_T \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ мы будем всегда получать одинаковые $\\mathbf{x}_0$;\n",
        "- при $\\eta = 1$ то же самое, что и DDPM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5ZzcEZUJz3l"
      },
      "source": [
        "А сейчас давайте вернемся к главной проблеме DDPM.\n",
        "\n",
        "**Вопрос.** Что за проблема?\n",
        "\n",
        "<details>\n",
        "  <summary><b>Ответ</b></summary>\n",
        "  \n",
        "  <font color='green'>Очень медленное сэмплирование.</font>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5lzM61lJz3m"
      },
      "source": [
        "### 1.7. Ускоренный процесс сэмплирования"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4GCSs1nJz3m"
      },
      "source": [
        "Обратный процесс, с помощью которого производится сэмплирование как в DDPM, так и в DDIM, с последовательностью шагов $t \\in [1, 2, \\ldots, T]$ имеет следующий вид:\n",
        "$$\n",
        "    p_\\theta(\\mathbf{x}_{0:T}) = p_\\theta(\\mathbf{x}_T) \\prod_{t=1}^T p_\\theta(\\mathbf{x}_{t-1} | \\mathbf{x}_t).\n",
        "$$\n",
        "\n",
        "А как можно ускорить такой процесс? Давайте рассмотрим **подпоследовательность** временных шагов $\\tau = [\\tau_1, \\tau_2, \\ldots, \\tau_S]$.\n",
        "\n",
        "Тогда обратный процесс для подпоследовательности можно записать как\n",
        "$$\n",
        "    p_\\theta(\\mathbf{x}_\\tau) = p_\\theta(\\mathbf{x}_T) \\prod_{t=1}^{\\color{olive}{S}} p_\\theta(\\mathbf{x}_{\\color{olive}{\\tau_{i-1}}} | \\mathbf{x}_{\\color{olive}{\\tau_{i}}}).\n",
        "$$\n",
        "\n",
        "Возникает вопрос: в чем же была проблема использовать ускоренный процесс сэмплирования для DDPM?\n",
        "\n",
        "Оказывется, что **маленькое число** шагов приводило к существенному **ухудшению качества** генерации.\n",
        "\n",
        "Однако ухудшения качества **удалось избежать** при использовании DDIM, как только обратный процесс стал более **детерминистичным**. Интуитивно это можно объяснить для себя таким образом:\n",
        "- Если мы используем **стохастичное сэмплирование**, то на каждом шаге мы добавляем случайный шум, причем если шагов мало, то и **шум будет больше каждый раз**.\n",
        "- Поскольку наша модель предсказания шума все-таки не идеальна, складываясь каждый раз с добавленным шумом (см. предыдущий пункт), полученное предсказание будет становиться **все менее и менее точным**.\n",
        "- В случае же **детерминистичного сэмплирования** накопления лишнего шума не происходит, потому мы и можем использовать меньшее число шагов **без потери качества**.\n",
        "\n",
        "Давайте рассмотрим пример из статьи DDIM, который приводят авторы. Они взяли обученную модель DDPM и смотрели на то, как различные сэмплирования влияют на качество генераций.\n",
        "\n",
        "Измерялась метрика FID (чем меньше, тем лучше) при варьировании\n",
        "- $\\eta$, отвечающей за стохастичность;\n",
        "- $S$, отвечающей за число шагов в процессе сэмплирования.\n",
        "\n",
        "<figure align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=1y7RcQG-9Y5reM0M7x9xrK1VIGJNrz3G9\" alt=\"ddpm-vs-ddim\" width=\"500\"/>\n",
        "    <figcaption> Генерация изображений CIFAR10 и CelebA измеряется в FID. η=1 соответствует DDPM, а η=0 соответствует DDIM. Источник: <a href=\"https://arxiv.org/abs/2010.02502\">Song et al. 2020</a> </figcaption>\n",
        "</figure>\n",
        "\n",
        "Из таблицы можно сделать следующие выводы:\n",
        "- Когда $\\eta = 1$ (DDPM), качество быстро ухудшается, когда $S$ уменьшается с 1000 до 10.\n",
        "- Когда $\\eta = 0$ (детерминистичный DDIM),\n",
        "  - качество не такое плохое даже при $S = 10$;\n",
        "  - качество при $S = 1000$ оказывается даже лучше, чем у DDPM!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.8. Реализация обучения и сэмплирования DDIM"
      ],
      "metadata": {
        "id": "AfpWWg7KT34b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как мы уже обсудили, обучение DDIM ничем не отличается от того же для DDPM. Поэтому мы рассмотрим пример кода, который был продемонстрирован на семинаре №3. Единственным отличием будет лишь реализация класса `NoiseScheduler`, который описывает процесс зашумления и расшумления модели."
      ],
      "metadata": {
        "id": "0tL6FOKoT-Hu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Итак, давайте приступим. Для начала импортируем все необходимые библиотеки."
      ],
      "metadata": {
        "id": "KGJN3FYvUYrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import init\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import TensorDataset\n",
        "from sklearn.datasets import make_moons\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "ApnPzzO9Um2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разберемся с данными, на которых будем учить нейросеть. Для простоты выбираем совсем несложную задачу — будем аппроксимировать распределение точек на плоскости, которое внешне похоже на две луны."
      ],
      "metadata": {
        "id": "Dv9qnofeW01c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def moons_dataset(n=8000):\n",
        "    X, _ = make_moons(n_samples=n, random_state=42, noise=0.03)\n",
        "    X[:, 0] = (X[:, 0] + 0.3) * 2 - 1\n",
        "    X[:, 1] = (X[:, 1] + 0.3) * 3 - 1\n",
        "    return TensorDataset(torch.from_numpy(X.astype(np.float32)))"
      ],
      "metadata": {
        "id": "UPhtiaUcXmXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Провизуализируем данные, просэмплировав их из датасета."
      ],
      "metadata": {
        "id": "C72OQAgBp_e8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = moons_dataset()\n",
        "sample = torch.stack([sample[0] for sample in dataset])\n",
        "\n",
        "plt.scatter(sample[:,0], sample[:,1], edgecolor=\"black\", label=\"Real Data\", color=\"green\")\n",
        "plt.grid(alpha=0.2)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PEhLTePsqAQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=1ujiiij0ZgcwHbpboaEXItXh3R8GRUune\" alt=\"moons\" width=\"400\"/>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "mrAu6HFLqCK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализуем класс `PositionalEmbedding`, который будет создавать эмбеддинги для временного шага $t$ в нашем процессе. Напомним, что это нужно потому, что наша нейросеть $\\hat{\\boldsymbol{\\epsilon}}_{\\theta}(\\mathbf{x}_t, t)$ имеет два входа: 1) зашумленный объект на шаге $t$; и 2) сам временной шаг $t$. Создание эмбеддингов нужно для того, чтобы модель лучше понимала текущее временное состояние."
      ],
      "metadata": {
        "id": "enBf4eFyUnsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SinusoidalEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    В качестве эмбеддингов будем использовать синусоидальные.\n",
        "    \"\"\"\n",
        "    def __init__(self, size: int, scale: float = 1.0):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = x * self.scale\n",
        "        half_size = self.size // 2\n",
        "        emb = torch.log(torch.Tensor([10000.0])) / (half_size - 1)\n",
        "        emb = torch.exp(-emb * torch.arange(half_size))\n",
        "        emb = x.unsqueeze(-1) * emb.unsqueeze(0)\n",
        "        emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=-1)\n",
        "        return emb\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, size: int, type: str, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer = SinusoidalEmbedding(size, **kwargs)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.layer(x)"
      ],
      "metadata": {
        "id": "sd9zyZq_VdX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь реализуем класс `MLP` непосредственно для нашей модели $\\hat{\\boldsymbol{\\epsilon}}_\\theta$. Поскольку данные, на которых мы будем обучать нашу модель, простые — точки на плоскости, мы возьмем очень простую архитектуру — буквально Multi Layer Perceptron (MLP)."
      ],
      "metadata": {
        "id": "E2rc1HVbVn58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\"\n",
        "    Реализация одного блока внутри MLP.\n",
        "    \"\"\"\n",
        "    def __init__(self, size: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ff = nn.Linear(size, size)\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return x + self.act(self.ff(x))\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Модель для предсказания шума.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size: int = 128, hidden_layers: int = 3, emb_size: int = 128,\n",
        "                 time_emb: str = \"sinusoidal\", input_emb: str = \"sinusoidal\"):\n",
        "        super().__init__()\n",
        "\n",
        "        self.time_mlp = PositionalEmbedding(emb_size, time_emb)\n",
        "        self.input_mlp1 = PositionalEmbedding(emb_size, input_emb, scale=25.0)\n",
        "        self.input_mlp2 = PositionalEmbedding(emb_size, input_emb, scale=25.0)\n",
        "\n",
        "        concat_size = len(self.time_mlp.layer) + \\\n",
        "            len(self.input_mlp1.layer) + len(self.input_mlp2.layer)\n",
        "        layers = [nn.Linear(concat_size, hidden_size), nn.GELU()]\n",
        "        for _ in range(hidden_layers):\n",
        "            layers.append(Block(hidden_size))\n",
        "        layers.append(nn.Linear(hidden_size, 2))\n",
        "        self.joint_mlp = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        x1_emb = self.input_mlp1(x[:, 0])\n",
        "        x2_emb = self.input_mlp2(x[:, 1])\n",
        "        t_emb = self.time_mlp(t)\n",
        "        x = torch.cat((x1_emb, x2_emb, t_emb), dim=-1)\n",
        "        x = self.joint_mlp(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "z6IEfRUCWOzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Наконец, перейдем к тому, что будет отличать нашу модель от той, что была на семинаре №3 — реализации процесса зашумления и расшумления. А именно, нам потребуются следующие изменения:\n",
        "1. С DDIM мы можем контролировать стохастичность, используя параметр $\\eta$. Добавим этот функционал в `__init__()`, вводя `eta`.\n",
        "2. Добавим метод `set_timesteps()` для того, чтобы можно было изменять число шагов сэмплирования на инференсе.\n",
        "3. Изменим метод `get_variance()`, в котором учтем, что шаги сэмплирования могут быть выбраны с пропусками.\n",
        "4. В методе `q_posterior()` изменим формулу, чтобы соответствовать процессу DDIM."
      ],
      "metadata": {
        "id": "NRxey5zVWrgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NoiseScheduler():\n",
        "    def __init__(self,\n",
        "                 num_timesteps: int = 1000,\n",
        "                 beta_start: float = 0.0001,\n",
        "                 beta_end: float = 0.02,\n",
        "                 beta_schedule: str = \"linear\",\n",
        "                 eta: float = 0.0,\n",
        "                 set_alpha_to_one: bool = True):\n",
        "\n",
        "        self.num_timesteps = num_timesteps\n",
        "        self.eta = eta\n",
        "\n",
        "        if beta_schedule == \"linear\":\n",
        "            self.betas = torch.linspace(\n",
        "                beta_start, beta_end, num_timesteps, dtype=torch.float32)\n",
        "\n",
        "        elif beta_schedule == \"quadratic\":\n",
        "            self.betas = torch.linspace(\n",
        "                beta_start ** 0.5, beta_end ** 0.5, num_timesteps, dtype=torch.float32) ** 2\n",
        "\n",
        "        self.alphas = 1.0 - self.betas\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
        "\n",
        "        # required for self.add_noise\n",
        "        self.sqrt_alphas_cumprod = self.alphas_cumprod ** 0.5\n",
        "        self.sqrt_one_minus_alphas_cumprod = (1 - self.alphas_cumprod) ** 0.5\n",
        "\n",
        "        # required for reconstruct_x0\n",
        "        self.sqrt_inv_alphas_cumprod = torch.sqrt(1 / self.alphas_cumprod)\n",
        "        self.sqrt_inv_alphas_cumprod_minus_one = torch.sqrt(\n",
        "            1 / self.alphas_cumprod - 1)\n",
        "\n",
        "        # At every step in DDIM, we are looking into the previous alphas_cumprod\n",
        "        # For the final step, there is no previous alphas_cumprod because we are already at 0\n",
        "        # `set_alpha_to_one` decides whether we set this parameter simply to one or\n",
        "        # whether we use the final alpha of the \"non-previous\" one.\n",
        "        self.final_alpha_cumprod = torch.tensor(1.0) if set_alpha_to_one else self.alphas_cumprod[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_timesteps\n",
        "\n",
        "    def add_noise(self, x_start, x_noise, timestep):\n",
        "        s1 = self.sqrt_alphas_cumprod[timestep]\n",
        "        s2 = self.sqrt_one_minus_alphas_cumprod[timestep]\n",
        "\n",
        "        s1 = s1.reshape(-1, 1)\n",
        "        s2 = s2.reshape(-1, 1)\n",
        "\n",
        "        return s1 * x_start + s2 * x_noise\n",
        "\n",
        "    def reconstruct_x0(self, x_t, timestep, noise):\n",
        "        s1 = self.sqrt_inv_alphas_cumprod[timestep]\n",
        "        s2 = self.sqrt_inv_alphas_cumprod_minus_one[timestep]\n",
        "\n",
        "        s1 = s1.reshape(-1, 1)\n",
        "        s2 = s2.reshape(-1, 1)\n",
        "\n",
        "        return s1 * x_t - s2 * noise\n",
        "\n",
        "    def set_timesteps(self, num_inference_steps):\n",
        "        self.num_inference_steps = num_inference_steps\n",
        "        timesteps = (\n",
        "            np.linspace(0, self.num_timesteps - 1, num_inference_steps)\n",
        "            .round()[::-1]\n",
        "            .copy()\n",
        "            .astype(np.int64)\n",
        "        )\n",
        "        self.timesteps = torch.from_numpy(timesteps)\n",
        "\n",
        "    def get_variance(self, timestep, prev_timestep):\n",
        "        if timestep == 0:\n",
        "            return 0\n",
        "        alpha_prod_t = self.alphas_cumprod[timestep]\n",
        "        alpha_prod_t_prev = self.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else self.final_alpha_cumprod\n",
        "        beta_prod_t = 1 - alpha_prod_t\n",
        "        beta_prod_t_prev = 1 - alpha_prod_t_prev\n",
        "\n",
        "        variance = (beta_prod_t_prev / beta_prod_t) * (1 - alpha_prod_t / alpha_prod_t_prev)\n",
        "        variance = variance.clip(1e-20)\n",
        "        return variance\n",
        "\n",
        "    def q_posterior(self, pred_epsilon, sample, timestep, prev_timestep):\n",
        "        alpha_prod_t = self.alphas_cumprod[timestep]\n",
        "        alpha_prod_t_prev = self.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else self.final_alpha_cumprod\n",
        "        sqrt_one_minus_alphas_prod_t = self.sqrt_one_minus_alphas_cumprod[timestep]\n",
        "        variance = self.get_variance(timestep, prev_timestep)\n",
        "        sigma_t = self.eta * variance ** (0.5)\n",
        "\n",
        "        pred_original_sample = self.reconstruct_x0(sample, timestep, pred_epsilon)\n",
        "        pred_sample_direction = (1 - alpha_prod_t_prev - sigma_t**2) ** (0.5) * pred_epsilon\n",
        "        mu = alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction\n",
        "\n",
        "        return mu\n",
        "\n",
        "    def step(self, model_output, timestep, sample):\n",
        "        prev_timestep = timestep - self.num_timesteps // self.num_inference_steps\n",
        "        pred_prev_sample = self.q_posterior(model_output, sample, timestep, prev_timestep)\n",
        "\n",
        "        variance = 0\n",
        "        if timestep > 0:\n",
        "            noise = torch.randn_like(model_output)\n",
        "            variance = self.get_variance(timestep, prev_timestep)\n",
        "            sigma_t = self.eta * variance ** (0.5)\n",
        "            variance = sigma_t * noise\n",
        "\n",
        "        pred_prev_sample = pred_prev_sample + variance\n",
        "\n",
        "        return pred_prev_sample"
      ],
      "metadata": {
        "id": "bMo13cfPX4al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "А сейчас, когда мы реализовали все, что нам было нужно, давайте перейдем к самому процессу обучения модели. Сперва зафиксируем гиперпараметры."
      ],
      "metadata": {
        "id": "_gXaGPDumdrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_SAMPLES_DATA = 10_000\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "HIDDEN_SIZE = 128\n",
        "HIDDEN_LAYERS = 3\n",
        "EMBEDDING_SIZE = 128\n",
        "TIME_EMBEDDING = \"sinusoidal\"\n",
        "INPUT_EMEDDING = \"sinusoidal\"\n",
        "\n",
        "NUM_TIMESTEPS = 50\n",
        "NUM_INFERENCE_STEPS = NUM_TIMESTEPS\n",
        "ETA = 0.0\n",
        "BETA_SCHEDULE = 'linear'\n",
        "LR = 5e-4\n",
        "\n",
        "NUM_EPOCHS = 200"
      ],
      "metadata": {
        "id": "IVbLWcxOmksQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задаем данные, модель и оптимизатор."
      ],
      "metadata": {
        "id": "m42-5ctUmoxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = moons_dataset(NUM_SAMPLES_DATA)\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=BATCH_SIZE , shuffle=True, drop_last=True)\n",
        "\n",
        "model = MLP(\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        hidden_layers=HIDDEN_LAYERS,\n",
        "        emb_size=EMBEDDING_SIZE,\n",
        "        time_emb=TIME_EMBEDDING,\n",
        "        input_emb=INPUT_EMEDDING)\n",
        "\n",
        "noise_scheduler = NoiseScheduler(\n",
        "        num_timesteps=NUM_TIMESTEPS,\n",
        "        beta_schedule=BETA_SCHEDULE,\n",
        "        eta=ETA)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=LR,\n",
        "    )"
      ],
      "metadata": {
        "id": "9hMtmDTQmqeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Напомним, что обучение строится следующим образом:\n",
        "- Берем сэмпл из реальных данных $\\mathbf{x}$.\n",
        "- Зашумляем сэмпл в соответствии со случайным моментом времени $t$.\n",
        "- Предсказываем шум, которым он был зашумлен, используя нашу модель $\\hat{\\boldsymbol{\\epsilon}}_\\theta$.\n",
        "- Подставляем полученное значение в функцию потерь и делаем шаг оптимизатора."
      ],
      "metadata": {
        "id": "1YbtGo_1mvpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global_step = 0\n",
        "frames = []\n",
        "losses = []\n",
        "\n",
        "for epoch in tqdm(range(NUM_EPOCHS)):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        batch = batch[0]\n",
        "        noise = torch.randn(batch.shape)\n",
        "        timesteps = torch.randint(\n",
        "            0, noise_scheduler.num_timesteps, (batch.shape[0],)\n",
        "        ).long()\n",
        "\n",
        "        noisy = noise_scheduler.add_noise(batch, noise, timesteps)\n",
        "        noise_pred = model(noisy, timesteps)\n",
        "        loss = F.mse_loss(noise_pred, noise)\n",
        "        loss.backward(loss)\n",
        "\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        losses.append(loss.detach().item())"
      ],
      "metadata": {
        "id": "E67FGUf0nJCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ура, мы обучили модель! А теперь давайте просэмплируем из нее, чтобы посмотреть на то, как мы аппроксимируем реальные данные."
      ],
      "metadata": {
        "id": "_mULIBZJnNdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "timesteps = list(range(len(noise_scheduler)))[::-1]\n",
        "noise_scheduler.set_timesteps(len(noise_scheduler))\n",
        "sample = torch.randn(1024, 2) # sampling from noise\n",
        "\n",
        "for i, t in enumerate(tqdm(timesteps)):\n",
        "    t = torch.from_numpy(np.repeat(t,  1024)).long()\n",
        "    with torch.no_grad():\n",
        "        residual = model(sample, t)\n",
        "    sample = noise_scheduler.step(residual, t[0], sample)"
      ],
      "metadata": {
        "id": "CQ72jWBCnVw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(sample[:,0], sample[:,1], edgecolor=\"black\", label=\"Generated Data\")\n",
        "plt.grid(alpha=0.2)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vVdD_fUmn-Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=1-BSvNVOf8Fcww0i4tfZEi7pW4yFHH0bN\" alt=\"moons-50\" width=\"400\"/>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "7wzXv7AtrESk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видно, просэмплированные с помощью модели данные выглядят так же, как и реальные.\n",
        "\n",
        "**Вопрос.** Что такого мы забыли пронаблюдать, что является ключевой возможностью модели DDIM?\n",
        "\n",
        "<details>\n",
        "  <summary><b>Ответ</b></summary>\n",
        "  \n",
        "  <font color='green'>Быстрое сэмплирование с пропуском шагов!</font>\n",
        "</details>"
      ],
      "metadata": {
        "id": "j9eeJMuhnZ8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте же теперь реализуем и его. Для этого достаточно просто изменить `timesteps`, из которых мы берем каждый временной шаг `t`."
      ],
      "metadata": {
        "id": "adXyyxcUnyUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_INFERENCE_STEPS = 10\n",
        "\n",
        "model.eval()\n",
        "timesteps = list(range(NUM_INFERENCE_STEPS))[::-1]\n",
        "noise_scheduler.set_timesteps(NUM_INFERENCE_STEPS)\n",
        "sample = torch.randn(1024, 2) # sampling from noise\n",
        "\n",
        "for i, t in enumerate(tqdm(timesteps)):\n",
        "    t = torch.from_numpy(np.repeat(t,  1024)).long()\n",
        "    with torch.no_grad():\n",
        "        residual = model(sample, t)\n",
        "    sample = noise_scheduler.step(residual, t[0], sample)"
      ],
      "metadata": {
        "id": "XV_u3nntoADd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(sample[:,0], sample[:,1], edgecolor='black', label=\"Generated Data\")\n",
        "plt.grid(alpha=0.2)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Cv0ghr75nXMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=1dKwuwLLbbVEHYHUoKiVlsRdxNhNis2Xm\" alt=\"moons-10\" width=\"400\"/>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "mA6WprgVrQWC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вот так, сократив число шагов сэмплирования с 50 до 10, мы практически ничего не потеряли: данные опять похожи на реальные."
      ],
      "metadata": {
        "id": "RpItXJQCLfms"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7PMUb_pJz3m"
      },
      "source": [
        "Таким образом, мы подробно рассмотрели DDIM:\n",
        "- Какая идея стояла за выводом прямого и обратного процессов.\n",
        "- Как получить обратный процесс.\n",
        "- Как выглядит сэмплирование.\n",
        "- В чем отличие от DDPM, и в чем схожесть.\n",
        "- Ускоренный процесс сэмплирования с помощью пропуска шагов.\n",
        "- Реализовали обучение модели DDIM и сэмплирование из нее на примере, разобранном в семинаре №3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwAmr1iCYOKp"
      },
      "source": [
        "## 2. Планировщики шума в диффузионных моделях"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TJadrDvJz3m"
      },
      "source": [
        "Итак, мы уже поняли, что проблема с медленным сэмплированием в DDPM может быть решена за счет изменения процедуры получения $\\mathbf{x}_{t-1}$ из $\\mathbf{x}_t$ в обратном процессе, как это происходило в DDIM.\n",
        "Идея DDIM оказалась крайне удачной в том числе и потому, что чисто математически обучение DDIM соответствует обучению DDPM.\n",
        "Ведь в таком случае, поменяв всего лишь процесс расшумления, мы получаем возможность существенного ускорения инференса модели!\n",
        "\n",
        "Однако остается открытым вопрос, можно ли как-то иначе ускорить процесс сэмплирования.\n",
        "Итак, мы переходим к другим методам, позволяющим изменить процессы зашумления и расшумления, чтобы достичь ускорения процесса."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzVH2nmMZ6Tc"
      },
      "source": [
        "### 2.1. Терминология"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPw8KUWkJz3m"
      },
      "source": [
        "Начнем с терминологии.\n",
        "\n",
        "В контексте диффузионных моделей принято использовать такое понятие, как **scheduler** или планировщик (также известный как sampler/solver).\n",
        "\n",
        "Планировщик — это некий алгоритм, который не содержит в себе обучаемых параметров. Он отвечает за то, как именно мы зашумляем изображение.\n",
        "\n",
        "Подробно о планировщиках в генеративных моделях написано в [статье от NVLabs](https://arxiv.org/pdf/2206.00364.pdf), однако мы постараемся составить краткое их описание.\n",
        "\n",
        "1. Итак, scheduler используется и на этапе обучения, и на этапе инференса для восстановления зашумленного изображения.\n",
        "2. Глобально существует два вида планировщиков: с детерминистическим сэмплированием и со стохастическим:\n",
        "   - Первый тип в $N$ шагов решает обыкновенное дифференциальное уравнение (ОДУ) динамики движения изображения к абсолютному шуму.\n",
        "   - Второй тип работает лучше с точки зрения качества генерации. На каждом шаге планировщика к данным добавляется свежий шум, сэмплированный из некоторого распределения. В этом случае уже решается стохастическое дифференциальное уравнения (СДУ).\n",
        "3. Планировщик в паре с нейронной сетью образуют цикл, в котором решается ДУ.\n",
        "\n",
        "Таким образом, планировщик задает правила обработки шума. Различные планировщики имеют разные скорости шумоподавления и компромиссы в отношении качества.\n",
        "\n",
        "**Вопрос.** Откуда два именно таких вида планировщиков: детерминистичные и стохастические?\n",
        "\n",
        "<details>\n",
        "  <summary><b>Ответ</b></summary>\n",
        "  \n",
        "  <font color='green'>Все кроется в математике непрерывного диффузионного процесса!</font>\n",
        "</details>\n",
        "\n",
        "Как бы нам ни хотелось упростить повествование, здесь не обойтись без небольшого напоминания непрерывного диффузионного процесса."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Начнем с того, что в целом процесс изменения во времени некоторого объекта $\\mathbf{x}$ может быть описан двумя принципиально разными подходами.\n",
        "\n",
        "**Обыкновенное дифференциальное уравнение (ОДУ)**\n",
        "\n",
        "$$\n",
        "    \\frac{d\\mathbf{x}(t)}{dt} = \\mathbf{f}(\\mathbf{x}(t), t), \\qquad \\mathbf{x}(0) = \\mathbf{x}_0,\n",
        "$$\n",
        "\n",
        "где функция $\\mathbf{f}(\\mathbf{x}(t), t)$ задает динамику изменения объекта во времени.\n",
        "\n",
        "**Стохастическое дифференциальное уравнение (СДУ)**\n",
        "\n",
        "$$\n",
        "    d\\mathbf{x} = \\mathbf{f}(\\mathbf{x}, t) dt + g(t) d\\mathbf{w}, \\qquad \\mathbf{x}(0) \\sim p_0(\\mathbf{x}) = \\pi(\\mathbf{x}),\n",
        "$$\n",
        "\n",
        "где функция $\\mathbf{f}(\\mathbf{x}(t), t)$ называется функцией **дрифта**, а $g(t)$ называется функцией **диффузии**.\n",
        "\n",
        "При этом в таком процессе плотность вероятности $\\mathbf{x}$ изменяется с течением времени согласно так называемому пути вероятности $p_t(\\mathbf{x}) = p(\\mathbf{x}, t)$.\n",
        "\n",
        "Здесь мы видим одну особенность, которая отличает СДУ от ОДУ.\n",
        "\n",
        "**Вопрос.** Что за слагаемое отличает СДУ от ОДУ?\n",
        "\n",
        "<details>\n",
        "  <summary><b>Ответ</b></summary>\n",
        "  \n",
        "  <font color='green'>Слагаемое $g(t) d\\mathbf{w}$, которое вносит случайность в этот процесс.</font>\n",
        "</details>\n",
        "\n",
        "Здесь через $\\mathbf{w}(t)$ мы обозначили стандартный винеровский процесс.\n",
        "\n",
        "Напомним, что это такой процесс, у которого все приращения независимы и нормальны, то есть $\\mathbf{w}(t) - \\mathbf{w}(s) \\sim \\mathcal{N}(0, (t-s)\\mathbf{I})$.\n",
        "\n",
        "Математически легко показать, что $d\\mathbf{w} = \\mathbf{w}(t + dt) - \\mathbf{w}(t) \\sim \\mathcal{N}(0, \\mathbf{I} \\cdot dt)$, то есть $d\\mathbf{w} = \\boldsymbol{\\epsilon} \\cdot \\sqrt{dt}$, где $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\mathbf{I})$.\n",
        "\n",
        "**Вопрос.** Какое условие на функции дрифта и диффузии приводит СДУ к ОДУ?\n",
        "\n",
        "<details>\n",
        "  <summary><b>Ответ</b></summary>\n",
        "\n",
        "  <font color='green'>$g(t) = 0$.</font>\n",
        "</details>"
      ],
      "metadata": {
        "id": "mBasivNFrZgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "А теперь вспомним из предыдущих занятий, что конкретный вид функций дрифта и диффузии приводит нас именно к диффузионному процессу DDPM, и этот вид следующий:\n",
        "\n",
        "$$\n",
        "    d\\mathbf{x} = -\\frac{1}{2} \\beta(t) \\mathbf{x}(t) dt + \\sqrt{\\beta(t)} \\cdot d\\mathbf{w},\n",
        "$$\n",
        "то есть функции дрифта и диффузии задаются как\n",
        "$$\n",
        "    \\mathbf{f}(\\mathbf{x}, t) = -\\frac{1}{2} \\beta(t) \\mathbf{x}(t), \\qquad g(t) = \\sqrt{\\beta(t)}.\n",
        "$$"
      ],
      "metadata": {
        "id": "SIJxltpWricZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Последние несколько фактов, которые нам пригодятся, позволяют обратить прямой процесс диффузии (написанный выше) и получить обратные СДУ и ОДУ.\n",
        "\n",
        "1. Для каждого СДУ вида $d\\mathbf{x} = \\mathbf{f}(\\mathbf{x}, t) dt + g(t) d\\mathbf{w}$ существует ОДУ с тем же самым путем вероятности $p_t(\\mathbf{x})$:\n",
        "   \n",
        "$$\n",
        "    d\\mathbf{x} = \\left( \\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2} g^2(t) \\nabla_\\mathbf{x} \\log p_t(\\mathbf{x}) \\right) dt.\n",
        "$$\n",
        "\n",
        "2. Для каждого СДУ вида $d\\mathbf{x} = \\mathbf{f}(\\mathbf{x}, t) dt + g(t) d\\mathbf{w}$ существует обратное СДУ:\n",
        "$$\n",
        "    d\\mathbf{x} = \\left( \\mathbf{f}(\\mathbf{x}, t) - \\color{purple}{ g^2(t) \\nabla_\\mathbf{x} \\log p_t(\\mathbf{x})} \\right) dt + \\color{olive}{g(t) d\\mathbf{w}}, \\qquad dt < 0.\n",
        "$$"
      ],
      "metadata": {
        "id": "OplpybtirlTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=1lyIB_qGUevIu4JZ0ridJc2OyvkCzSCGR\" alt=\"sde-inverse\" width=\"600\"/>\n",
        "    <figcaption> Прямой и обратный диффузионные процессы как SDE. Источник: <a href=\"https://arxiv.org/abs/2011.13456\">Song et al. 2020</a> </figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "JpVBOZqBsKm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "То есть для нашего прямого процесса диффузии мы можем\n",
        "1. Выписать обратный процесс диффузии, который тоже будет СДУ.\n",
        "2. Для полученного обратного процесса выписать соотвествующее ему ОДУ.\n",
        "\n",
        "**Вопрос.** Как это можно использовать, если мы уже обучили свою диффузионную модель?\n",
        "\n",
        "<details>\n",
        "  <summary><b>Ответ</b></summary>\n",
        "  \n",
        "  <font color='green'>Обратим внимание на то, что стоит в формуле обратного процесса, а именно на градиент логарифма плотности $\\nabla_\\mathbf{x} \\log p_t(\\mathbf{x})$. Это ведь и есть score-функция нашего зашумленного распределения изображения. И именно ее мы аппроксимируем, используя нейросеть $\\hat{\\boldsymbol{\\epsilon}}_\\theta$!</font>\n",
        "\n",
        "  <font color='green'>Так что же теперь получается? Мы можем\n",
        "1. Обучить нейронную сеть так же, как это было в DDPM.\n",
        "2. Использовать ее для аппроксимации score-функции $\\nabla_{\\mathbf{x}} \\log p_t(\\mathbf{x})$.\n",
        "3. Решая это СДУ (или соответствующее ОДУ), получать сэмпл $\\mathbf{x}_0$.\n",
        "</font>\n",
        "</details>"
      ],
      "metadata": {
        "id": "JGCB1AzsroBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Здесь хочется отметить то, как связаны процессы сэмлирования DDPM и DDIM с решением таких уравнений.\n",
        "\n",
        "На самом деле, все довольно логично и красиво:\n",
        "- Дискретизация обратного **СДУ** для дифффузионного процесса приводит нас к **DDPM-сэмплированию**.\n",
        "- Дискретизация обратного **ОДУ** для дифффузионного процесса приводит нас к **DDIM-сэмплированию**.\n",
        "\n",
        "Это очень хорошо соотносится с тем, что мы получили ранее, — что на самом деле DDIM лишь детерминистичная версия DDPM."
      ],
      "metadata": {
        "id": "-iD5ENNarrGg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оказывается, что люди уже рассмотрели много разных способов описания диффузионного процесса через СДУ.\n",
        "\n",
        "Кроме того, существует уже и множество методов, позволяющих решать такие СДУ и ОДУ наиболее эффективно.\n",
        "\n",
        "Теперь давайте перейдем к тому, что рассмотрим основные такие методы, которые чаще всего применяются на практике."
      ],
      "metadata": {
        "id": "vGqF8KObrtXJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAvxbUh-Rhi-"
      },
      "source": [
        "### 2.2. Таксономия планировщиков"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx5Tde1Ne4J1"
      },
      "source": [
        "Начнем с того, что распределим основные известные планировщики по нескольким категориям. Далее мы еще коснемся большинства из них подробнее."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF3KVvbBe8eg"
      },
      "source": [
        "#### 2.2.1. Old-School ODE solvers\n",
        "\n",
        "Это самые простые планровщики, которые на самом деле были впервые предложены еще **несколько сотен лет назад**. Конечно, это было не в контексте диффузионных моделей, а для решения обыкновенных дифференциальных уравнений (ОДУ/ODE).\n",
        "\n",
        "- Euler — самый что ни на есть простой солвер первого порядка.\n",
        "- Heun — более точный, но медленнее Euler.\n",
        "- LMS (Linear multi-step method) — такой же быстрый, как Euler, но обычно более точный.\n",
        "\n",
        "**Важно** подметить, что все они являются сугубо **детерминистичными**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте в качестве примера разберем реализацию Euler и Heun, как довольно простых методов решения ОДУ.\n",
        "\n",
        "Нас интересует уравнение следующего вида:\n",
        "$$\n",
        "    \\frac{d\\mathbf{x}(t)}{dt} = \\mathbf{f}(\\mathbf{x}(t), t).\n",
        "$$\n",
        "\n",
        "**Euler**\n",
        "\n",
        "Первый метод, Euler, получается просто из разностной аппроксимации этого уравнения. Это метод первого порядка, что означает, что локальная ошибка разностной аппроксимации будет порядка $o(\\Delta t)$.\n",
        "- Заметим, что $d\\mathbf{x} = \\mathbf{x}(t + dt) - \\mathbf{x}(t)$.\n",
        "- Зафиксируем конечное **отрицательное** приращение по времени $dt = - \\Delta t$ (отрицательное потому, что мы берем **обратный** процесс диффузии).\n",
        "- Положим $\\mathbf{x}(t) = \\mathbf{x}_t$ и $\\mathbf{x}(t + dt) = \\mathbf{x}_{t-1}$ для дискретных шагов времени $t$.\n",
        "\n",
        "Тогда мы получаем следующее правило обновления, которое и называется методом Эйлера (Euler):\n",
        "$$\n",
        "    \\mathbf{x}_{t-1} = \\mathbf{x}_t - \\Delta t \\cdot \\mathbf{f}(\\mathbf{x}_t, t).\n",
        "$$\n",
        "\n",
        "**Heun**\n",
        "\n",
        "Второй метод, Heun, уже является **методом второго порядка**. Это означает, что локальная ошибка разностной аппроксимации будет порядка $o((\\Delta t)^2)$.\n",
        "- Для начала используем ту же разностную аппроксимацию, что и в методе Эйлера, тем самым получая приближение к $\\mathbf{x}_{t-1}$.\n",
        "- Однако на этом не останавливаемся и заменяем $\\mathbf{f}(\\mathbf{x}_t, t)$ на усредненное по двум точкам значение:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    \\tilde{\\mathbf{x}}_{t-1} &= \\mathbf{x}_t - \\Delta t \\cdot \\mathbf{f}(\\mathbf{x}_t, t), \\\\\n",
        "    \\mathbf{x}_{t-1} &= \\mathbf{x}_t - \\frac{\\Delta t}{2} \\cdot \\left[ \\mathbf{f}(\\mathbf{x}_t, t) + \\mathbf{f}(\\tilde{\\mathbf{x}}_{t-1}, t-1) \\right]. \\\\\n",
        "\\end{aligned}\n",
        "$$"
      ],
      "metadata": {
        "id": "6dTfH1TTs081"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перейдем к их реализации для решения простого дифференциального уравнения для экспоненты:\n",
        "$$\n",
        "    \\frac{dx}{dt} = x, \\qquad x(0) = 1.\n",
        "$$\n",
        "\n",
        "Его решением, как нетрудно заметить, является функция $x(t) = e^t$. Посмотрим, какое решение выдаст каждый из рассмотренных методов."
      ],
      "metadata": {
        "id": "Mfh47aW9s4Fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the ODE function\n",
        "def f(t, x):\n",
        "    return x\n",
        "\n",
        "# Euler method\n",
        "def euler_method(f, x0, t_range, h):\n",
        "    x = [x0]\n",
        "    t = [0]\n",
        "    for i in range(1, t_range):\n",
        "        x.append(x[-1] + h * f(t[-1], x[-1]))\n",
        "        t.append(t[-1] + h)\n",
        "    return t, x\n",
        "\n",
        "# Heun method\n",
        "def heun_method(f, x0, t_range, h):\n",
        "    x = [x0]\n",
        "    t = [0]\n",
        "    for i in range(1, t_range):\n",
        "        k1 = f(t[-1], x[-1])\n",
        "        k2 = f(t[-1] + h, x[-1] + h * k1)\n",
        "        x.append(x[-1] + h * (k1 + k2) / 2)\n",
        "        t.append(t[-1] + h)\n",
        "    return t, x\n",
        "\n",
        "# Parameters\n",
        "x0 = 1\n",
        "t_range = 100\n",
        "h = 0.1 # это шаг дискретизации\n",
        "\n",
        "# Solve using Euler method\n",
        "t_euler, x_euler = euler_method(f, x0, t_range, h)\n",
        "\n",
        "# Solve using Heun method\n",
        "t_heun, x_heun = heun_method(f, x0, t_range, h)\n",
        "\n",
        "# Exact solution\n",
        "t_exact = np.linspace(0, t_range * h, t_range)\n",
        "x_exact = np.exp(t_exact)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(t_euler, x_euler, label='Euler Method', linestyle='--')\n",
        "plt.plot(t_heun, x_heun, label='Heun Method', linestyle='-.')\n",
        "plt.plot(t_exact, x_exact, label='Exact Solution', linestyle='-')\n",
        "plt.xlabel('Time (t)')\n",
        "plt.ylabel('x(t)')\n",
        "plt.title('Comparison of Euler and Heun Methods')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HYTTrZwYs7sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=11Xt2uUJ0TpVZ6suPmC2rR26s7V29iEM9\" alt=\"euler-heun\" width=\"400\"/>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "P-I6TXZEs_Lf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно видеть, что метод Euler отстает по качеству от метода Heun.\n",
        "\n",
        "Аналогичным образом эти методы могут применяться и для решения более сложных дифференциальных уравнений, в частности и DDPM.\n",
        "\n",
        "А теперь давайте перейдем к другим, более навороченным методам, на которых уже сфокусируемся менее детально."
      ],
      "metadata": {
        "id": "r-Oo1yTytBaC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV_lpi0Ce-1e"
      },
      "source": [
        "#### 2.2.2. Ancestral samplers\n",
        "\n",
        "Так называемые анкестральные (ancestral) сэмплеры. Идея в том, что, в отличие от обычных ODE solvers, они уже не являются детерминистичными за счет внесения шума в получаемый сэмпл. Причем этот шум вносится всегда, из-за чего такие солверы **не сходятся** (продолжают изменять изображение даже при большом числе шагов).\n",
        "\n",
        "- Euler a\n",
        "- DPM2 a\n",
        "- DPM++ 2S a\n",
        "- DPM++ 2S a Karras\n",
        "\n",
        "Приписка Karras здесь отсылает нас к [статье](https://arxiv.org/abs/2206.00364), в которой авторы предложили делать расписание шума еще более маленьким в конце процесса."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA7MU5CpfBQP"
      },
      "source": [
        "#### 2.2.3. DDIM и PLMS\n",
        "\n",
        "Ранее мы уже обсудили, что собой представляет DDIM. Отметим лишь, что PLMS — это более новая и быстрая альтернатива для DDIM. По большому счету, эти двое уже давно устарели и сейчас уже не так широко применяются."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AjUZxvkfDqu"
      },
      "source": [
        "#### 2.2.4. DPM и DPM++\n",
        "\n",
        "DPM (Diffusion probabilistic model solver) и DPM++ представляют целое семейство солверов схожей архитектуры.\n",
        "\n",
        "- DPM и DPM2 очень похожи между собой, разве что DPM2 является методом 2-го порядка (а потому более точный, но и более долгий).\n",
        "- DPM++ является усовершенствованием DPM.\n",
        "- DPM adaptive подстраивает размер шага адаптивным образом. Это может замедлить процесс, поскольку не гарантирует окончание за определенное число шагов сэмплирования."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kYaerWBR1hY"
      },
      "source": [
        "#### 2.2.5. UniPC\n",
        "\n",
        "UniPC (Unified Predictor-Corrector) был вдохновлен методом predictor-corrector, который широко известен и применим при решении ОДУ. Благодаря этому методу можно генерировать качественные изображения всего за 5—10 шагов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBidxrLSW6A5"
      },
      "source": [
        "### 2.3. Какой солвер выбрать?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvR4DrJMfKuj"
      },
      "source": [
        "> \"Что лучше: Ferrari или Jeep?\"\n",
        ">\n",
        "> Очевидно, ответ зависит от того, собираетесь вы ехать по бездорожью или нет, правда же?\n",
        "\n",
        "Так и в вопросе о том, какой планировщик является самым лучшим. В зависимости от того, что вы хотите получить в итоге, вам может понадобиться использовать различные варианты. Рассмотрим основные наши желания."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOp5GvUNfMnH"
      },
      "source": [
        "#### 2.3.1. Качество изображения\n",
        "\n",
        "Если вы стремитесь получить наилучшее качество итогового изображения, стоит обратить внимание на сходимость методов. Как ранее обсуждалось, анкестральные методы не сходятся, так что сразу их отметаем. Если Ввы при этом не хотите ждать несколько сотен шагов, то и DDIM тоже отпадает. Heun или LMS Karras обычно показывают хорошие результаты, но лучше использовать DPM++ 2M или его Karras-версию.\n",
        "\n",
        "Вы также можете попробовать DPM adaptive, если никуда не спешите, или же UniPC, если все-таки экономите время.\n",
        "\n",
        "С упомянутыми сэмплерами получится достичь хороших результатов генерации всего за 20—30 шагов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GkBSe9ZfO95"
      },
      "source": [
        "#### 2.3.2. Скорость генерации\n",
        "\n",
        "Если вы тестируете различные промпты и не хотите тратить много времени на ожидание, вам определенно стоит обратить внимание на DPM++ 2M или UniPC с небольшим числом шагов.\n",
        "\n",
        "Всего 10—15 шагов хватит для получения довольно неплохих результатов.\n",
        "\n",
        "Если вы все-таки не особенно волнуетесь за воспроизводимость, то можете попробовать и Euler A, быстрый и довольно качественный анкестральный сэмплер."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHbx-i8ZW-ZX"
      },
      "source": [
        "#### 2.3.3. Креативность и гибкость\n",
        "\n",
        "Здесь хотелось бы отметить анкестральные и стохастичные сэмплеры. Их проблема (но одновременно и достоинство, смотря как воспринимать) заключается в том, что если на 40-м шаге вы получили хороший сэмпл, то на 50-м ничего не мешает получить такой же, а то и хуже. Эта лотерея делает такие сэмплеры более «креативными», поскольку вам определенно потребуется постоянно менять число шагов для получения желаемого результата.\n",
        "\n",
        "Ну и, конечно, Euler A и DPM++ SDE Karras здесь особенно выделяются. Попробуйте сгенерировать изображения за 15, 20, 25 шагов и посмотрите, что будет получаться."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SolKakZcZshI"
      },
      "source": [
        "### 2.4. Реализация в коде"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqmNzWV9Z2KD"
      },
      "source": [
        "Как и большинство других методов для диффузионных моделей, планировщики широко представлены в библиотеке [🤗 Diffusers](https://huggingface.co/docs/diffusers/api/schedulers/overview).\n",
        "\n",
        "Все schedulers наследуются от базового класса [`SchedulerMixin`](https://huggingface.co/docs/diffusers/v0.31.0/en/api/schedulers/overview#diffusers.SchedulerMixin), в котором имплементированы базовые низкоуровневые операции.\n",
        "\n",
        "На выходе scheduler возвращает `SchedulerOutput`, который содержит в себе единственный аргумент — `prev_sample`, то есть сэмпл для предыдущего момента времени.\n",
        "\n",
        "[`KarrasDiffusionSchedulers`](https://github.com/huggingface/diffusers/blob/a69754bb879ed55b9b6dc9dd0b3cf4fa4124c765/src/diffusers/schedulers/scheduling_utils.py#L32) представляют собой широкое обобщение сэмплеров из линейки 🤗 Diffusers. Солверы этого класса отличаются своей стратегией сэмплирования шума, типом сети и масштабирования, стратегией обучения и тем, как взвешивается лосс.\n",
        "\n",
        "Различные солверы в этом классе, в зависимости от типа средства решения обыкновенных дифференциальных уравнений (ODE), подпадают под вышеуказанную таксономию и обеспечивают хорошую абстракцию для проектирования основных сэмплеров, реализованных в 🤗 Diffusers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4QrFOMkJz3o"
      },
      "source": [
        "При этом нельзя сказать, что какой-то из них лучше других. Каждый из них имеет свои плюсы и минусы.\n",
        "\n",
        "Лучший способ узнать, какой из них лучше всего подходит вам, — попробовать их."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vK7uS8om47QD"
      },
      "outputs": [],
      "source": [
        "# Устанавливаем необходимые библиотеки и импортируем их\n",
        "# !pip install diffusers transformers\n",
        "\n",
        "import torch\n",
        "from diffusers import (\n",
        "    # Stable Diffusion\n",
        "    StableDiffusionPipeline,\n",
        "    # schedulers\n",
        "    DPMSolverMultistepScheduler,\n",
        "    DPMSolverSinglestepScheduler,\n",
        "    KDPM2DiscreteScheduler,\n",
        "    KDPM2AncestralDiscreteScheduler,\n",
        "    EulerDiscreteScheduler,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    HeunDiscreteScheduler,\n",
        "    LMSDiscreteScheduler,\n",
        "    DEISMultistepScheduler,\n",
        "    UniPCMultistepScheduler,\n",
        ")\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from diffusers.utils import make_image_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgvTEjr24-jm"
      },
      "outputs": [],
      "source": [
        "# И определяем модель и Shedulers для тестирования\n",
        "model_id = \"stabilityai/stable-diffusion-2\"\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id)\n",
        "\n",
        "schedulers = [\n",
        "    DPMSolverMultistepScheduler(),  # DPM++ 2M\n",
        "    DPMSolverMultistepScheduler(use_karras_sigmas=True),  # DPM++ 2M Karras\n",
        "    DPMSolverMultistepScheduler(algorithm_type=\"sde-dpmsolver++\"),  # DPM++ 2M SDE\n",
        "    DPMSolverMultistepScheduler(use_karras_sigmas=True, algorithm_type=\"sde-dpmsolver++\"),  # DPM++ 2M SDE Karras\n",
        "    DPMSolverSinglestepScheduler(),  # DPM++ SDE\n",
        "    DPMSolverSinglestepScheduler(use_karras_sigmas=True),  # DPM++ SDE Karras\n",
        "    KDPM2DiscreteScheduler(),  # DPM2\n",
        "    KDPM2DiscreteScheduler(use_karras_sigmas=True),  # DPM2 Karras\n",
        "    KDPM2AncestralDiscreteScheduler(), # DPM2 a\n",
        "    KDPM2AncestralDiscreteScheduler(use_karras_sigmas=True), # DPM2 a Karras\n",
        "    EulerDiscreteScheduler(),  # Euler\n",
        "    EulerAncestralDiscreteScheduler(),  # Euler a\n",
        "    HeunDiscreteScheduler(),  # Heun\n",
        "    LMSDiscreteScheduler(),  # LMS\n",
        "    LMSDiscreteScheduler(use_karras_sigmas=True),  # LMS Karras\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMST7NQNJz3p"
      },
      "source": [
        "Вы можете узнать, какие планировщики совместимы с текущей моделью, вызвав метод `compatibles`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xL2SnCgDJz3p"
      },
      "outputs": [],
      "source": [
        "pipe.scheduler.compatibles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLbxfV72Jz3p"
      },
      "source": [
        "А теперь перейдем к генерациям."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CmpuoGsZyFL"
      },
      "outputs": [],
      "source": [
        "# Задаем промпт и другие параметры для генерации\n",
        "prompt = \"a red-haired kitten sitting on a stone against the background of a green garden\"\n",
        "negative_prompt = \"blurry, noisy, overexposure, artefacts\"\n",
        "guidance_scale = 5.0\n",
        "num_inference_steps = 10\n",
        "\n",
        "images = []\n",
        "\n",
        "# Тестируем каждый планировщик\n",
        "for scheduler in schedulers:\n",
        "\n",
        "    # Получаем название\n",
        "    scheduler_name = scheduler.__class__.__name__\n",
        "    print(f\"Testing {scheduler_name}...\")\n",
        "\n",
        "    # Переключаться между планировщиками довольно просто:\n",
        "    pipe.scheduler = scheduler.from_config(pipe.scheduler.config)\n",
        "    pipe = pipe.to(\"cuda\")\n",
        "\n",
        "    # Генерируем изображение, используя заданный sheduler\n",
        "    image = pipe(\n",
        "        prompt=prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        guidance_scale=guidance_scale,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        generator=torch.Generator(device='cpu').manual_seed(307),\n",
        "    ).images[0]\n",
        "\n",
        "    images.append(image)\n",
        "\n",
        "# Выводим сетку изображений\n",
        "make_image_grid(images, 3, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQxL7pftJz3p"
      },
      "source": [
        "<figure align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=1JO55VWUwu23AMPRzcJq-W65ZtYJCKJ-X\" alt=\"schedulers\" width=\"800\"/>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo67SQQ5Jz3p"
      },
      "source": [
        "Посмотрим на то, насколько сильно зависит каждый из сэмплеров от числа шагов расшумления."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdsWxTT_Jz3p"
      },
      "outputs": [],
      "source": [
        "featured_schedulers = [\n",
        "    DPMSolverMultistepScheduler(),  # DPM++ 2M\n",
        "    DPMSolverSinglestepScheduler(),  # DPM++ SDE\n",
        "    KDPM2DiscreteScheduler(),  # DPM2\n",
        "    KDPM2AncestralDiscreteScheduler(), # DPM2 a\n",
        "    EulerDiscreteScheduler(),  # Euler\n",
        "    EulerAncestralDiscreteScheduler(),  # Euler a\n",
        "    HeunDiscreteScheduler(),  # Heun\n",
        "    LMSDiscreteScheduler(),  # LMS\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAKQsLjwfg7_"
      },
      "outputs": [],
      "source": [
        "images = []\n",
        "\n",
        "steps_list = [3, 5, 7, 10, 15, 20]\n",
        "\n",
        "# Тестируем каждый scheduler\n",
        "for scheduler in tqdm(featured_schedulers):\n",
        "\n",
        "    # Подгружаем новый планировщик\n",
        "    pipe.scheduler = scheduler.from_config(pipe.scheduler.config)\n",
        "    pipe = pipe.to(\"cuda\")\n",
        "\n",
        "    for num_inference_steps in steps_list:\n",
        "\n",
        "        # Генерируем изображение\n",
        "        image = pipe(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            guidance_scale=guidance_scale,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            generator=torch.Generator(device='cpu').manual_seed(307),\n",
        "        ).images[0]\n",
        "\n",
        "        images.append(image)\n",
        "\n",
        "# Выводим сетку из изображений\n",
        "make_image_grid(images, len(featured_schedulers), len(steps_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGTQw_C1Jz3p"
      },
      "source": [
        "<figure align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=1f61ZoZL-yffQ1eGbRzmk2-xa3RFgSQCa\" alt=\"schedulers-steps\" width=\"800\"/>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWQz0oIDJz3q"
      },
      "source": [
        "- Каждая строка соответствует одному сэмплеру\n",
        "- Каждый столбец соответствует определенному числу шагов расшумления на инференсе\n",
        "- Можно видеть, что одним из лучших решений будет DPMSolver"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nooqwkwqYeKg"
      },
      "source": [
        "## 3. Использование нескольких текстовых энкодеров (text encoders)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU5pK1aMJz3t"
      },
      "source": [
        "Следующий наш пункт в сегодняшнем обсуждении современных методов в диффузионных моделях — это использование нескольких текстовых энкодеров.\n",
        "\n",
        "**Вопрос.** Как вы думаете: зачем использовать несколько текстовых энкодеров?\n",
        "\n",
        "<details>\n",
        "  <summary><b>Ответ</b></summary>\n",
        "  \n",
        "  <font color='green'>Каждый из энкодеров привносит свою особенную информацию о входной последовательности. В итоге, объединяя их выходы, можно получить более точное и полноценное описание.</font>\n",
        "</details>\n",
        "\n",
        "В нашем сегодняшнем разговоре мы остановимся на некоторых известных диффузионных моделях, в которых авторы предложили использовать несколько текстовых энкодеров."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpN2tGT8Jz3t"
      },
      "source": [
        "### 3.1. SDXL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiFFW815Jz3t"
      },
      "source": [
        "И первая статья, которая попадает под наше рассмотрение, — [Podell et al. *SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis.*](https://arxiv.org/abs/2307.01952). Мы не будем надолго останавливаться на детальном обзоре SDXL, а сконцентрируемся именно на том, что авторы говорят о методике использования нескольких text encoders.\n",
        "\n",
        "> Мы выбираем более мощный предварительно обученный кодировщик текста, который используем для обработки текста. В частности, мы используем открытый OpenCLIP ViT-bigG в сочетании с CLIP ViT-L, где объединяем предпоследние выходные данные текстового кодера вдоль оси канала.\n",
        "\n",
        "Таким образом, объединение эмбеддингов происходит именно на уровне **предпоследнего слоя**, причем объединение идет **вдоль оси канала**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OI6StWDJz3t"
      },
      "source": [
        "> Помимо использования слоев перекрестного внимания для создания условий для модели при вводе текста, мы дополнительно создаем условия для модели при внедрении pooled текстовых эмбеддингов из модели OpenCLIP.\n",
        "\n",
        "Следовательно, важно отметить, что используются именно pooled текстовые эмбеддинги.\n",
        "\n",
        "**Вопрос.** А что такое pooled эмбеддинги?\n",
        "\n",
        "<details>\n",
        "  <summary><b>Ответ</b></summary>\n",
        "  \n",
        "  <font color='green'>Pooled эмбеддинг содержит информацию обо всем предложении сразу, а не о последовательности отдельных токенов.</font>\n",
        "</details>\n",
        "\n",
        "**Пример.** Представьте, что у вас есть текст, состоящий из 200 токенов, и вы хотите его обработать энкодером с максимальной длиной последовательности 512. В итоге вы получите последовательность из 200 эмбеддингов, каждый размерности 512. А эмбеддинг всего предложения, то есть pooled эмбеддинг, напротив, будет являться всего одним вектором размерности 512.\n",
        "\n",
        "На данном семинаре мы не будем подробно останавливаться на том, какие есть методы пулинга, чтобы получить итоговый эмбеддинг предложения, однако упомянем несколько из них:\n",
        "1. **CLS pooling.** Первый часто используемый метод пулинга заключается в использовании специального `<CLS>`-токена в начале каждого предложения. Он как раз и создается для того, чтобы улавливать информацию обо всем предложении сразу. Следовательно, слой пулинга заключается просто в том, что на его выходе выдается **эмбеддинг CLS-токена** и он используется как эмбеддинг всего предложения. (Например, в процессе обучения BERT такой CLS-токен использовался для предсказания следующего предложения, благодаря чему и обучался.)\n",
        "2. **Mean pooling.** Второй опять же часто используемый метод — это пулинг усреднением. Как предполагается из названия, он аггрегирует информацию о предложении, производя усреднение эмбеддингов отдельных токенов. Аналогичные методы также могут использовать **max pooling** или **mean sqrt pooling**.\n",
        "\n",
        "К сожалению, четкого ответа на вопрос о том, какой метод пулинга использовать, не существует, однако на HuggingFace **по умолчанию стоит именно CLS pooling**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_iNBRBWJz3t"
      },
      "source": [
        "Вернемся к нашим нескольким текстовым энкодерам и рассмотрим то, как происходит процедура объединения их эмбеддингов в модели SXDL.\n",
        "\n",
        "Как мы обсудили, по сути агрегация текстовой информации происходит в два этапа:\n",
        "1. Итоговый текстовый эмбеддинг, который попадает на вход диффузионной модели, использует CLIP ViT-L & OpenCLIP ViT-bigG.\n",
        "2. При этом помимо основного текстового эмбеддинга обуславливание на текстовую информацию происходит и за счет слоев перекрестного внимания (cross-attention), в которые попадают выходы OpenCLIP ViT-bigG после пулинга.\n",
        "\n",
        "Рассмотрим реализацию каждого из пунктов в отдельности."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "govdhsqGJz3u"
      },
      "source": [
        "#### 3.1.1. Основной текстовый эмбеддинг на выходе двух энкодеров"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXYXEehYJz3u"
      },
      "source": [
        "Для того чтобы не загромождать наш ноутбук большим количеством кода, отсылаем вас к реализации `encode_prompt()` в [пайплайне SDXL](https://github.com/huggingface/diffusers/blob/c7617e482a522173ea6f922223aa010058552af8/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py#L214). А мы сконцентрируемся лишь на части этого метода."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQPc4veHJz3u"
      },
      "source": [
        "```python\n",
        "# в реализации можно задать два различных промпта, для каждого энкодера в отдельности\n",
        "prompt = ...\n",
        "prompt_2 = ...\n",
        "\n",
        "# пропускаем промпты через токенайзеры (опять же отдельные для каждого энкодера)\n",
        "text_inputs = tokenizer(\n",
        "    prompt,\n",
        "    padding=\"max_length\",\n",
        "    max_length=tokenizer.model_max_length,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "text_inputs_2 = tokenizer_2(\n",
        "    prompt_2,\n",
        "    padding=\"max_length\",\n",
        "    max_length=tokenizer_2.model_max_length,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# получаем токенизированные входы для энкодеров\n",
        "text_input_ids = text_inputs.ids\n",
        "text_input_ids_2 = text_inputs_2.ids\n",
        "\n",
        "# получаем эмбеддинги с помощью энкодеров\n",
        "prompt_embeds = text_encoder(text_input_ids, output_hidden_states=True)\n",
        "prompt_embeds_2 = text_encoder_2(text_input_ids_2, output_hidden_states=True)\n",
        "\n",
        "# на выходе хотим иметь предпоследний слой\n",
        "prompt_embeds = prompt_embeds.hidden_states[-2] # prompt_embeds.shape: torch.Size([1, 77, 768])\n",
        "prompt_embeds_2 = prompt_embeds_2.hidden_states[-2] # prompt_embeds_2.shape: torch.Size([1, 77, 1280])\n",
        "\n",
        "# конкатенируем эмбеддинги\n",
        "prompt_embeds = torch.concat([prompt_embeds, prompt_embeds_2], dim=-1) # prompt_embeds.shape: torch.Size([1, 77, 2048])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FueecS1aJz3u"
      },
      "source": [
        "Таким образом, видно, как исходные два текстовых эмбеддинга объединяются в один, причем для каждого токена идет объединение по соответствующей размерности эмбеддинга."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-csbhqRIJz3u"
      },
      "source": [
        "#### 3.1.2. Перекрестное внимание с текстовым эмбеддингом после пулинга"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4Daa6-mJz3u"
      },
      "source": [
        "Давайте рассмотрим следующую простую реализацию, которую предложили сами авторы статьи SDXL.\n",
        "\n",
        "Ранее мы еще не обсуждали следующие моменты, но сейчас столкнемся с дополнительным обуславливанием на:\n",
        "1. Размер изображения\n",
        "2. Выбор только части изображения (так называемый crop)\n",
        "3. Соотношение сторон изображения\n",
        "\n",
        "Каждое из этих условий также предобрабатывается заранее, а потом подается и конкатенируется вместе с текстовым эмбеддингом после пулинга."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aj5AuNZ1Jz3u"
      },
      "outputs": [],
      "source": [
        "from einops import rearrange\n",
        "import torch\n",
        "import math\n",
        "\n",
        "batch_size = 16\n",
        "# channel dimension of pooled output of text encoder(s)\n",
        "pooled_dim = 512\n",
        "\n",
        "def fourier_embedding(inputs, outdim=256, max_period=10000):\n",
        "    \"\"\"\n",
        "    Classical sinusoidal timestep embedding\n",
        "    as commonly used in diffusion models\n",
        "    :param inputs: batch of integer scalars shape [b,]\n",
        "    :param outdim: embedding dimension\n",
        "    :param max_period: max freq added\n",
        "    :return: batch of embeddings of shape [b, outdim]\n",
        "    \"\"\"\n",
        "    device = inputs.device\n",
        "    half_dim = outdim // 2\n",
        "    embeddings = math.log(max_period) / (half_dim - 1)\n",
        "    embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "    embeddings = inputs[:, None] * embeddings[None, :]\n",
        "    embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "    return embeddings\n",
        "\n",
        "def cat_along_channel_dim(\n",
        "        x:torch.Tensor,) -> torch.Tensor:\n",
        "    if x.ndim == 1:\n",
        "        x = x[...,None]\n",
        "    assert x.ndim == 2\n",
        "    b, d_in = x.shape\n",
        "    x = rearrange(x, \"b din -> (b din)\")\n",
        "    # fourier fn adds additional dimension\n",
        "    emb = fourier_embedding(x)\n",
        "    d_f = emb.shape[-1]\n",
        "    emb = rearrange(emb, \"(b din) df -> b (din df)\",\n",
        "                        b=b, din=d_in, df=d_f)\n",
        "    return emb\n",
        "\n",
        "def concat_embeddings(\n",
        "        # batch of size and crop conditioning cf. Sec. 3.2\n",
        "        c_size: torch.Tensor,\n",
        "        c_crop: torch.Tensor,\n",
        "        # batch of aspect ratio conditioning cf. Sec. 3.3\n",
        "        c_ar: torch.Tensor,\n",
        "        # final output of text encoders after pooling cf. Sec. 3.1\n",
        "        c_pooled_txt: torch.Tensor, ) -> torch.Tensor:\n",
        "    print('====> concat_embeddings()')\n",
        "    # fourier feature for size conditioning\n",
        "    c_size_emb = cat_along_channel_dim(c_size)\n",
        "    print('c_size_emb.shape:', c_size_emb.shape)\n",
        "    # fourier feature for crop conditioning\n",
        "    c_crop_emb = cat_along_channel_dim(c_crop)\n",
        "    print('c_crop_emb.shape:', c_crop_emb.shape)\n",
        "    # fourier feature for aspect ratio conditioning\n",
        "    c_ar_emb = cat_along_channel_dim(c_ar)\n",
        "    print('c_ar_emb.shape:', c_ar_emb.shape)\n",
        "    # the concatenated output is mapped to the same\n",
        "    # channel dimension than the noise level conditioning\n",
        "    # and added to that conditioning before being fed to the unet\n",
        "    return torch.cat([c_pooled_txt,\n",
        "                      c_size_emb,\n",
        "                      c_crop_emb,\n",
        "                      c_ar_emb], dim=1)\n",
        "\n",
        "# simulating c_size as in Sec. 3.2\n",
        "c_size = torch.zeros((batch_size, 2)).long()\n",
        "print('c_size.shape:', c_size.shape)\n",
        "# simulating c_crop as in Sec. 3.2\n",
        "c_crop = torch.zeros((batch_size, 2)).long()\n",
        "print('c_crop.shape:', c_crop.shape)\n",
        "# simulating c_ar as in Sec. 3.3\n",
        "c_ar = torch.zeros((batch_size, 2)).long()\n",
        "print('c_ar.shape:', c_ar.shape)\n",
        "# simulating pooled text encoder output as in Sec. 3.3\n",
        "c_pooled = torch.zeros((batch_size, pooled_dim)).long()\n",
        "print('c_pooled.shape:', c_pooled.shape)\n",
        "\n",
        "# get concatenated embedding\n",
        "c_concat = concat_embeddings(c_size, c_crop, c_ar, c_pooled)\n",
        "print('c_concat.shape:', c_concat.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z0QguonJz3u"
      },
      "source": [
        "Таким образом, на выходе получаем эмбеддинг размера 2048, который объединяет в себе сразу всю информацию об исходном промпте и наборе параметров генерации."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxMuHAtpJz3u"
      },
      "source": [
        "### 3.2. Stable Diffusion 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_p6zk0vJz3u"
      },
      "source": [
        "А теперь давайте перейдем к более современной модели — [Stable Diffusion 3](https://stabilityai-public-packages.s3.us-west-2.amazonaws.com/Stable+Diffusion+3+Paper.pdf).\n",
        "\n",
        "Она так же, как и SDXL, использует несколько текстовых энкодеров, однако делает это немного иначе. Опять же, сейчас мы не будем вдаваться в подробности архитектуры (спойлер: мы это обсудим в следующем разделе), а сконцентрируемся только на теме текстовых эмбеддингов.\n",
        "\n",
        "Итак, в начале статьи авторы упоминают, что идея использования нескольких текстовых энкодеров различного сорта (не CLIP и OpenCLIP, как в SDXL, а принципиально разных) уже получила свое развитие в работе [Balaji et al. 2023 *eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers*](https://arxiv.org/pdf/2211.01324).\n",
        "\n",
        "**Вопрос.** Как вы думаете: в чем может быть смысл использовать принципиально разные текстовые энкодеры, например CLIP и T5?\n",
        "\n",
        "<details>\n",
        "  <summary><b>Ответ</b></summary>\n",
        "  \n",
        "  <font color='green'>Каждый из энкодеров имеет уникальную архитектуру и обучался на своем наборе данных, благодаря чему какой-то может обеспечивать лучшую композицию кадра, а какой-то лучше понимает стиль и детали.</font>\n",
        "</details>\n",
        "\n",
        "Однако помимо базовой идеи с различным пониманием текста у энкодеров есть и другая причина использовать несколько разных моделей. Авторы Stable Diffusion 3 во время обучения с вероятностью $46.4\\%$ выкидывают каждый из текстовых энкодеров (буквально зануляют его выходы, производя тем самым drop-out). Как они утверждают (и на самом деле доказывают экспериментально), это позволяет уже на инференсе использовать только один из энкодеров, более легковесный.\n",
        "\n",
        "**Вопрос.** Как вы думаете: почему именно $46.4\\%$ — шанс зануления каждого из эмбеддингов?\n",
        "\n",
        "**Ответ.** На самом деле все просто — в таком случае в процессе обучения модель учится без текстового условия примерно в $10\\%$ случаев, что является рекомендуемым значением в обучении с classifier-free guidance подходом.\n",
        "\n",
        "Если переходить к конкретике, авторы используют комбинацию из трех различных энкодеров: OpenCLIP-bigG/14, CLIP-L/14 и T5 XXL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25C5aHPnJz3u"
      },
      "source": [
        "#### 3.2.1. Архитектура"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lvxlwr2RJz3u"
      },
      "source": [
        "Перейдем к архитектуре, а именно к той ее части, которая связана с текстовыми энкодерами. Рассмотрим следующую схемку из оригинальной статьи Stable Diffusion 3.\n",
        "\n",
        "<figure align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=1Cb17ztm0Bjg0DLGVmjzKbOUU1KPOgk0x\" alt=\"sd3-text\" width=\"500\"/>\n",
        "    <figcaption> Использование трех текстовых энкодеров в модели Stable Diffusion 3. Pooled эмбеддинги (левая часть) от энкодеров CLIP используются в слоях перекрестного внимания (cross-attention). Конкатенированные эмбеддинги от всех трех энкодеров подаются как контекст на вход модели. Источник: <a href=\"https://stabilityai-public-packages.s3.us-west-2.amazonaws.com/Stable+Diffusion+3+Paper.pdf\">Esser et al. 2024</a> </figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhM9TQJtJz3u"
      },
      "source": [
        "Аналогично тому, как это происходило в SDXL, процесс обработки текстовой информации (caption) состоит из нескольких частей:\n",
        "1. Итоговый текстовый эмбеддинг является конкатенацией трех эмбеддингов от отдельных замороженных моделей: OpenCLIP-bigG/14, CLIP-L/14 и T5 XXL.\n",
        "2. В слои перекрестного внимания (cross-attention) подаются эмбеддинги после пулинга, полученные конкатенацией от двух моделей CLIP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTPSneNJJz3u"
      },
      "source": [
        "То есть, опять же,\n",
        "1. (Справа на картинке) Конкатенация предпоследних скрытых представлений вдоль оси канала: $c_{\\text{ctxt}}^{\\text{CLIP-G/14}} \\in \\mathbb{R}^{77 \\times 1280}$ и $c_{\\text{ctxt}}^{\\text{CLIP-L/14}} \\in \\mathbb{R}^{77 \\times 768}$ объединяются в $c_{\\text{ctxt}}^{\\text{CLIP}} \\in \\mathbb{R}^{77 \\times 2048}$, а затем и с $c_{\\text{ctxt}}^{\\text{T5}} \\in \\mathbb{R}^{77 \\times 4096}$, при этом добиваются нулями до размерности 4096 вдоль каждого токена. Итак, финальный контекст на вход модели есть $c_{\\text{ctxt}} \\in \\mathbb{R}^{154 \\times 4096}$.\n",
        "2. (Слева на картинке) Конкатенация pooled-выходов размеров $768$ и $1280$ от CLIP-L/14 и OpenCLIP-bigG/14 соответственно."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccSGPluOJz3v"
      },
      "source": [
        "#### 3.2.2. Пример влияния отключения T5-энкодера на качество генерации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOedOW9bJz3v"
      },
      "source": [
        "Рассмотрим, как сказывается отбрасывание T5 XXL текстового энкодера на генерации."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snxVn6QhJz3v"
      },
      "source": [
        "<figure align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=179dSuEpwIQfQvOl8Zxd0papvPBGLbdWv\" alt=\"text-encoder-drop\" width=\"500\"/>\n",
        "    <figcaption> <b>Влияние T5-энкодера.</b> Можно наблюдать, что T5 более важен для сложных промптов, в частности для качественной детализации. Однако для большинства промптов удаление T5 на инференсе все же дает существенное ускорение. Источник: <a href=\"https://stabilityai-public-packages.s3.us-west-2.amazonaws.com/Stable+Diffusion+3+Paper.pdf\">Esser et al. 2024</a> </figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc2N_gCJJz3v"
      },
      "source": [
        "Таким образом, по большому счету прирост в качестве действительно есть, но только в сложных композициях, требующих хорошего понимания текстового запроса.\n",
        "\n",
        "Тем не менее для большинства промптов будет хватать обычного CLIP-энкодера."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAmrS9GuYbB3"
      },
      "source": [
        "## 4. DiT и реализация MMDiT из SD3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_QXtzvRZ6_a"
      },
      "source": [
        "Перейдем к последней на сегодня очень интересной теме — использованию трансформерных архитектур в диффузионных моделях.\n",
        "\n",
        "Напомним, что классическим решением для предсказания шума в DDPM-подобных моделях было использование U-Net.\n",
        "\n",
        "И наверняка вы могли задаваться вопросом: почему именно U-Net, нет ли возможности использовать что-то еще?\n",
        "\n",
        "Так вот сейчас мы обсудим другую модель, фундаментом которой стал Vision Transformer (ViT), а именно **Diffusion Transformer (DiT)**, предложенную в [этой статье](https://arxiv.org/abs/2212.09748)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au82Z6nZJz3v"
      },
      "source": [
        "### 4.1. Напоминание о Vision Transformer (ViT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNnVZFj4Jz3v"
      },
      "source": [
        "Для того чтобы погрузиться в реализацию DiT, сначала вспомним базовые концепции и идеи из Vision Transformer.\n",
        "\n",
        "<figure align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=1onA7x-72bmLnCnOKxnog8g0eicn2-4FE\" alt=\"vit\" width=\"800\"/>\n",
        "    <figcaption> <b>Архитектура модели Vision Transformer (ViT).</b> Изображение разрезается на патчи фиксированного размера, затем они линейно преобрауются в эмбеддинги, к ним добавляются позиционные эмбеддинги, а после этого результат подается в виде векторов в стандартный трансформерный энкодер. Для задачи классификации также обучается специальный `[class] embedding`. Источник: <a href=\"https://arxiv.org/pdf/2010.11929\">Dosovitskiy et al. 2021</a> </figcaption>\n",
        "</figure>\n",
        "\n",
        "1. ViT состоит из последовательных трансформерных энкодерных блоков — **Transformer Encoder Layer**. Эти слои работают с последовательностью визуальных токенов исходного изображения.\n",
        "2. Эти визуальные токены — части исходного изображения, которые выглядят как отдельные квадраты (патчи) в сетке на изображении (обычно порядка $16 \\times 16$ патчей на изображении).\n",
        "3. Совместно с этими визуальными токенами на вход трансформерных блоков также подается и специальный обучаемый **`<CLS>`-токен**. Когда ViT обучается для задачи классификации изображений, этот `<CLS>`-токен подается на вход итоговой MLP головы на выходе сети.\n",
        "4. Для преобразования исходного изображения в последовательность патчей внутри ViT есть специальный блок — **Patch Embedding Block** (слева на картинке). Внутри него патчи с помощью линейного слоя преобразуются в векторы, к которым затем добавляются позиционные эмбеддинги (классические синусоидальные).\n",
        "5. Архитектура трансформерного слоя достаточно классическая (справа на картинке) и включает в себя последовательность из LayerNorm, Multi-Head Attention, LayerNorm и MLP. Отметим, что также используются **два слоя skip connection**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kejgMo7EJz3v"
      },
      "source": [
        "### 4.2. Реализация DiT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrtewNrrJz3v"
      },
      "source": [
        "Теперь перейдем к тому, как авторы DiT решили изменить архитектуру ViT, чтобы предсказывать добавляемый к изображению шум (на самом деле это все происходит в латентном пространстве, не стоит об этом забывать, но для простоты изложения думаем об этом как о реальных изображениях).\n",
        "\n",
        "<figure align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=1kuVMWAN2YwEKYsnJVyhNgRvhuMac4Ywz\" alt=\"dit\" width=\"800\"/>\n",
        "    <figcaption> <b>Архитектура модели Diffusion Transformer (DiT).</b> Слева: Входная латентная информация разбивается на патчи и обрабатывается несколькими блоками DiT. Справа: Подробная информация о блоках DiT. Эксперименты с вариантами стандартных блоков-трансформеров (adaLN-Zero, Cross-Attention, In-Context Conditioning) привели к тому, что первый работает лучше всего. Источник: <a href=\"https://arxiv.org/abs/2212.09748\">Peebles et al. 2023</a> </figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEF1ASV0Jz3v"
      },
      "source": [
        "1. Первое, на что мы обратим внимание, — модель DiT является **условной**, она принимает на вход помимо исходного Noised Latent также **Embed**, который аггрегирует в себе информацию о\n",
        "*   классовой метке изображения $y$,\n",
        "*   временном шаге $t$.\n",
        "\n",
        "2. Вторая особенность заключается в использовании специального **Adaptive LayerNorm-Zero** блока. Похожая идея использовалась еще в архитектуре [StyleGAN](https://arxiv.org/abs/1812.04948), где авторы предлагали Adaptive InstanceNorm.\n",
        "\n",
        "   - Сам по себе Adaprive LayerNorm предполагает обучение двух параметров $\\gamma$ и $\\beta$, которые отвечают за умножение и сдвиг. Вместо того, чтобы просто напрямую их учить, в DiT предлагается получать их с помощью MLP из совместного эмбеддинга Embed (из label и timestep).\n",
        "   - Важный момент состоит в приписке **-Zero**, которая говорит о том, что авторы инициализируют MLP нулями.\n",
        "   - **Вопрос.** Как при этом тогда выглядит выход всего блока DiT?\n",
        "   - **Ответ.** Он совпадает со входом, поскольку в архитектуре есть слои skip connection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GULSd0qsJz3v"
      },
      "source": [
        "Итак, мы обсудили, чем отличается DiT от классического ViT.\n",
        "\n",
        "**Вопрос (с подвохом).** А где мы используем текстовую информацию (caption), чтобы построить диффузионную модель text2image?\n",
        "\n",
        "<details>\n",
        "  <summary><b>Ответ</b></summary>\n",
        "  \n",
        "  <font color='green'>DiT не предполагает использование текстового контекста, а обуславливается лишь на метку класса и временной шаг. Для того чтобы использовать DiT для text2image-генерации, нужно изменить архитектуру обработки контекста.\n",
        "\n",
        "<b>Примечание.</b> Конечно, можно зашить текстовый эмбеддинг в качестве **Embed** и таким образом подавать его в модель, и такие варианты есть. Однако есть вариант и получше :)</font>\n",
        "</details>\n",
        "\n",
        "Таким образом, мы плавно пришли к тому, что сейчас активно используется в последних диффузионных моделях — архитектуре MM-DiT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x887ONRKJz3v"
      },
      "source": [
        "### 4.3. Multimodal DiT (MM-DiT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYfPObncJz3v"
      },
      "source": [
        "Авторы [Stable Diffusion 3](https://stabilityai-public-packages.s3.us-west-2.amazonaws.com/Stable+Diffusion+3+Paper.pdf) построили свою архитектуру на основе DiT.\n",
        "\n",
        "Аналогично DiT, они используют эмбеддинги временного шага $t$ и $c_{\\text{vec}}$ как входы адаптивных слоев нормализации (или, как они пишут, механизмов модуляции).\n",
        "\n",
        "Важный момент заключается в том, что такого обуславливания критически не хватает для полноценной text2image-генерации, ведь pooled текстовые эмбеддинги содержат только обобщенную информацию о предложении.\n",
        "\n",
        "Таким образом, авторы предлагают использовать полноценный текстовый контекст как дополнительный вход каждого блока."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xukQ4zB0Jz3w"
      },
      "source": [
        "<figure align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=10b1ZU9FwtOVI1FQO8YZCSYGJmWd5s1Pp\" alt=\"mmdit\" width=\"1000\"/>\n",
        "    <figcaption> <b>Архитектура модели Multimodal Diffusion Transformer (MM-DiT).</b> Слева: полная архитектура со всеми компонентами. Справа: один MM-DiT блок. Источник: <a href=\"https://arxiv.org/abs/2212.09748\">Peebles et al. 2023</a> </figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9gFJdjRJz3w"
      },
      "source": [
        "Давайте последовательно разбираться с тем, что происходит с текстовой и визуальной информацией в этой модели.\n",
        "\n",
        "**Текст**\n",
        "\n",
        "1. На вход модели приходит **Caption**, то есть текстовое описание, по которому мы хотим сгенерировать изображение.\n",
        "2. Этот **Caption** проходит через три различных текстовых энкодера (как обсуждалось ранее), при этом получаются pooled текстовый эмбеддинг $c_{\\text{vec}} \\in \\mathbb{R}^{2048}$ и полноценный текстовый контекст $c_{\\text{ctxt}} \\in \\mathbb{R}^{154 \\times 4096}$.\n",
        "3. Pooled текстовый эмбеддинг $c_{\\text{vec}}$ соединяется с векторным представлением временного шага **Timestep**, который уже заранее предобрабатывается с позиционным эмбеддингом. В итоге получается $y$.\n",
        "4. Полноценный текстовый контекст $c_{\\text{ctxt}}$ проходит через линейный слой, в итоге реализуя контекст $c$.\n",
        "   \n",
        "**Изображение**\n",
        "\n",
        "1. Зашумленный латент **Noised Latent** разделяется на патчи и пропускается через линейный слой, совмещаясь с позиционными эмбеддингами, как это было и в DiT. В итоге имеем $x$.\n",
        "\n",
        "**Вопрос.** А что будет происходить дальше, судя по схеме?\n",
        "\n",
        "**Ответ.** Текстовая и визуальная информация **по отдельности** проходят через трансформерные блоки MM-DiT, взаимодействуя **только** посредством механизма **Attention**.\n",
        "\n",
        "Да, на первый взгляд такая архитектура кажется ужасным усложнением, ведь мы буквально дублируем исходную архитектуру DiT и повторяем ее дважды. Однако именно это позволяет нам полноценно учитывать текстовую информацию."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt78VfOlJz3w"
      },
      "source": [
        "### 4.4. Имплементация одного блока MM-DiT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmCB1eQTJz3w"
      },
      "source": [
        "А теперь давайте перейдем к реализации одного блока MM-DiT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1kwQKKLJz3w"
      },
      "outputs": [],
      "source": [
        "# pip install x_transformers\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Module, ModuleList\n",
        "\n",
        "from einops import rearrange, repeat, pack, unpack\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "from x_transformers.attend import Attend\n",
        "from x_transformers import (\n",
        "    RMSNorm,\n",
        "    FeedForward\n",
        ")\n",
        "\n",
        "# Вспомогательные функции\n",
        "\n",
        "def exists(v):\n",
        "    return v is not None\n",
        "\n",
        "def default(v, d):\n",
        "    return v if exists(v) else d\n",
        "\n",
        "def softclamp(t, value):\n",
        "    return (t / value).tanh() * value\n",
        "\n",
        "# Слой нормировки\n",
        "\n",
        "class MultiHeadRMSNorm(Module):\n",
        "    def __init__(self, dim, heads = 1):\n",
        "        super().__init__()\n",
        "        self.scale = dim ** 0.5\n",
        "        self.gamma = nn.Parameter(torch.ones(heads, 1, dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.normalize(x, dim = -1) * self.gamma * self.scale\n",
        "\n",
        "# Слой совместного внимания текста и изображения\n",
        "\n",
        "class JointAttention(Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        dim_inputs: Tuple[int, ...],\n",
        "        dim_head = 64,\n",
        "        heads = 8,\n",
        "        qk_rmsnorm = False,\n",
        "        flash = False,\n",
        "        softclamp = False,\n",
        "        softclamp_value = 50.,\n",
        "        attend_kwargs: dict = dict()\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        ein notation\n",
        "\n",
        "        b - batch\n",
        "        h - heads\n",
        "        n - sequence\n",
        "        d - feature dimension\n",
        "        \"\"\"\n",
        "\n",
        "        dim_inner = dim_head * heads\n",
        "\n",
        "        num_inputs = len(dim_inputs)\n",
        "        self.num_inputs = num_inputs\n",
        "\n",
        "        self.to_qkv = ModuleList([nn.Linear(dim_input, dim_inner * 3, bias = False) for dim_input in dim_inputs])\n",
        "\n",
        "        self.split_heads = Rearrange('b n (qkv h d) -> qkv b h n d', h = heads, qkv = 3)\n",
        "\n",
        "        self.attend = Attend(\n",
        "            flash = flash,\n",
        "            softclamp_logits = softclamp,\n",
        "            logit_softclamp_value = softclamp_value,\n",
        "            **attend_kwargs\n",
        "        )\n",
        "\n",
        "        self.merge_heads = Rearrange('b h n d -> b n (h d)')\n",
        "\n",
        "        self.to_out = ModuleList([nn.Linear(dim_inner, dim_input, bias = False) for dim_input in dim_inputs])\n",
        "\n",
        "        self.qk_rmsnorm = qk_rmsnorm\n",
        "        self.q_rmsnorms = (None,) * num_inputs\n",
        "        self.k_rmsnorms = (None,) * num_inputs\n",
        "\n",
        "        if qk_rmsnorm:\n",
        "            self.q_rmsnorms = ModuleList([MultiHeadRMSNorm(dim_head, heads = heads) for _ in range(num_inputs)])\n",
        "            self.k_rmsnorms = ModuleList([MultiHeadRMSNorm(dim_head, heads = heads) for _ in range(num_inputs)])\n",
        "\n",
        "        self.register_buffer('dummy', torch.tensor(0), persistent = False)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        inputs: Tuple[Tensor],\n",
        "        masks: Tuple[Tensor | None] | None = None\n",
        "    ):\n",
        "\n",
        "        device = self.dummy.device\n",
        "\n",
        "        assert len(inputs) == self.num_inputs\n",
        "\n",
        "        masks = default(masks, (None,) * self.num_inputs)\n",
        "\n",
        "        # Проецируем каждую модальность отдельно на qkv\n",
        "\n",
        "        all_qkvs = []\n",
        "        all_masks = []\n",
        "\n",
        "        for x, mask, to_qkv, q_rmsnorm, k_rmsnorm in zip(inputs, masks, self.to_qkv, self.q_rmsnorms, self.k_rmsnorms):\n",
        "\n",
        "            qkv = to_qkv(x)\n",
        "            qkv = self.split_heads(qkv)\n",
        "\n",
        "            # Опционально делаем отдельно нормировку qk для каждой модальности\n",
        "\n",
        "            if self.qk_rmsnorm:\n",
        "                q, k, v = qkv\n",
        "                q = q_rmsnorm(q)\n",
        "                k = k_rmsnorm(k)\n",
        "                qkv = torch.stack((q, k, v))\n",
        "\n",
        "            all_qkvs.append(qkv)\n",
        "\n",
        "            # Оперируем масками для каждой модальности\n",
        "\n",
        "            if not exists(mask):\n",
        "                mask = torch.ones(x.shape[:2], device = device, dtype = torch.bool)\n",
        "\n",
        "            all_masks.append(mask)\n",
        "\n",
        "        # Комбинируем все qkv и маски\n",
        "\n",
        "        all_qkvs, packed_shape = pack(all_qkvs, 'qkv b h * d')\n",
        "        all_masks, _ = pack(all_masks, 'b *')\n",
        "\n",
        "        # Разделяем qkv на части\n",
        "\n",
        "        q, k, v = all_qkvs\n",
        "\n",
        "        outs, *_ = self.attend(q, k, v, mask = all_masks)\n",
        "\n",
        "        # Объединяем отдельные головы\n",
        "\n",
        "        outs = self.merge_heads(outs)\n",
        "        outs = unpack(outs, packed_shape, 'b * d')\n",
        "\n",
        "        # Отделяем комбинации голов для каждой модальности\n",
        "\n",
        "        all_outs = []\n",
        "\n",
        "        for out, to_out in zip(outs, self.to_out):\n",
        "            out = to_out(out)\n",
        "            all_outs.append(out)\n",
        "\n",
        "        return tuple(all_outs)\n",
        "\n",
        "# Класс одного блока MM-DiT\n",
        "\n",
        "class MMDiTBlock(Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim_joint_attn,\n",
        "        dim_text,\n",
        "        dim_image,\n",
        "        dim_cond = None,\n",
        "        dim_head = 64,\n",
        "        heads = 8,\n",
        "        qk_rmsnorm = False,\n",
        "        flash_attn = False,\n",
        "        ff_kwargs: dict = dict()\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Обуславливание на временной шаг\n",
        "\n",
        "        has_cond = exists(dim_cond)\n",
        "        self.has_cond = has_cond\n",
        "\n",
        "        if has_cond:\n",
        "            dim_gammas = (\n",
        "                *((dim_text,) * 4),\n",
        "                *((dim_image,) * 4)\n",
        "            )\n",
        "\n",
        "            dim_betas = (\n",
        "                *((dim_text,) * 2),\n",
        "                *((dim_image,) * 2),\n",
        "            )\n",
        "\n",
        "            self.cond_dims = (*dim_gammas, *dim_betas)\n",
        "\n",
        "            to_cond_linear = nn.Linear(dim_cond, sum(self.cond_dims))\n",
        "\n",
        "            self.to_cond = nn.Sequential(\n",
        "                Rearrange('b d -> b 1 d'),\n",
        "                nn.SiLU(),\n",
        "                to_cond_linear\n",
        "            )\n",
        "\n",
        "            nn.init.zeros_(to_cond_linear.weight)\n",
        "            nn.init.zeros_(to_cond_linear.bias)\n",
        "            nn.init.constant_(to_cond_linear.bias[:sum(dim_gammas)], 1.)\n",
        "\n",
        "        # Адаптивная нормализация\n",
        "\n",
        "        self.text_attn_layernorm = nn.LayerNorm(dim_text, elementwise_affine = not has_cond)\n",
        "        self.image_attn_layernorm = nn.LayerNorm(dim_image, elementwise_affine = not has_cond)\n",
        "\n",
        "        self.text_ff_layernorm = nn.LayerNorm(dim_text, elementwise_affine = not has_cond)\n",
        "        self.image_ff_layernorm = nn.LayerNorm(dim_image, elementwise_affine = not has_cond)\n",
        "\n",
        "        # Attention и FeedForward\n",
        "\n",
        "        self.joint_attn = JointAttention(\n",
        "            dim = dim_joint_attn,\n",
        "            dim_inputs = (dim_text, dim_image),\n",
        "            dim_head = dim_head,\n",
        "            heads = heads,\n",
        "            flash = flash_attn\n",
        "        )\n",
        "\n",
        "        self.text_ff = FeedForward(dim_text, **ff_kwargs)\n",
        "        self.image_ff = FeedForward(dim_image, **ff_kwargs)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        *,\n",
        "        text_tokens,\n",
        "        image_tokens,\n",
        "        text_mask = None,\n",
        "        time_cond = None,\n",
        "        skip_feedforward_text_tokens = True\n",
        "    ):\n",
        "        assert not (exists(time_cond) ^ self.has_cond), 'time condition must be passed in if dim_cond is set at init. it should not be passed in if not set'\n",
        "\n",
        "        if self.has_cond:\n",
        "            (\n",
        "                text_pre_attn_gamma,\n",
        "                text_post_attn_gamma,\n",
        "                text_pre_ff_gamma,\n",
        "                text_post_ff_gamma,\n",
        "                image_pre_attn_gamma,\n",
        "                image_post_attn_gamma,\n",
        "                image_pre_ff_gamma,\n",
        "                image_post_ff_gamma,\n",
        "                text_pre_attn_beta,\n",
        "                text_pre_ff_beta,\n",
        "                image_pre_attn_beta,\n",
        "                image_pre_ff_beta,\n",
        "            ) = self.to_cond(time_cond).split(self.cond_dims, dim = -1)\n",
        "\n",
        "        # Attention адаптивная нормализация\n",
        "\n",
        "        text_tokens_residual = text_tokens\n",
        "        image_tokens_residual = image_tokens\n",
        "\n",
        "        text_tokens = self.text_attn_layernorm(text_tokens)\n",
        "        image_tokens = self.image_attn_layernorm(image_tokens)\n",
        "\n",
        "        if self.has_cond:\n",
        "            text_tokens = text_tokens * text_pre_attn_gamma + text_pre_attn_beta\n",
        "            image_tokens = image_tokens * image_pre_attn_gamma + image_pre_attn_beta\n",
        "\n",
        "        # Attention\n",
        "\n",
        "        text_tokens, image_tokens = self.joint_attn(\n",
        "            inputs = (text_tokens, image_tokens),\n",
        "            masks = (text_mask, None)\n",
        "        )\n",
        "\n",
        "        # Обусловленный attention выход\n",
        "\n",
        "        if self.has_cond:\n",
        "            text_tokens = text_tokens * text_post_attn_gamma\n",
        "            image_tokens = image_tokens * image_post_attn_gamma\n",
        "\n",
        "        # Добавляем attention residual\n",
        "\n",
        "        text_tokens = text_tokens + text_tokens_residual\n",
        "        image_tokens = image_tokens + image_tokens_residual\n",
        "\n",
        "        # FeedForward адаптивная нормализация\n",
        "\n",
        "        text_tokens_residual = text_tokens\n",
        "        image_tokens_residual = image_tokens\n",
        "\n",
        "        text_tokens = self.text_attn_layernorm(text_tokens)\n",
        "        image_tokens = self.image_attn_layernorm(image_tokens)\n",
        "\n",
        "        if self.has_cond:\n",
        "            text_tokens = text_tokens * text_pre_ff_gamma + text_pre_ff_beta\n",
        "            image_tokens = image_tokens * image_pre_ff_gamma + image_pre_ff_beta\n",
        "\n",
        "        # FeedForward на изображениях\n",
        "\n",
        "        image_tokens = self.image_ff(image_tokens)\n",
        "\n",
        "        # Картиночный обусловленный attention выход\n",
        "\n",
        "        if self.has_cond:\n",
        "            image_tokens = image_tokens * image_post_ff_gamma\n",
        "\n",
        "        # Добавляем картиночный attention residual\n",
        "\n",
        "        image_tokens = image_tokens + image_tokens_residual\n",
        "\n",
        "        # Преждевременный выход, без последнего слоя\n",
        "\n",
        "        if skip_feedforward_text_tokens:\n",
        "            return text_tokens, image_tokens\n",
        "\n",
        "        # FeedForward для текста\n",
        "\n",
        "        text_tokens = self.text_ff(text_tokens)\n",
        "\n",
        "        # Текстовый обусловленный attention выход\n",
        "\n",
        "        if self.has_cond:\n",
        "            text_tokens = text_tokens * text_post_ff_gamma\n",
        "\n",
        "        # Добавляем текстовый attention residual\n",
        "\n",
        "        text_tokens = text_tokens + text_tokens_residual\n",
        "\n",
        "        return text_tokens, image_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-NwaHhAJz3w"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Инициализируем один блок MM-DiT\n",
        "\n",
        "block = MMDiTBlock(\n",
        "    dim_joint_attn=512,\n",
        "    dim_cond=256,\n",
        "    dim_text=768,\n",
        "    dim_image=512,\n",
        "    qk_rmsnorm=True\n",
        ")\n",
        "\n",
        "# Инициализируем входные тензоры\n",
        "\n",
        "time_cond = torch.randn(2, 256)\n",
        "\n",
        "text_tokens = torch.randn(2, 512, 768)\n",
        "text_mask = torch.ones((2, 512)).bool()\n",
        "\n",
        "image_tokens = torch.randn(2, 1024, 512)\n",
        "\n",
        "# Пропускаем через блок MM-DiT\n",
        "\n",
        "text_tokens_next, image_tokens_next = block(\n",
        "    time_cond=time_cond,\n",
        "    text_tokens=text_tokens,\n",
        "    text_mask=text_mask,\n",
        "    image_tokens=image_tokens\n",
        ")\n",
        "\n",
        "print('text_tokens_next.shape:', text_tokens_next.shape)\n",
        "print('image_tokens_next.shape:', image_tokens_next.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvWTOnmxJz3w"
      },
      "source": [
        "Таким образом, мы с Вами реализовали один блок из архитектуры MM-DiT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs9fY0rrJz3w"
      },
      "source": [
        "## Итоги"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U92FhoTJz3w"
      },
      "source": [
        "Подведем итоги нашего сегодняшнего занятия.\n",
        "1. Модель DDPM слишком медленно работает на инференсе, одним из решений является DDIM, в котором прямой процесс перестает быть марковским. При этом функция потерь в DDIM совпадает с той же в DDPM, что позволяет обучить предсказатель шума в парадигме DDPM, а затем использовать DDIM для ускоренного инференса. Ускоренный процесс сэмплирования достигается за счет пропуска шагов в обратном процессе и имеет тем меньшую погрешность, чем более детерминистичный процесс используется. Именно поэтому DDIM оказывается удачным решением для ускорения.\n",
        "2. Помимо DDIM существует множество других «планировщиков шума», которые выполняют функцию зашумления в прямом процессе и функцию расшумления в обратном. Глобально их можно разделить на детерминистические и стохастические. Выбор планировщика шума для определенной задачи остается за вами, но мы рекомендуем присмотреться к [`DPMSolverMultistepScheduler`](https://huggingface.co/docs/diffusers/api/schedulers/multistep_dpm_solver) и [`EulerDiscreteScheduler`](https://huggingface.co/docs/diffusers/api/schedulers/euler).\n",
        "3. Для повышения согласованности сгенерированного изображения заданной текстовой инструкции в последних диффузионных моделях часто используют несколько текстовых энкодеров. Так, в модели **SDXL** применяют два энкодера: CLIP ViT-L и OpenCLIP-bigG, а в **Stable Diffusion 3** уже три: CLIP ViT-L, OpenCLIP-bigG и T5 XXL.\n",
        "4. Несмотря на то, что классическим выбором архитектуры для предсказания шума исконно являлся U-Net, последние диффузионные модели используют трансформерные архитектуры на основе ViT. Первая такая модель (DiT) обуславливалась на метку класса, но для text-to-image генерации этого оказалось мало. В Stable Diffusion 3 предложили новую архитектуру, которая независимыми трансформерными блоками обрабатывает текстовую и визуальную информацию, пересекая их только лишь на слое перекрестного внимания."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSj-p5FhJz3w"
      },
      "source": [
        "### Полезные источники"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvrYnvu1Jz3x"
      },
      "source": [
        "Планировщики шума:\n",
        "- https://blog.segmind.com/what-are-schedulers-in-stable-diffusion/\n",
        "- https://huggingface.co/docs/diffusers/api/schedulers/overview\n",
        "- https://stable-diffusion-art.com/samplers/\n",
        "- https://www.felixsanz.dev/articles/complete-guide-to-samplers-in-stable-diffusion\n",
        "- https://jarvislabs.ai/docs/samplers\n",
        "- https://civitai.com/articles/7484/understanding-stable-diffusion-samplers-beyond-image-comparisons\n",
        "\n",
        "Несколько текстовых энкодеров (на примере SDXL и SD3):\n",
        "- https://discuss.huggingface.co/t/sdxl-custom-pipeline-input-to-unet-why-2-text-encoders/49731/4\n",
        "- https://arxiv.org/abs/2307.01952\n",
        "- https://stabilityai-public-packages.s3.us-west-2.amazonaws.com/Stable+Diffusion+3+Paper.pdf\n",
        "- https://arxiv.org/pdf/2211.01324\n",
        "\n",
        "Трансформерные архитектуры в диффузионных моделях (на примере DiT и SD3):\n",
        "- https://arxiv.org/abs/2212.09748\n",
        "- https://github.com/facebookresearch/dit\n",
        "- https://medium.com/@threehappyer/understanding-dit-diffusion-transformer-in-one-article-2f7c330ad0ea\n",
        "- https://huggingface.co/docs/diffusers/api/pipelines/dit\n",
        "- https://github.com/lucidrains/mmdit?tab=readme-ov-file\n",
        "- https://encord.com/blog/stable-diffusion-3-text-to-image-model/\n",
        "- https://youtu.be/aSLDXdc2hkk?si=hG45HC8HCVGCWafb"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "afilatov_hed",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}