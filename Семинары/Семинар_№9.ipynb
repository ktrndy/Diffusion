{"cells":[{"cell_type":"markdown","metadata":{"id":"K0-jjmFY9nmz"},"source":["# Семинар 9. Диффузия для видео"]},{"cell_type":"markdown","metadata":{"id":"n8ZmODb-9rK1"},"source":["**Преподаватель:** Никита Киселев"]},{"cell_type":"markdown","metadata":{"id":"Ks7_n5OH9s-2"},"source":["## Введение"]},{"cell_type":"markdown","metadata":{"id":"pgMttDan9tgn"},"source":["На этом семинаре мы:\n","- Поймем, чем отличается задача генерации видео от задачи генерации изображений.\n","- Разберемся с тремя основными подходами к решению этой задачи.\n","- Адаптируем предобученную T2I диффузионную модель для имитации видео.\n","- Изучим возможные способы дообучения картиночной модели для получения качественных анимаций.\n","- Углубимся в процесс обучения T2V-диффузии с нуля, а также освоим ее расширения до задач I2V и V2V."]},{"cell_type":"markdown","metadata":{"id":"neGWA6Dh9xBT"},"source":["**План занятия:**\n","\n","1. Deforum как один из способов имитации видео.\n","2. Адаптация предобученной T2I-модели для генерации видео с AnimateDiff.\n","3. CogVideoX: универсальный подход для реализации T2V, I2V и V2V генераций.\n"]},{"cell_type":"markdown","metadata":{"id":"Prwkj4Mq921b"},"source":["## 1. Deforum"]},{"cell_type":"markdown","metadata":{"id":"knve2BTimh5M"},"source":["Рассмотрим такой подход к генерации анимаций и видео, как Deforum, исходно предложенный для моделей семейства Stable Diffusion.\n","\n","Прежде чем перейти к его реализации в коде и конкретным примерам, давайте зададимся вопросом о том, что вообще представляет собой полноценное видео и в чем его принципиальное отличие от набора отдельных изображений.\n","\n","**Вопрос.** Что отличает видео от набора отдельных изображений?\n","\n","<details>\n","  <summary><b>Ответ</b></summary>\n","\n","  <font color='green'>Согласованность кадров во времени! Да, довольно простое и естественное требование, но как мы можем потребовать его от генеративной модели? Давайте разбираться.</font>\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"65xlE66uo-pC"},"source":["Допустим, вам нужно составить текстовую инструкцию, по которой далее модель формата text-to-video (T2V) сгенерирует полноценный видеоролик. Скорее всего, первое, о чем вы задумаетесь, — как именно будет происходить изменение содержимого в кадре. Без особого труда можно придумать множество таких изменений:\n","1. Движение объекта внутри неподвижной сцены.\n","2. Смена сцены вокруг неподвижного объекта.\n","3. Движение камеры вокруг неподвижного объекта.\n","4. Движение камеры и одновременное движение объекта.\n","5. Изменение цвета.\n","6. Преобразование одного объекта в другой.\n","\n","Чаще всего мы представляем, что у нас есть некоторая 3D-сцена, внутри которой расположен объект в определенных внешних условиях, причем эта сцена каким-то образом изменяется. Следовательно, для генерации консистентного (согласованного) по времени набора кадров нужно **обязательно учесть физическое расположение объекта на сцене**."]},{"cell_type":"markdown","metadata":{"id":"5gq_ZZM90pli"},"source":["Оказывается, если ограничиться только простыми физическими изменениями положения объекта в кадре, то можно использовать для генерации видео уже хорошо известную нам text-to-image (T2I) диффузионную модель, например Stable Diffusion. Один из первых подходов в этом направлении — Deforum."]},{"cell_type":"markdown","metadata":{"id":"bRRHpZeZ0Tup"},"source":["### 1.1. Неподвижная сцена"]},{"cell_type":"markdown","metadata":{"id":"KnYQvBDS1DSH"},"source":["Для начала рассмотрим совсем простой пример, когда сцена никак не изменяется в отношении физического расположения объекта внутри. Например, мы можем захотеть сгенерировать видеоролик, где:\n","1. Сначала в центре кадра сидит **кот в шляпе**.\n","2. Затем этого кота сменяет **собака в солнцезащитных очках**.\n","\n","<figure align=\"center\">\n","    <img src=\"https://drive.google.com/uc?export=view&id=1Cwqsf9Om6BPCKeRbQZsT5Le2NquFdkUw\" alt=\"cat-dog\" width=\"600\"/>\n","</figure>\n","\n","Поскольку любое видео — это последовательность из большого числа взаимосвязанных кадров, плавно перетекающих из одного в другой, то в качестве идеи можно попробовать:\n","1. Взять текстовое описание начальной сцены.\n","2. Взять текстовое описание финальной сцены.\n","3. Перевести эти текстовые описания в эмбеддинги — векторы в определенном пространстве.\n","4. Зафиксировать число кадров в видео, например 48, то есть 2 секунды с 24 FPS.\n","5. Проинтерполировать (как?) первый и последний векторы, чтобы получить 48 плавно перетекающих друг в друга векторов.\n","6. Для каждого из векторов сгенерировать соответствующее изображение при помощи предобученной T2I-модели.\n","\n","**Вопрос.** Получится ли в таком случае согласованность между отдельными кадрами?\n","\n","<details>\n","  <summary><b>Ответ</b></summary>\n","  \n","  <font color='green'>Нет! Потому что мы никак не учитываем предыдущий кадр в качестве условия для генерации следующего! И как же быть? А давайте вспомним подход к редактированию изображений при помощи диффузионной модели, то есть image-to-image-преобразование.</font>\n","</details>\n","\n","Как нам уже известно из лекции про редактирование изображений и действия с предобученными моделями, схематично подход image-to-image (I2I) пайплайна генерации можно представить так:\n","1. Берем исходное изображение.\n","2. Зашумляем его на несколько шагов, при этом **полученный шум содержит информацию об исходной картинке**.\n","3. Берем этот шум и начинаем действовать в обратную сторону, постепенно расшумляя картинку, при этом используем уже **новое текстовое описание**.\n","4. В итоге получаем изображение, похожее на исходное, но при этом согласованное с новой текстовой инструкцией!\n","\n","<figure align=\"center\">\n","    <img src=\"https://drive.google.com/uc?export=view&id=1D6TD3VBNqpufwlIMIFPhIH7pBweh3S2u\" alt=\"sd-edit\" width=\"800\"/>\n","    <figcaption> Генерация изображения из наброска при помощи SDEdit. Синие точки показывают процесс редактирования. Зеленые и синие контуры представляют распределения картинок и набросков соответственно. Имея набросок, мы сперва зашумляем его гауссовским шумом, а затем постепенно удаляем этот шум в обратном процессе. Источник: <a href=\"https://arxiv.org/abs/2108.01073\">Meng et al. 2021</a> </figcaption>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"x0qi1Ey09POV"},"source":["Таким образом, мы пришли к тому, что представляет собой подход Deforum **для неподвижной сцены**:\n","1. Интерполяцию текстового описания между ключевыми кадрами.\n","2. Генерацию промежуточных кадров по этим проинтерполированным промптам в режиме I2I."]},{"cell_type":"markdown","metadata":{"id":"POtRXEKH-c4M"},"source":["<figure align=\"center\">\n","    <img src=\"https://drive.google.com/uc?export=view&id=1VZIvF2Cxl4uDbH030WWEMQF2syz3YU0P\" alt=\"deforum-kandi-1\" width=\"800\"/>\n","    <figcaption> 1. <b>Промпт:</b> Jungle with flowers, graphic artist Magali Villeneuve, flowers, Pixar art, green and pink colors, highly detailed leaves, Lin Tinggui style, intricate oil painting, epic, delight. 2. <b>Промпт:</b> Bird of paradise painting, graphic artist Magali Villeneuve, flowers, Pixar art, green and pink colors, highly detailed leaves, Lin Tinggui style, intricate oil painting, epic, delight. <b>Режим:</b> на месте. Источник: <a href=\"https://habr.com/ru/companies/sberbank/articles/766968/\">Kandinsky Deforum</a> </figcaption>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"NXJyI9RW9poJ"},"source":["### 1.2. Движение камеры вокруг объекта"]},{"cell_type":"markdown","metadata":{"id":"ZGuftrId9xH_"},"source":["Однако зачастую больший интерес для пользователей представляют динамичные анимации, в которых меняется расположение объектов, происходят повороты, сдвиги, искажения. Поэтому стоит расширить уже изученный подход на случай **движения камеры вокруг объекта**.\n","\n","Как мы уже обсуждали, для этого нужно каким-то образом учесть физическое распололожение объектов на изображении.\n","\n","**Вопрос.** Какой из известных вам подходов в компьютерном зрении может  справиться с такой задачей?\n","\n","<details>\n","  <summary><b>Ответ</b></summary>\n","  \n","  <font color='green'> <b>Depth Estimation, или оценка глубины!</b> В рамках этого подхода создается так называемая depthmap (карта глубины) — изображение, где для каждого пикселя вместо цвета хранится его расстояние до камеры. Диапазон значений этой карты по оси $z$ (в глубину) лежит на некотором отрезке $[z_{\\text{near}}, z_{\\text{far}}]$ (в системе координат, о которой речь пойдет чуть ниже). Существует множество способов для получения карты глубины, в том числе и нейросетевые, например [MiDaS](https://github.com/isl-org/MiDaS) и [DepthPro](https://github.com/apple/ml-depth-pro). </font>\n","\n","<figure align=\"center\">\n","    <img src=\"https://drive.google.com/uc?export=view&id=1CjHsTGEKzh6hxQyGi1Kt7lKHlw9auyKV\" alt=\"sd-edit\" width=\"800\"/>\n","    <figcaption> <b>Вверху:</b> исходные изображения. <b>Посередине:</b> карты глубины, построенные с помощью модели MiDaS. <b>Внизу:</b> соответствующие изображениям облака точек (отрендеренные с новых точек обзора). Источник: <a href=\"https://arxiv.org/abs/1907.01341\">Ranftl et al. 2019</a> </figcaption>\n","</figure>\n","\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"j3-TLb3W-8xx"},"source":["Так мы получаем представление о том, как именно расположены объекты в кадре (хотя и относительно неточное, поскольку с единственным изображением мы знаем сцену только с одной стороны). И далее необходимо понять, как **изменить положение камеры, чтобы соответствовать желаемому преобразованию сцены**."]},{"cell_type":"markdown","metadata":{"id":"48kAowx_BiXI"},"source":["Камера характеризуется координатами положения в пространстве сцены $(x, y, z)$ и направлением взгляда, которое задается тремя углами $(\\alpha, \\beta, \\gamma)$. Соответственно, чтобы задать траекторию камеры, необходимо определить зависимости\n","$$\n","    \\begin{cases}\n","        x = x(t), \\\\\n","        y = y(t), \\\\\n","        z = z(t), \\\\\n","    \\end{cases}\n","    \\qquad\n","    \\begin{cases}\n","        \\alpha = \\alpha(t), \\\\\n","        \\beta = \\beta(t), \\\\\n","        \\gamma = \\gamma(t). \\\\\n","    \\end{cases}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"zwpdHT7rCzzv"},"source":["Импровизированная съемка сцены камерой от первого лица представляет собой операцию перспективных проективных преобразований. Изначально камера зафиксирована в начале координат, а сцена удалена от нее на расстояние $z_{\\text{near}}$.\n","\n","<figure align=\"center\">\n","    <img src=\"https://habrastorage.org/r/w1560/getpro/habr/upload_files/41f/28d/a4b/41f28da4bde433383e648e9ab393e478.png\" alt=\"scene\" width=\"400\"/>\n","    <figcaption> Положение камеры и псевдо-3D-сцены в начальный момент времени. Источник: <a href=\"https://habr.com/ru/companies/sberbank/articles/766968/\">Kandinsky Deforum</a> </figcaption>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"kq3SFdOJD_M0"},"source":["Важно, что движение и повороты камеры эквивалентны тем же движениям и поворотам нашей сцены, взятыми со знаком минус. Соответственно **преобразования поворота и трансляции можно применять к нашей сцене**, а камеру зафиксировать в начале координат.\n","\n","Но из чего состоит сцена? Поскольку изображение — это набор пикселей, фактически и сцена для нас состоит из набора отдельных точек. Поэтому наши преобразования можно применить непосредственно к каждой из этих точек, вращая их относительно осей, проходящих через центр сцены, а затем применяя трансляцию этого центра. Вращение в трехмерном пространстве распадается на вращение вокруг трех ортогональных осей.\n","\n","Представляя каждую точку нашей сцены как вектор $v_i$ с началом в центре сцены, мы получаем преобразование общего вида\n","$$\n","    f(v_i) =\n","    \\begin{pmatrix}\n","        1 & 0 & 0 \\\\\n","        0 & \\cos \\alpha & \\sin \\alpha \\\\\n","        0 & -\\sin \\alpha & \\cos \\alpha \\\\\n","    \\end{pmatrix}\n","    \\begin{pmatrix}\n","        \\cos \\beta & 0 & \\sin \\beta \\\\\n","        0 & 1 & 0 \\\\\n","        - \\sin \\beta & 0 & \\cos \\beta \\\\\n","    \\end{pmatrix}\n","    \\begin{pmatrix}\n","        \\cos \\gamma & \\sin \\gamma & 0 \\\\\n","        - \\sin \\gamma & \\cos \\gamma & 0 \\\\\n","        0 & 0 & 1 \\\\\n","    \\end{pmatrix}\n","    v_i -\n","    \\begin{pmatrix}\n","        x \\\\\n","        y \\\\\n","        z\n","    \\end{pmatrix},\n","$$\n","то есть мы последовательно\n","1. Поворачиваем на угол $\\gamma$ вокруг оси $z$\n","2. Поворачиваем на угол $\\beta$ вокруг оси $y$\n","3. Поворачиваем на угол $\\alpha$ вокруг оси $x$\n","4. Сдвигаем на вектор с координатами $x, y, z$\n","\n","Следовательно, такое преобразование применяется к каждой точке для того, чтобы посмотреть на нее с камеры $(x, y, z)$ под направлением взгляда $(\\alpha, \\beta, \\gamma)$. Подробнее про операцию перспективного проектирования читайте в [статье](https://habr.com/ru/articles/252771/)."]},{"cell_type":"markdown","metadata":{"id":"K1VwVWbYF1c4"},"source":["#### Некоторые частные случаи (примеры 2D-перспективных проекций)\n","\n","<figure align=\"center\">\n","    <img src=\"https://habrastorage.org/r/w1560/getpro/habr/upload_files/1b0/074/416/1b007441643e4be83aa3fdcfec63618a.png\" alt=\"zoom\" width=\"400\"/>\n","    <figcaption> Отдаление. Источник: <a href=\"https://habr.com/ru/companies/sberbank/articles/766968/\">Kandinsky Deforum</a> </figcaption>\n","</figure>\n","\n","**Отдаление:** преобразование трансляции вдоль оси, проходящей через камеру и центр сцены. Трансляция производится в сторону удаления от камеры. Динамика движения камеры описывается уравнениями\n","$$\n","    \\begin{cases}\n","        x(t) = 0, \\\\\n","        y(t) = 0, \\\\\n","        z(t) = z_0 + v \\cdot t, \\\\\n","    \\end{cases}\n","    \\qquad\n","    \\begin{cases}\n","        \\alpha(t) = 0, \\\\\n","        \\beta(t) = 0, \\\\\n","        \\gamma(t) = 0, \\\\\n","    \\end{cases}\n","$$\n","где $v$ — скорость движения, откуда выводится динамика каждого пикселя исходного изображения.\n","\n","<figure align=\"center\">\n","    <img src=\"https://habrastorage.org/r/w1560/getpro/habr/upload_files/bf7/597/8dc/bf75978dc4079248f9e289ef9d14e1f3.png\" alt=\"rotation\" width=\"400\"/>\n","    <figcaption> Поворот. Источник: <a href=\"https://habr.com/ru/companies/sberbank/articles/766968/\">Kandinsky Deforum</a> </figcaption>\n","</figure>\n","\n","**Поворот:** преобразования поворота вдоль одной оси, проходящей через центр сцены. Динамика описывается следующими уравнениями:\n","$$\n","    \\begin{cases}\n","        x(t) = 0, \\\\\n","        y(t) = 0, \\\\\n","        z(t) = z_0, \\\\\n","    \\end{cases}\n","    \\qquad\n","    \\begin{cases}\n","        \\alpha(t) = 0, \\\\\n","        \\beta(t) = 0, \\\\\n","        \\gamma(t) = \\omega \\cdot t, \\\\\n","    \\end{cases}\n","$$\n","где $\\omega$ — угловая скорость вращения, откуда выводится динамика каждого пикселя исходного изображения.\n","\n","Обратите внимание, что карта глубины одной картинки все же не является полноценной 3D-сценой, и небольшие отклонения камеры от исходной проекции будут давать искажения, как это видно на изображениях выше. Поэтому есть два необходимых дополнения к этим рассуждениям:\n","- Каждое из последовательных преобразований должно **незначительно менять** положение сцены, то есть применяемые операции должны быть инфинитезимальными преобразованиями для получения проекции без сильного искажения.\n","- После каждого такого преобразования **необходимо применять подход I2I** для устранения на изображении возможных искажений."]},{"cell_type":"markdown","metadata":{"id":"xXh8_DGiJIFV"},"source":["<figure align=\"center\">\n","    <img src=\"https://habrastorage.org/r/w1560/getpro/habr/upload_files/b8d/62f/69d/b8d62f69dd4f683a82e9f57939cafd1d.png\" alt=\"scheme\" width=\"600\"/>\n","    <figcaption> Схема метода Deforum. Источник: <a href=\"https://habr.com/ru/companies/sberbank/articles/766968/\">Kandinsky Deforum</a> </figcaption>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"RSgRD69CJS1m"},"source":["В качестве иллюстрации рассмотрим, как устроены, например, операции отдаления и поворота (на очень наглядной картинке ниже).\n","\n","<figure align=\"center\">\n","    <img src=\"https://habrastorage.org/r/w1560/getpro/habr/upload_files/702/adc/bcc/702adcbcc172e109fb058b6f27914eae.png\" alt=\"scheme\" width=\"600\"/>\n","    <figcaption> Отдаление и поворот с Deforum. Источник: <a href=\"https://habr.com/ru/companies/sberbank/articles/766968/\">Kandinsky Deforum</a> </figcaption>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"m9q668K2tB-c"},"source":["А теперь давайте посмотрим на то, как выглядят видео, сгенерированные с другими движениями в Kandinsky Deforum."]},{"cell_type":"markdown","metadata":{"id":"8bT5zqhXtO2e"},"source":["<figure align=\"center\">\n","    <img src=\"https://habrastorage.org/getpro/habr/upload_files/7ba/b08/1e8/7bab081e82312fc0068bcaab434aa847.gif\" alt=\"deforum-kandi-2\" width=\"1000\"/>\n","    <img src=\"https://habrastorage.org/getpro/habr/upload_files/355/6c9/c50/3556c9c501f72d86cb37db5233f7a7ec.gif\" alt=\"deforum-kandi-2\" width=\"1000\"/>\n","    <figcaption> Примеры генераций с различными режимами в Kandinsky Deforum. Источник: <a href=\"https://habr.com/ru/companies/sberbank/articles/766968/\">Kandinsky Deforum</a> </figcaption>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"BOXivR4nJjHI"},"source":["### 1.3. Реализация Deforum своими руками"]},{"cell_type":"markdown","metadata":{"id":"DemrnAIRKfe5"},"source":["Наконец, перейдем к тому, как устроен метод Deforum в коде. Учитывая, что каждое из допустимых преобразований сцены задается своей математической формулой, обычно реализации Deforum довольно сложные. Поэтому здесь мы сконцентрируемся на концептуальном понимании происходящего. Для этого нам подойдет и случай с неподвижной сценой."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lw3st-8Iijgd"},"outputs":[],"source":["# Устанавливаем необходимые библиотеки\n","!pip install diffusers transformers torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_G2vlb7Mijge"},"outputs":[],"source":["# Импортируем необходимые модули\n","import torch\n","from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, DPMSolverMultistepScheduler\n","from transformers import CLIPTextModel, CLIPTokenizer\n","import numpy as np\n","import cv2\n","from tqdm import tqdm\n","from PIL import Image\n","import imageio\n","from IPython.display import display, Video"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z78qqbdrijge"},"outputs":[],"source":["# Загрузка предобученной модели Stable Diffusion\n","model_id = \"CompVis/stable-diffusion-v1-4\"\n","pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n","pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n","pipe = pipe.to(\"cuda\")\n","pipe.set_progress_bar_config(disable=True)\n","\n","# Загрузка image-to-image pipeline\n","img2img_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n","img2img_pipe.scheduler = DPMSolverMultistepScheduler.from_config(img2img_pipe.scheduler.config)\n","img2img_pipe = img2img_pipe.to(\"cuda\")\n","img2img_pipe.set_progress_bar_config(disable=True)\n","\n","# Текстовые описания начальной и финальной сцен\n","initial_prompt = \"A beautiful sunrise over a calm lake\"\n","final_prompt = \"A serene sunset over a calm lake\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pZTpgJZRijge"},"outputs":[],"source":["# Преобразование текстовых описаний в эмбеддинги\n","def get_text_embeddings(prompt):\n","    inputs = pipe.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n","    with torch.no_grad():\n","        text_embeddings = pipe.text_encoder(**inputs).last_hidden_state\n","    return text_embeddings\n","\n","initial_embeddings = get_text_embeddings(initial_prompt)\n","final_embeddings = get_text_embeddings(final_prompt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1adccZCXijge"},"outputs":[],"source":["# Число кадров в видео\n","num_frames = 6 * 8\n","\n","# Интерполяция эмбеддингов\n","def interpolate_embeddings(embed1, embed2, num_frames):\n","    alpha = np.linspace(0, 1, num_frames)\n","    interpolated_embeddings = []\n","    for a in alpha:\n","        interpolated_embedding = (1 - a) * embed1 + a * embed2\n","        interpolated_embeddings.append(interpolated_embedding)\n","    return torch.stack(interpolated_embeddings)\n","\n","interpolated_embeddings = interpolate_embeddings(initial_embeddings, final_embeddings, num_frames)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZkLzTB4aijgf"},"outputs":[],"source":["# Генерация изображений для каждого эмбеддинга\n","images = []\n","previous_image = None\n","\n","for i, embedding in enumerate(tqdm(interpolated_embeddings, desc=\"Generating images\")):\n","    if i == 0:\n","        # Генерация первого кадра\n","        with torch.no_grad():\n","            image = pipe(prompt_embeds=embedding).images[0]\n","    else:\n","        # Генерация последующих кадров с использованием image-to-image pipeline\n","        with torch.no_grad():\n","            image = img2img_pipe(prompt_embeds=embedding, image=previous_image, strength=0.6).images[0]\n","\n","    images.append(image)\n","    previous_image = image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ttneZz3Rijgf"},"outputs":[],"source":["# Переводим PIL картинки в набор np.array\n","image_arrays = [np.array(img) for img in images]\n","\n","# Инициализируем имя и число FPS\n","output_video_path = 'deforum-from-scratch.mp4'\n","fps = 8\n","\n","# Записываем картинки в файл, используя imageio\n","with imageio.get_writer(output_video_path, fps=fps) as writer:\n","    for img_array in image_arrays:\n","        writer.append_data(img_array)\n","\n","print(f\"Video saved to {output_video_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5zdxE_UYijgf"},"outputs":[],"source":["display(Video(output_video_path, embed=True))"]},{"cell_type":"markdown","source":["![](https://drive.google.com/uc?export=view&id=1zqeN8BkfRSz2V9LEVIEWvPcEhDLjSDm0)"],"metadata":{"id":"7FV6PD_DkGpf"}},{"cell_type":"markdown","metadata":{"id":"IeEWc4COvYnq"},"source":["Получилась интересная анимация, и для ее генерации мы не прибегали ни к какому дообучению и тем более обучению моделей. Таким образом, мы разобрали один из возможных **training-free (без обучения)** подходов к text-to-video (T2V) генерации."]},{"cell_type":"markdown","metadata":{"id":"t3_kT_a_94RI"},"source":["## 2. AnimateDiff"]},{"cell_type":"markdown","metadata":{"id":"bm-aN_HP0awo"},"source":["Перейдем к следующему виду моделей генерации видео, который существенно отличается от предыдущего. Как мы уже отметили, предыдущий подход заключался в **использовании предобученной text-to-image (T2I) диффузионной модели**. Вполне естественно ожидать, что если **дообучить такую модель на наборе видео**, то мы получим куда лучшее качество генерации.\n","\n","На самом деле сейчас все подходы к генерации видео можно разделить на три группы:\n","1. Использование T2I-модели без дообучения (Deforum и так далее).\n","2. Дообучение T2I-модели на наборе видео (AnimateDiff и так далее).\n","3. Обучение T2V-модели с нуля (CogVideoX, HunyuanVideo и так далее).\n","\n","Постепенно мы рассмотрим каждую из возможных альтернатив, но сейчас давайте сфокусируемся на варианте с дообучением. Ниже увидите примеры видео, сгенерированных при помощи такой модели AnimateDiff."]},{"cell_type":"markdown","metadata":{"id":"eFASxwvLzgY8"},"source":["### 2.1. Мотивация и архитектура"]},{"cell_type":"markdown","metadata":{"id":"7wNANiz8zj77"},"source":["Авторы статьи [AnimateDiff](https://arxiv.org/abs/2307.04725) подошли к задаче генерации видео в первую очередь со стороны данных, на которых такая модель должна учиться. Совершенно понятно, что модель T2I-формата, например Stable Diffusion, училась на огромном наборе качественных пар (текст, изображение). Однако собрать датасет с хорошо размеченными видеороликами, к тому же такими, чтобы на них не было артефактов, дефектов, они были динамичными, — очень сложно.\n","\n","Поэтому подход состоит из дообучения в **три последовательных этапа**:\n","1. Уничтожение негативных эффектов от обучения на новом наборе видео при помощи **Domain Adapter (адаптера домена)**.\n","2. Обучение базовым движениям с **Motion Module (модулем движения)**.\n","3. Адаптация к новым движениям с **MotionLoRA**.\n","\n","Общая схема метода представлена ниже.\n","\n","<figure align=\"center\">\n","    <img src=\"https://drive.google.com/uc?export=view&id=1lWuuz0FVMgHKvLhAtX7CglFakWaZRQE2\" alt=\"animatediff-scheme\" width=\"1000\"/>\n","    <figcaption> <b>Пайплайн обучения AnimateDiff</b>. AnimateDiff имеет три стадии обучения для соответствуюших модулей. 1) Сначала domain adapter обучается для того, чтобы минимизировать разницу в распределениях доменов выбранной T2I-модели и обучающего набора видео. 2) Затем вставляется motion module, который выучиывает основные базовые принципы движения из видео. 3) Наконец (опционально), MotionLoRA обучается на небольшом числе примеров, чтобы адаптироваться под движения конкретно из них. Источник: <a href=\"https://arxiv.org/abs/2307.04725\">Guo et al. 2023</a> </figcaption>\n","</figure>\n","\n","Далее мы пройдемся подробно по каждому блоку, но сперва давайте вспомним базовые вещи из теории диффузионных моделей, а также дообучения нейронных сетей.\n","\n","**Stable Diffusion.** В качестве базовой T2I-модели авторы исходно взяли Stable Diffusion (SD). SD прозводит диффузионный процесс в латентном пространстве предобученного автоэнкодера $\\mathcal{E}(\\cdot)$ и $\\mathcal{D}(\\cdot)$. В процессе обучения закодированное изображение $\\mathbf{z}_0 = \\mathcal{E}(\\mathbf{x}_0)$ зашумляется до $\\mathbf{z}_t$ прямым процессом:\n","$$\n","    \\mathbf{z}_t = \\sqrt{\\bar{\\alpha}_t} \\mathbf{z}_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\mathbf{I}),\n","$$\n","для $t = 1, \\ldots, T$, где заранее заданные $\\bar{\\alpha_t}$ определяют силу зашумления на шаге $t$. Нейросеть $\\boldsymbol{\\epsilon}_{\\theta}(\\cdot)$ пытается инвертировать этот процесс, предсказывая добавленный шум, при этом минимизируя функцию потерь:\n","$$\n","    \\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathcal{E}(\\mathbf{x}_0), \\mathbf{y}, \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\mathbf{I}), t} \\left[ \\left\\| \\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{z}_t, t, \\tau_{\\phi}(\\mathbf{y})) \\right\\|_2^2 \\right],\n","$$\n","где $\\mathbf{y}$ — это текстовый промпт, соотвествующий $\\mathbf{x}_0$, а $\\tau_{\\phi}(\\cdot)$ — это текстовый энкодер. В Stable Diffusion нейросеть $\\boldsymbol{\\epsilon}_{\\theta}(\\cdot)$ реализована в виде U-Net. При этом на каждом из блоков down/up-сэмплинга находятся **слои само- и перекрестного внимания**, которые связывают генерацию с текстовой инструкцией. Далее они нам еще очень пригодятся!\n","\n","**Вопрос.** Какие ключевые сущности есть у слоя внимания (Attention)?\n","\n","<details>\n","  <summary><b>Ответ</b></summary>\n","  \n","  <font color='green'>Queries, Keys, Values — проекции исходных представлений на новые пространства признаков.</font>\n","</details>\n","\n","\n","**Low-rank adaptation (LoRA).** Как вы можете знать, существует подход к дообучению больших моделей, который основан на низкоранговой аппроксимации линейных слоев, он называется LoRA. Вместо того чтобы учить все параметры исходной модели, LoRA для каждого линейного слоя с весами $\\mathbf{W}$ добавляет две новые матрицы $\\mathbf{A}$ и $\\mathbf{B}$, а затем производит только их оптимизацию. Более конкретно, представьте, что вы дообучаете этот линейный слой так, что вносите изменение $\\Delta \\mathbf{W}$, то есть получаете новые веса\n","$$\n","    \\mathbf{W}' = \\mathbf{W} + \\Delta \\mathbf{W}.\n","$$\n","Идея в том, чтобы матрицу $\\Delta \\mathbf{W} \\in \\mathbb{R}^{m \\times n}$ приблизить произведением двух матриц **маленького ранга** (отсылаем вас к теории вычислительной алгебры и разложениям матриц) $\\mathbf{A} \\in \\mathbb{R}^{m \\times r}$ и $\\mathbf{B} \\in \\mathbb{R}^{n \\times r}$:\n","$$\n","    \\mathbf{W}' = \\mathbf{W} + \\Delta \\mathbf{W} = \\mathbf{W} + \\mathbf{A} \\mathbf{B}^\\top.\n","$$\n","Причем $r$ является гиперпараметром, и вы можете самостоятельно его подбирать, опираясь на свои нужды.\n","\n","**Вопрос.** Представьте, что у вас есть линейный слой $\\texttt{Linear}(1028, 1028)$, к которому вы применяете дообучение с LoRA ранга $r = 16$. На сколько процентов меньше параметров нужно учить по сравнению с дообучением всего слоя?\n","\n","<details>\n","  <summary><b>Ответ</b></summary>\n","  \n","  <font color='green'>В таком случае матрица, которую бы вы учили в обычном случае, имела бы размер $1028 \\times 1028 = 2^{10} \\times 2^{10} = 2^{20} \\approx 1$M параметров. Если же использовать LoRA, то число обучаемых параметров составит $1028 \\times 16 + 16 \\times 1028 = 2 \\times 2^4 \\times 2^{10} = 2^{15} \\approx 30$k параметров. Таким образом, выгода от использования LoRA практически 97%! </font>\n","</details>\n","\n","Помимо преимущества в числе обучаемых параметров (и, как следствие, меньшем объеме необходимой памяти на устройстве), есть еще несколько плюсов использования такого подхода:\n","- меньшая склонность к переобучению;\n","- меньшая вероятность сдвига домена;\n","- взаимозаменяемость LoRA-модулей (например, можно обучить один модуль под домен аниме, а другой — под домен картин Малевича).\n","\n","**Важно отметить**, на каких именно слоях чаще всего применяется LoRA. На практике обычно используют ее на Attention-слоях: само- и перекрестного внимания. На то есть как минимум две причины: 1) эти слои линейные с точностью до нелинейного $\\texttt{softmax}$ в конце; 2) именно они чаще всего содержат информацию о взаимосвязи текста и картинки, так что полезно учить именно их."]},{"cell_type":"markdown","metadata":{"id":"-wM7URV_-uQ_"},"source":["### 2.2. Domain Adapter"]},{"cell_type":"markdown","metadata":{"id":"QzgF1aFB_FK7"},"source":["Поскольку мы хотим научить модель генерировать видео, мы должны сперва адаптировать ее к тому домену, из которого они взяты. В первую очередь важно отметить, что если **дообучать T2I модель на отдельных кадрах из видео**, то часто будут попадаться такие артефакты, как смазанные или низкокачественные картинки, на которых также могут быть и водяные знаки.\n","\n","Авторы проводят детальное сравнение, в результате которого выясняется, что обучение дополнительного **Domain Adapter** помогает модели лучше выучить динамику видео, поскольку минимизирует зазор между доменами.\n","\n","Чтобы не терять качество базовой T2I-модели, авторы предлагают использовать в качестве такового адаптера именно LoRA, которые удобно и понятно взаимозаменяемы. А именно, они вставляют LoRA в слои само- и перекрестного внимания T2I-модели Stable Diffusion.\n","\n","Для примера рассмотрим проекцию query (Q). Преобразование внутренней переменной $\\mathbf{z}$ после такой проекции выглядит как\n","$$\n","    Q = \\mathbf{W}_Q \\mathbf{z} + \\texttt{AdapterLayer}(\\mathbf{z}) = \\mathbf{W}_Q \\mathbf{z} + \\alpha \\cdot \\mathbf{A} \\mathbf{B}^\\top \\mathbf{z}.\n","$$\n","Здесь скаляр $\\alpha = 1$ может настраиваться в процессе инференса (например, можно поставить $\\alpha = 0$, чтобы обнулить эффект адаптера домена)."]},{"cell_type":"markdown","metadata":{"id":"f3rWsG0V_BQe"},"source":["### 2.3. Motion Module"]},{"cell_type":"markdown","metadata":{"id":"nfcv567c_FDo"},"source":["Чтобы смоделировать динамику вдоль временной размерности поверх предобученной T2I модели, мы должны\n","1. Подготовить 2D-диффузионную модель к работе с 3D-видеоданными.\n","2. Спроектировать такой модуль, который бы эффективно распространял информацию вдоль временной оси.\n","\n","**Адаптация к 3D-данным.** Здесь все достаточно просто, поскольку по умолчанию диффузионная модель работает с изображениями **независимо**. Следовательно, базовым решением будет преобразование входной последовательности видео $\\mathbf{x} \\in \\mathbb{R}^{b \\times c \\times f \\times h \\times w}$ в последовательность изображений $\\tilde{\\mathbf{x}} \\in \\mathbb{R}^{(b \\times f) \\times c \\times h \\times w}$.\n","\n","**Дизайн самого модуля.** Последние работы в области генерации видео исследовали различные варианты дизайна архитектуры для моделирования временной компоненты. В AnimateDiff авторы адаптировали для этой цели **трансформерную архитектуру**. В нее входят несколько слоев self-attention-блоков вдоль временной оси, а также синусоидальные позиционные эмбеддинги для кодирования локации каждого кадра в анимации."]},{"cell_type":"markdown","metadata":{"id":"BbeaudDeEyDp"},"source":["Поскольку motion module (модуль движения) никак не учитывает **пространственную взаимосвязь**, для него все $h \\times w$ пикселей на самом деле равноправны. Поэтому входную последовательность кадров стоит рассматривать как такой набор матриц:\n","$$\n","    \\mathbf{z}_1, \\ldots, \\mathbf{z}_f, \\quad \\mathbf{z}_i \\in \\mathbb{R}^{(b \\times h \\times w) \\times c}.\n","$$\n","Таким образом, слои временного само-внимания можно представить как\n","$$\n","    \\mathbf{z}_{\\text{out}} = \\texttt{Attention}(Q, K, V) = \\texttt{softmax}(QK^\\top / \\sqrt{c}) \\cdot V,\n","$$\n","где матрицы проекций обозначены соответственно через $Q = \\mathbf{W}_Q \\mathbf{z}$, $K = \\mathbf{W}_K \\mathbf{z}$ и $V = \\mathbf{W}_V \\mathbf{z}$."]},{"cell_type":"markdown","metadata":{"id":"4AjDqHDn_DON"},"source":["### 2.4. MotionLoRA"]},{"cell_type":"markdown","metadata":{"id":"V62c8t0y_ExO"},"source":["В то время как обученный motion module понимает общие паттерны движений, возникает вопрос, когда мы хотим эффективно адаптировать нашу модель под новые движения, такие как:\n","- приближение камеры;\n","- отдаление камеры;\n","- поворот камеры;\n","- смещение камеры;\n","- определенные движения объекта.\n","\n","Авторы предлагают **MotionLoRA** для решения этой проблемы. Архитектура на самом деле простая: они просто вешают LoRA-слои на self-attention-блоки модуля движения.\n","\n","**Вопрос.** Руководствуясь примерами выше, подумайте: к каким матрицам применяются LoRA-слои?\n","\n","<details>\n","  <summary><b>Ответ</b></summary>\n","  \n","  <font color='green'>К матрицам $Q$, $K$ и $V$.</font>\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"6jOxlkQpGjhs"},"source":["### 2.5. Контролируемая генерация"]},{"cell_type":"markdown","metadata":{"id":"uppoJ7XqGpk9"},"source":["Один из главных плюсов модели AnimateDiff в том, что такая архитектура позволяет использовать все обученные блоки **независимо и взаимозаменяемо.**\n","\n","Поэтому появляется еще одна опция у AnimateDiff — генерация с дополнительным условием, например, через [ControlNet](https://arxiv.org/abs/2302.05543). Это возможно потому, что веса ControlNet также довешиваются **поверх предобученной T2I-модели**.\n","\n","Таким образом, можно использовать AnimateDiff в том числе и для задач:\n","- генерации видео с учетом движения человека (open pose);\n","- стилизации видео (depth, canny)."]},{"cell_type":"markdown","metadata":{"id":"OPBhIZmIH6wR"},"source":["### 2.6. Использование AnimateDiff в коде"]},{"cell_type":"markdown","metadata":{"id":"zAZoWYKQH-mt"},"source":["Перейдем к инференсу — самой интересной части занятия, связанной с диффузионными моделями. Здесь все довольно просто, поскольку мы будем использовать уже известную вам библиотеку [🤗 Diffusers](https://huggingface.co/docs/diffusers/index). Вы можете ознакомиться с полным списком возможных реализаций [здесь](https://huggingface.co/docs/diffusers/api/pipelines/animatediff). Сейчас же мы сфокусируемся на text-to-video (T2V) и video-to-video (V2V) пайплайнах, которые обсудили выше."]},{"cell_type":"markdown","metadata":{"id":"PIMTp9WKIQej"},"source":["Начнем с примера T2V-генерации с использованием базовой модели RealisticVisionV5.1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lYX6xB_nIrDU"},"outputs":[],"source":["import torch\n","from diffusers import AnimateDiffPipeline, DDIMScheduler, MotionAdapter\n","from diffusers.utils import export_to_gif\n","\n","# Подгружаем motion adapter\n","adapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5-2\", torch_dtype=torch.float16)\n","# Подгружаем базовую модель, основанную на SD 1.5\n","model_id = \"SG161222/Realistic_Vision_V5.1_noVAE\"\n","pipe = AnimateDiffPipeline.from_pretrained(model_id, motion_adapter=adapter, torch_dtype=torch.float16)\n","scheduler = DDIMScheduler.from_pretrained(\n","    model_id,\n","    subfolder=\"scheduler\",\n","    clip_sample=False,\n","    timestep_spacing=\"linspace\",\n","    beta_schedule=\"linear\",\n","    steps_offset=1,\n",")\n","pipe.scheduler = scheduler\n","\n","# enable memory savings\n","pipe.enable_vae_slicing()\n","pipe.enable_model_cpu_offload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TXM0_Y2VJTk8"},"outputs":[],"source":["output = pipe(\n","    prompt=(\n","        \"masterpiece, bestquality, highlydetailed, ultradetailed, sunset, \"\n","        \"orange sky, warm lighting, fishing boats, ocean waves seagulls, \"\n","        \"rippling water, wharf, silhouette, serene atmosphere, dusk, evening glow, \"\n","        \"golden hour, coastal landscape, seaside scenery\"\n","    ),\n","    negative_prompt=\"bad quality, worse quality\",\n","    num_frames=16,\n","    guidance_scale=7.5,\n","    num_inference_steps=25,\n","    generator=torch.Generator(\"cpu\").manual_seed(42),\n",")\n","frames = output.frames[0]\n","export_to_gif(frames, \"animation.gif\")"]},{"cell_type":"markdown","metadata":{"id":"m7glAEY9L2Zr"},"source":["<figure align=\"center\">\n","    <img src=\"https://drive.google.com/uc?export=view&id=1Zd0FXOMjBCvkhU4fSOkSyUsZj24-jYUM\" alt=\"animation\" width=\"400\"/>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"bo-q9zgMI0G1"},"source":["У нас получилась классная и реалистичная генерация морских волн. Теперь давайте попробуем реализовать V2V-генерацию при помощи ControlNet-модуля."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dzRvryZ8K4dy"},"outputs":[],"source":["# !pip install controlnet_aux"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lGVeg6jlJcUp"},"outputs":[],"source":["import torch\n","from diffusers import AnimateDiffControlNetPipeline, AutoencoderKL, ControlNetModel, MotionAdapter, LCMScheduler\n","from diffusers.utils import export_to_gif, load_video\n","\n","# Дополнительно нам потребуется предобработать видео, прежде чем использовать его с ControlNet\n","from controlnet_aux.processor import ZoeDetector\n","\n","# Подгружаем предобученный ControlNet\n","controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-depth\", torch_dtype=torch.float16)\n","\n","# Мы будем использовать модуль AnimateLCM для этого примера, однако можно использовать и другие\n","motion_adapter = MotionAdapter.from_pretrained(\"wangfuyun/AnimateLCM\")\n","\n","vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\", torch_dtype=torch.float16)\n","pipe = AnimateDiffControlNetPipeline.from_pretrained(\n","    \"SG161222/Realistic_Vision_V5.1_noVAE\",\n","    motion_adapter=motion_adapter,\n","    controlnet=controlnet,\n","    vae=vae,\n",").to(device=\"cuda\", dtype=torch.float16)\n","pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config, beta_schedule=\"linear\")\n","pipe.load_lora_weights(\"wangfuyun/AnimateLCM\", weight_name=\"AnimateLCM_sd15_t2v_lora.safetensors\", adapter_name=\"lcm-lora\")\n","pipe.set_adapters([\"lcm-lora\"], [0.8])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iH4hCkd0Jdjz"},"outputs":[],"source":["# Для того чтобы генерировать при помощи ControlNet, нам понадобятся модели,\n","# которые для изображений будут возвращать condition например, карту глубины\n","depth_detector = ZoeDetector.from_pretrained(\"lllyasviel/Annotators\").to(\"cuda\")\n","video = load_video(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/animatediff-vid2vid-input-1.gif\")\n","conditioning_frames = []\n","\n","with pipe.progress_bar(total=len(video)) as progress_bar:\n","    for frame in video:\n","        conditioning_frames.append(depth_detector(frame))\n","        progress_bar.update()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4k5ZioLnJBTP"},"outputs":[],"source":["prompt = \"a panda, playing a guitar, sitting in a pink boat, in the ocean, mountains in background, realistic, high quality\"\n","negative_prompt = \"bad quality, worst quality\"\n","\n","video = pipe(\n","    prompt=prompt,\n","    negative_prompt=negative_prompt,\n","    num_frames=len(video),\n","    num_inference_steps=10,\n","    guidance_scale=2.0,\n","    conditioning_frames=conditioning_frames,\n","    generator=torch.Generator().manual_seed(42),\n",").frames[0]\n","\n","export_to_gif(video, \"animatediff_controlnet.gif\", fps=8)"]},{"cell_type":"markdown","metadata":{"id":"fKX2BLExMkRw"},"source":["<figure align=\"center\">\n","    <img src=\"https://drive.google.com/uc?export=view&id=1y6KfxFzFo7qn_1gx8smWTKSY9jn47R2w\" alt=\"animatediff_controlnet\" width=\"400\"/>\n","</figure>\n","\n","<figure align=\"center\">\n","    <img src=\"https://drive.google.com/uc?export=view&id=1HzawCorwVSM4Uykrbi7KBx2E0MxxH1Sk\" alt=\"animatediff_controlnet\" width=\"400\"/>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"7QxSertpJD-9"},"source":["Получилось круто! В исходном видео мы видим енота, играющего на гитаре, а в сгенерированном нами — уже панду. 😀"]},{"cell_type":"markdown","metadata":{"id":"OLVWVEmtKSao"},"source":["Отметим, что AnimateDiff действительно популярная модель, поэтому для нее вы найдете великое множество различных адаптеров, которые позволят создать именно тот контент, который вы пожелаете!"]},{"cell_type":"markdown","metadata":{"id":"mOSKePxJ0X_T"},"source":["## 3. CogVideoX"]},{"cell_type":"markdown","metadata":{"id":"_AR1DaK5GpWK"},"source":["В завершение этого занятия мы рассмотрим **CogVideoX** — одну из ключевых моделей на  момент создания материала. Напомним, что ранее мы уже рассмотрели модели, основанные на 1) использовании предобученной T2I; 2) дообучении такой модели. CogVideoX отличается от них тем, что **обучена полностью с нуля** одновременно **и на картинках, и на видео**."]},{"cell_type":"markdown","metadata":{"id":"2rRiCZ5O0Zy7"},"source":["Поскольку здесь мы ставим задачу обучить модель с нуля, важно учесть каждую из ее частей по отдельности, а именно:\n","- автоэнкодер, который преобразовывает видео в латентные переменные и обратно;\n","- модель предсказания шума в латентном пространстве."]},{"cell_type":"markdown","metadata":{"id":"wLw21Y29OVLa"},"source":["Далее мы рассмотрим каждую из составляющих более детально, но сейчас для начала вдохновимся красивыми генерациями CogVideoX."]},{"cell_type":"markdown","metadata":{"id":"aFZz9e2_OqHC"},"source":["> An elderly gentleman, with a serene expression, sits at the water's edge, a steaming cup of tea by his side. He is engrossed in his artwork, brush in hand, as he renders an oil painting on a canvas that's propped up against a small, weathered table. The sea breeze whispers through his silver hair, gently billowing his loose-fitting white shirt, while the salty air adds an intangible element to his masterpiece in progress. The scene is one of tranquility and inspiration, with the artist's canvas capturing the vibrant hues of the setting sun reflecting off the tranquil sea.\n","\n","![](https://drive.google.com/uc?export=view&id=1Zlwd1oI5n6hWPLSZoEyqfMza0E1zZi0w)\n","\n","> A garden comes to life as a kaleidoscope of butterflies flutters amidst the blossoms, their delicate wings casting shadows on the petals below. In the background, a grand fountain cascades water with a gentle splendor, its rhythmic sound providing a soothing backdrop. Beneath the cool shade of a mature tree, a solitary wooden chair invites solitude and reflection, its smooth surface worn by the touch of countless visitors seeking a moment of tranquility in nature's embrace.\n","\n","![](https://drive.google.com/uc?export=view&id=1YFzaQZqqNC03SNSIW7tGSAHmWf_XtbAu)\n","\n","> A golden retriever, sporting sleek black sunglasses, with its lengthy fur flowing in the breeze, sprints playfully across a rooftop terrace, recently refreshed by a light rain. The scene unfolds from a distance, the dog's energetic bounds growing larger as it approaches the camera, its tail wagging with unrestrained joy, while droplets of water glisten on the concrete behind it. The overcast sky provides a dramatic backdrop, emphasizing the vibrant golden coat of the canine as it dashes towards the viewer.\n","\n","![](https://drive.google.com/uc?export=view&id=1PT2I-Ji3KafMXwAN9KFCGYCCWanBjZt7)\n","\n","> A suited astronaut, with the red dust of Mars clinging to their boots, reaches out to shake hands with an alien being, their skin a shimmering blue, under the pink-tinged sky of the fourth planet. In the background, a sleek silver rocket, a beacon of human ingenuity, stands tall, its engines powered down, as the two representatives of different worlds exchange a historic greeting amidst the desolate beauty of the Martian landscape.\n","\n","![](https://drive.google.com/uc?export=view&id=1APVP-f61f2n65QpekbKZFD22R1YnBKgZ)\n","\n","> In a dimly lit bar, purplish light bathes the face of a mature man, his eyes blinking thoughtfully as he ponders in close-up, the background artfully blurred to focus on his introspective expression, the ambiance of the bar a mere suggestion of shadows and soft lighting.\n","\n","![](https://drive.google.com/uc?export=view&id=1PGLP-iRBSB4WQoFRkWABKq7cyImtFPMw)\n","\n","> On a brilliant sunny day, the lakeshore is lined with an array of willow trees, their slender branches swaying gently in the soft breeze. The tranquil surface of the lake reflects the clear blue sky, while several elegant swans glide gracefully through the still water, leaving behind delicate ripples that disturb the mirror-like quality of the lake. The scene is one of serene beauty, with the willows' greenery providing a picturesque frame for the peaceful avian visitors.\n","\n","![](https://drive.google.com/uc?export=view&id=1XcXmDzlmeUqAdk73COIFNnkPe17Qw8Sa)"]},{"cell_type":"markdown","metadata":{"id":"j3t6JOfbQ-9c"},"source":["Итак, давайте же разберемся, что именно позволяет CogVideoX генерировать настолько качественные и детализированные видео только лишь по текстовым инструкциям."]},{"cell_type":"markdown","metadata":{"id":"Wm4M8m6vRGvr"},"source":["### 3.1. 3D Causal Vae"]},{"cell_type":"markdown","metadata":{"id":"gyE9AE6jRI3k"},"source":["Мы начнем с разбора архитектуры автоэнкодера как одной из главных составляющих качественной диффузионной модели. Так как видео содержат и пространственную, и временную информацию, то **использование автоэнкодера совершенно необходимо**, иначе объем данных был бы просто огромным.\n","\n","Чтобы справиться с вычислительными сложностями моделирования видео данных, авторы [CogVideoX](https://arxiv.org/abs/2408.06072) предлагают использовать 3D вариационный автоэнкодер (3D VAE). Идея состоит в том, чтобы внедрить для сжатия видео **3D сверточные слои**, чтобы сжимать по обеим размерностям — пространственной и временной. Это позволяет достичь большего сжатия с максимальным качеством и непрерывностью в реконструкции видео."]},{"cell_type":"markdown","metadata":{"id":"I3i3FYaZS9Dy"},"source":["<figure align=\"center\">\n","    <img src=\"https://drive.google.com/uc?export=view&id=1sCkMZpEWl5KeBshqCfwjCHyEJ2QjYjrZ\" alt=\"cogvideox-vae\" width=\"1000\"/>\n","    <figcaption> (a) Структура 3D VAE в CogVideoX. Он состоит из энкодера, декодера и латентного регуляризатора, при этом обеспечивая $8 \\times 8 \\times 4$ сжатие из пиксельного пространства в латентное. (b) Реализация временной причинной свертки с контекстной параллелизацией.  Источник: <a href=\"https://arxiv.org/abs/2408.06072\">Yang et al. 2024</a> </figcaption>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"ywOnyvqkSOXj"},"source":["Схема выше иллюстрирует структуру предлагаемого 3D VAE:\n","- Он состоит из энкодера, декодера и регуляризатора латентного пространства — Kullback—Leibler (KL) regularizer.\n","- Энкодер и декодер состоят из симметричных блоков, соответственно производящих $2\\times$ downsampling и upsampling, между которыми располагаются промежуточные ResNet-блоки.\n","- Некоторые блоки также осуществляют 3D downsampling (upsampling), в то время как остальные — только 2D."]},{"cell_type":"markdown","metadata":{"id":"WesM2zdtS8ym"},"source":["Авторы применяют **сверточные слои с временной причинной связью**, которые добавляют паддинги в начало сверточного пространства. Благодаря этому **информация из будущего не влияет на предсказание настоящего или тем более прошлого**."]},{"cell_type":"markdown","metadata":{"id":"xIw2VBPzUJnr"},"source":["Также используется уже и чисто технический трюк, который позволяет уменьшить время коммуникации между отдельными GPU при обучении. Идея в применении **контекстной параллелизации по временной размерности для 3D сверток**, чтобы распределить вычисления между девайсами. Из-за причинной природы таких сверток каждая GPU просто отправляет сегмент длины $k-1$ следующей GPU, где $k$ обозначает размер ядра по времени."]},{"cell_type":"markdown","metadata":{"id":"31wLhzwyVTrg"},"source":["### 3.2. Expert Transformer"]},{"cell_type":"markdown","metadata":{"id":"q0OkDoyYVWSL"},"source":["Интересно, что большинство state-of-the-art моделей генерации видео используют именно трансформерную архитектуру для предсказания шума, в том числе и CogVideoX. Давайте разберемся, как именно происходит обработка входных данных в таком трансформере."]},{"cell_type":"markdown","metadata":{"id":"dMqZ0UvaVklF"},"source":["<figure align=\"center\">\n","    <img src=\"https://drive.google.com/uc?export=view&id=197pCgPWHnfiTQy4F4EJ3Rncdb4VawgVE\" alt=\"cogvideox-vae\" width=\"500\"/>\n","    <figcaption> Полная архитектура CogVideoX. Источник: <a href=\"https://arxiv.org/abs/2408.06072\">Yang et al. 2024</a> </figcaption>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"bkUab8wdWrr1"},"source":["**Patchify.** Как вы знаете из занятия про современные архитектуры в диффузии, любой диффузионный трансформер (DiT) начинает свою работу с разбиения входного латента на отдельные части — патчи. Так, 3D VAE кодирует видео в латент размера $T \\times H \\times W \\times C$, а затем слой патчификации разбивает его на последовательность $\\mathbf{z}_{\\text{vision}}$ длины $\\frac{T}{q} \\cdot \\frac{H}{p} \\cdot \\frac{W}{p}$."]},{"cell_type":"markdown","metadata":{"id":"ALoIdql7XfDW"},"source":["**3D-RoPE.** В трансформерных моделях позиционные эмбеддинги используются для добавления информации о порядке токенов в последовательности. Это необходимо, так как самовнимание (self-attention) в Transformer не учитывает порядок токенов. Обычные позиционные эмбеддинги добавляются к входным эмбеддингам токенов, чтобы модель могла учитывать порядок токенов в последовательности. Пример обычного позиционного эмбеддинга:\n","$$\n","    \\text{Input Embedding} + \\text{Positional Embedding}\n","$$\n","\n","[Rotary Positioned Embeddings](https://arxiv.org/abs/2104.09864) (RPE) — это более сложный метод добавления позиционной информации к эмбеддингам токенов. Вместо того чтобы просто добавлять позиционные эмбеддинги к входным эмбеддингам, RPE использует вращение (rotation) в пространстве эмбеддингов.\n","\n","Как это работает:\n","- **Разделение эмбеддингов.** Эмбеддинги токенов разделяются на две части, которые можно интерпретировать как координаты в комплексном пространстве.\n","- **Вращение.** Каждая позиция в последовательности ассоциируется с углом вращения. Эмбеддинги токенов вращаются на этот угол в комплексном пространстве.\n","- **Комбинирование.** Вращенные эмбеддинги используются в модели вместо исходных эмбеддингов.\n","\n","Формально, если у нас есть эмбеддинг токена $\\mathbf{x}$ и позиционный эмбеддинг $\\mathbf{p}$, то RPE можно описать следующим образом:\n","$$\n","    \\mathbf{x}' = R(\\theta) \\cdot \\mathbf{x},\n","$$\n","где $R(\\theta)$ — матрица вращения, зависящая от позиции $\\theta$.\n","\n","Основное преимущество RPE — более гладкая интерполяция, так как они позволяют более плавно интерполировать позиционную информацию.\n"]},{"cell_type":"markdown","metadata":{"id":"WA838zBLZNQu"},"source":["**Expert Adaptive Layernorm.** Авторы конкатенируют текстовые и видеоэмбеддинги, чтобы лучше выравнить их семантическую близость. Однако, поскольку пространства признаков текста и видео все же сильно отличаются, их эмбеддинги, скорее всего, имеют различный масштаб. Чтобы учесть это, предлагается использовать, как и в оригинальном [DiT](https://arxiv.org/abs/2212.09748), модулирование сигнала путем подачи временного шага $t$ из диффузионного процесса на вход механизма модуляции.\n","\n","Затем **Vision** Expert Adaptive Layernorm и **Text** Expert Adaptive Layernorm применяют эти масштабирования к промежуточным состояниям и таким образом уравновешивают признаки."]},{"cell_type":"markdown","metadata":{"id":"EoxNKZfuaIYf"},"source":["**3D Full Attention.** Несмотря на то, что большинство предыдущих работ обычно разделяли внимание на пространственную и временную размерности, авторы CogVideoX решили от этого отказаться в пользу лучшего качества модели, используя полноценный 3D Full Attention.\n","\n","<figure align=\"center\">\n","    <img src=\"https://drive.google.com/uc?export=view&id=1O9KeQuRSelnrOu6Aq5EnbadR7LuhDoIm\" alt=\"cogvideox-vae\" width=\"500\"/>\n","    <figcaption> Раздельные слои пространственного и временного внимания не справляются с полноценным учетом движения в кадре. Так, на рисунке голова человека на кадре i+1 не может явно \"обратить внимание\" на нее же, но на i-м кадре. Источник: <a href=\"https://arxiv.org/abs/2408.06072\">Yang et al. 2024</a> </figcaption>\n","</figure>\n","\n","Конечно, такой подход более мощный, поскольку позволяет учесть многие краевые эффекты (см. картинку выше), однако он требует значительно больших вычислений, а потому и время работы модели сильно увеличивается."]},{"cell_type":"markdown","metadata":{"id":"5EH7-be8bg2H"},"source":["### 3.3. Использование CogVideoX в коде"]},{"cell_type":"markdown","metadata":{"id":"P_KN2-qmbkKB"},"source":["Наконец, мы можем перейти от теории к реализации. Здесь мы, как обычно, воспользуемся нашей любимой библиотекой [🤗 Diffusers](https://huggingface.co/docs/diffusers/index) и импортируем уже заранее подготовленные пайплайны:\n","- Text-to-Video\n","- Image-to-Video\n","- Video-to-Video\n","\n","В процессе проговорим каждый из них чуть подробнее."]},{"cell_type":"markdown","metadata":{"id":"ZfQIHNhab9EJ"},"source":["#### 3.3.1. Text-to-Video"]},{"cell_type":"markdown","metadata":{"id":"_HC2q6KWdxDR"},"source":["Здесь все довольно просто — пайплайн такой же, как мы обсуждали ранее. Подаем текстовое описание, а модель на выходе генерирует соответствующее ему видео."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CFXWjwICb_EG"},"outputs":[],"source":["!pip install diffusers transformers hf_transfer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I8LIY48IcIlT"},"outputs":[],"source":["# !pip install git+https://github.com/huggingface/accelerate\n","!pip install accelerate==0.33.0"]},{"cell_type":"markdown","metadata":{"id":"0arvq8EbcJo-"},"source":["Импортируем необходимые библиотеки. Кусок кода ниже опционален, однако если его включить, то подгрузка модели с HF Hub будет быстрее."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aXfkUMTGcRqv"},"outputs":[],"source":["import os\n","os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RIa-FEjQcSov"},"outputs":[],"source":["import torch\n","from diffusers import (\n","    AutoencoderKLCogVideoX,\n","    CogVideoXTransformer3DModel,\n","    CogVideoXPipeline,\n","    CogVideoXImageToVideoPipeline,\n","    CogVideoXVideoToVideoPipeline,\n","    CogVideoXDPMScheduler\n",")\n","from diffusers.utils import export_to_video, load_image, load_video\n","from transformers import T5EncoderModel\n","from IPython.display import display, Video"]},{"cell_type":"markdown","metadata":{"id":"K9zoGDp3cThP"},"source":["Подгружаем модели и создаем пайплайн.\n","\n","**Примечание.** `bfloat16`, который является рекомендованным `dtype` для CogVideoX-5B, вызовет ошибку Out Of Memory в виду недостаточной поддержки на Turing GPUs. Поэтому мы будем использовать `float16`, который, к сожалению, может привести к более плохому качеству генерации. Лучше всего использовать Ampere или GPUs , но имеем то, что имеем..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aOSWnkhJcxAc"},"outputs":[],"source":["# Спасибо [@camenduru](https://github.com/camenduru)!\n","# Причина, по которой мы используем чекпоинт Camenduru вместо оригинального, состоит в том, что они\n","# были экспортированы с max_shard_size в размере \"5GB\", когда сохранялись с `.save_pretrained`.\n","# Оригинальные же веса были с max_shard_size в размере \"10GB\", что на Colab привело бы к OOM (на CPU).\n","\n","transformer = CogVideoXTransformer3DModel.from_pretrained(\"camenduru/cogvideox-5b-float16\", subfolder=\"transformer\", torch_dtype=torch.float16)\n","text_encoder = T5EncoderModel.from_pretrained(\"camenduru/cogvideox-5b-float16\", subfolder=\"text_encoder\", torch_dtype=torch.float16)\n","vae = AutoencoderKLCogVideoX.from_pretrained(\"THUDM/CogVideoX-5b\", subfolder=\"vae\", torch_dtype=torch.float16)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tckTx-25dL44"},"outputs":[],"source":["# Создаем пайплайн и запускаем инференс\n","pipe = CogVideoXPipeline.from_pretrained(\n","    \"THUDM/CogVideoX-5b\",\n","    text_encoder=text_encoder,\n","    transformer=transformer,\n","    vae=vae,\n","    torch_dtype=torch.float16,\n",")"]},{"cell_type":"markdown","metadata":{"id":"u1a6xLpWdPsf"},"source":["Подключаем оптимизации памяти. Отметим, что очень важно использование последовательной GPU offloading, чтобы запускать модель на Turing и ниже архитектурах GPU. Это агрессивно переносит практически все на CPU и перемещает на GPU только то, что нужно именно в моменте. Это сохраняет очень много VRAM, однако приводит к сильно долгим генерациям... Но что поделать на Colab...\n","\n","Еще обратим ваше внимание на такую технику, как tiling, или разбиение на фрагменты во время энкодинга и декодинга VAE. Идея в том, что в отличие от обработки изображений (2D данных), преобразование фрагмента видео (3D данных) требует куда большее число памяти за раз, из-за чего не всегда удается даже просто сделать forward pass у энкодера или декодера VAE. Tiling — это процедура, при которой объект разбивается по пространственной размерности на отдельные фрагменты (тайлы), перекрывающие друг друга с небольшим запасом, которые обрабатываются последовательно, а затем конкатенируются для получения итогового результата. При этом накладывающиеся друг на друга края смешиваются (происходит blending)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"REZa3k9_dmjp"},"outputs":[],"source":["pipe.enable_sequential_cpu_offload()\n","# pipe.vae.enable_tiling()"]},{"cell_type":"markdown","metadata":{"id":"IHnRgaRDdnld"},"source":["Генерируем!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9QtM9_atdpEG"},"outputs":[],"source":["prompt = (\n","    \"A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. \"\n","    \"The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other \"\n","    \"pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, \"\n","    \"casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. \"\n","    \"The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical \"\n","    \"atmosphere of this unique musical performance.\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eRz87yWVdqG5"},"outputs":[],"source":["video = pipe(prompt=prompt, guidance_scale=6, use_dynamic_cfg=True, num_inference_steps=50).frames[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CgHROGwOdrvm"},"outputs":[],"source":["export_to_video(video, \"output-t2v.mp4\", fps=8)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8umnOjsnfuV7"},"outputs":[],"source":["display(Video(\"output-t2v.mp4\", embed=True))"]},{"cell_type":"markdown","source":["![](https://drive.google.com/uc?export=view&id=1sHYyrwws_UxHl6BvHsrvlxN15nMbYbyW)"],"metadata":{"id":"hELUP_QBjxsJ"}},{"cell_type":"markdown","metadata":{"id":"Yq4cutsGfeG9"},"source":["Итак, у нас получилось сгенерировать видео с помощью CogVideoX в формате Text-to-Video."]},{"cell_type":"markdown","metadata":{"id":"LY4zm-rzdtZF"},"source":["#### 3.3.2. Image-to-Video"]},{"cell_type":"markdown","metadata":{"id":"D1qYXtCid4sq"},"source":["Этот пайплайн уже посложнее, поскольку просто так, без дообучения, работать не будет. Авторы модифицируют архитектуру, **конкатенируя исходный латент с латентом картинки**. Благодаря этому модель учится использовать картинку в качестве первого кадра видео."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CbJJuOMAijgo"},"outputs":[],"source":["# Инициализируем части пайплайна\n","transformer = CogVideoXTransformer3DModel.from_pretrained(\"THUDM/CogVideoX-5b-I2V\", subfolder=\"transformer\", torch_dtype=torch.float16)\n","text_encoder = T5EncoderModel.from_pretrained(\"THUDM/CogVideoX-5b-I2V\", subfolder=\"text_encoder\", torch_dtype=torch.float16)\n","vae = AutoencoderKLCogVideoX.from_pretrained(\"THUDM/CogVideoX-5b-I2V\", subfolder=\"vae\", torch_dtype=torch.float16)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Njmj15eZeYby"},"outputs":[],"source":["# Создаем пайплайн и запускаем инференс\n","pipe = CogVideoXImageToVideoPipeline.from_pretrained(\n","    \"THUDM/CogVideoX-5b-I2V\",\n","    text_encoder=text_encoder,\n","    transformer=transformer,\n","    vae=vae,\n","    torch_dtype=torch.float16,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"euNeAcnJeg64"},"outputs":[],"source":["pipe.enable_sequential_cpu_offload()\n","# pipe.vae.enable_tiling()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zOf9eXCfeiIN"},"outputs":[],"source":["prompt = \"An astronaut hatching from an egg, on the surface of the moon, the darkness and depth of space realised in the background. High quality, ultrarealistic detail and breath-taking movie-like camera shot.\"\n","image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/astronaut.jpg\")\n","images.show()"]},{"cell_type":"markdown","source":["![](https://drive.google.com/uc?export=view&id=1r7ANGgIaYIrBSKbF7cAUcDSc22aTDXAh)"],"metadata":{"id":"EIexVfLCljQ7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tCSmqe4Eejjq"},"outputs":[],"source":["video = pipe(image=image, prompt=prompt, guidance_scale=6, use_dynamic_cfg=True, num_inference_steps=50).frames[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GbCgdtmNekd4"},"outputs":[],"source":["export_to_video(video, \"output-i2v.mp4\", fps=8)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3c0f9MHQfw4L"},"outputs":[],"source":["display(Video(\"output-i2v.mp4\", embed=True))"]},{"cell_type":"markdown","source":["![](https://drive.google.com/uc?export=view&id=1A70FqReIK5c0ANwV6HVCtEbsACpLSBgb)"],"metadata":{"id":"RkYGSkh-jStZ"}},{"cell_type":"markdown","metadata":{"id":"4ClwtuZnfYtz"},"source":["Итак, у нас получилось сгенерировать видео с помощью CogVideoX в формате Image-to-Video."]},{"cell_type":"markdown","metadata":{"id":"I03MoO8ger-f"},"source":["#### 3.3.3. Video-to-Video"]},{"cell_type":"markdown","metadata":{"id":"FGqz5nm4euii"},"source":["В отличие от Image-to-Video, пайплайн Video-to-Video работает без дообучения. Идея такая же, как и в случае image-to-image генерации изображений (см. примеры из Deforum выше). Мы частично зашумляем входное видео, а затем стартуем с этой промежуточной точки на траектории и расшумляем уже с новым текстовым описанием."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xe_F_3tOe_dJ"},"outputs":[],"source":["# Создаем пайплайн и запускаем инференс\n","pipe = CogVideoXVideoToVideoPipeline.from_pretrained(\n","    \"THUDM/CogVideoX-5b\",\n","    text_encoder=text_encoder,\n","    transformer=transformer,\n","    vae=vae,\n","    torch_dtype=torch.float16,\n",")\n","pipe.scheduler = CogVideoXDPMScheduler.from_config(pipe.scheduler.config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XAMuGlyOfMwg"},"outputs":[],"source":["pipe.enable_sequential_cpu_offload()\n","pipe.vae.enable_tiling()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_i-cBcXPfO7c"},"outputs":[],"source":["input_video = load_video(\n","    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/hiker.mp4\"\n",")\n","prompt = (\n","    \"An astronaut stands triumphantly at the peak of a towering mountain. Panorama of rugged peaks and \"\n","    \"valleys. Very futuristic vibe and animated aesthetic. Highlights of purple and golden colors in \"\n","    \"the scene. The sky is looks like an animated/cartoonish dream of galaxies, nebulae, stars, planets, \"\n","    \"moons, but the remainder of the scene is mostly realistic.\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zJFQ_92VfQT-"},"outputs":[],"source":["video = pipe(video=input_video, prompt=prompt, strength=0.7, guidance_scale=6, use_dynamic_cfg=True, num_inference_steps=50).frames[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KbHQi6wKfSKw"},"outputs":[],"source":["export_to_video(input_video, \"input.mp4\", fps=8)\n","export_to_video(video, \"output-v2v.mp4\", fps=8)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xR68MzlSfV5r"},"outputs":[],"source":["display(Video(\"input.mp4\", embed=True))\n","display(Video(\"output-v2v.mp4\", embed=True))"]},{"cell_type":"markdown","source":["![](https://drive.google.com/uc?export=view&id=16f3sk0p5GNsSVQH-x9tJXmBZIlxXkv1c)\n","![](https://drive.google.com/uc?export=view&id=1-E-2wOeGx6euHAY2DXYz1stIAQDaNm9B)\n"],"metadata":{"id":"6UCMtB-0jeuE"}},{"cell_type":"markdown","metadata":{"id":"w3m1HA-Aff7u"},"source":["Итак, у нас получилось сгенерировать видео с помощью CogVideoX в формате Video-to-Video."]},{"cell_type":"markdown","metadata":{"id":"Rf-Ryz5s96X9"},"source":["## Итоги"]},{"cell_type":"markdown","metadata":{"id":"R8fKY39R97JO"},"source":["Подведем итоги нашего сегодняшнего занятия.\n","1. Есть три основных подхода к генерации видео: 1) с использованием предобученной text-to-image (T2I) модели; 2) дообучение T2I; 3) обучение T2V-модели с нуля.\n","2. Одним из первых подходов к генерации видео без обучения является Deforum. Идея состоит в последовательном использовании image-to-image (I2I) редактирования изображений. При этом также используются различные пространственные преобразования для изменения положения камеры.\n","3. Дообучение T2I-модели приводит к лучшему пониманию динамики сцены, а также более высокому качеству генерации. Один из ключевых подходов AnimateDiff позволяет достичь этого благодаря трем модулям: 1) Domain Adapter; 2) Motion Module; 3) MotionLoRA.\n","4. Одна из лучших моделей генерации видео на текущий момент, CogVideoX, имеет трансформерную архитектуру и обрабатывает латенты с помощью 3D Causal Vae. Большое число качественных данных и многостадийное обучение позволили CogVideoX получить отличные генерации во всех трех режимах: T2V, I2V и V2V."]},{"cell_type":"markdown","metadata":{"id":"1mWhgay79-tq"},"source":["#### Полезные источники"]},{"cell_type":"markdown","metadata":{"id":"xc6hp-05LPGW"},"source":["Deforum:\n","- https://github.com/deforum-art/sd-webui-deforum\n","- https://reticulated.net/dailyai/stable-diffusion-videos-with-visions-of-chaos-deforum-animation/\n","- https://dreamingcomputers.com/deforum-stable-diffusion/deforum-stable-diffusion-settings/\n","- https://medium.com/@louis24/deforum-tutorial-94b15dea53d1\n","- https://docs.google.com/document/d/1RrQv7FntzOuLg4ohjRZPVL7iptIyBhwwbcEYEW2OfcI/edit#heading=h.7z6glzthkva2\n","- https://github.com/XLabs-AI/deforum-x-flux\n","- https://habr.com/ru/companies/sberbank/articles/766968/\n","- https://colab.research.google.com/github/deforum-art/deforum-stable-diffusion/blob/main/Deforum_Stable_Diffusion.ipynb#scrollTo=IJjzzkKlWM_s\n","- https://huggingface.co/docs/transformers/tasks/monocular_depth_estimation\n","\n","AnimateDiff:\n","- https://animatediff.github.io/\n","- https://huggingface.co/docs/diffusers/main/api/pipelines/animatediff\n","- https://colab.research.google.com/github/dmarx/notebooks/blob/main/AnimateDiff.ipynb\n","- https://github.com/huggingface/diffusers/tree/main/examples/community#animatediff-controlnet-pipeline\n","- https://stable-diffusion-art.com/animatediff/\n","\n","CogVideoX:\n","- https://github.com/THUDM/CogVideo\n","- https://huggingface.co/spaces/THUDM/CogVideoX-5B-Space\n","- https://huggingface.co/docs/diffusers/api/pipelines/cogvideox\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"nkiselev-video","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"}},"nbformat":4,"nbformat_minor":0}